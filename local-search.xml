<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Beta Distribution Learning for Reliable Roadway Crash Risk Assessment</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_04886v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_04886v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04886v1">http://arxiv.org/abs/2511.04886v1</a><br>Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions–a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A benchmark multimodal oro-dental dataset for large vision-language models</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_04948v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_04948v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04948v1">http://arxiv.org/abs/2511.04948v1</a><br>The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_04963v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_04963v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04963v1">http://arxiv.org/abs/2511.04963v1</a><br>Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time&#x2F;gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR&#x2F;SSIM scores of 29.83 dB&#x2F;90.84% for fMRI synthesis (+1.54 dB&#x2F;+4.12% over baselines) and 30.00 dB&#x2F;77.55% for dMRI synthesis (+1.02 dB&#x2F;+2.2%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92%&#x2F;66.02%&#x2F;64.15% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \href{<a href="https://github.com/SXR3015/PDS%7D%7BPDS">https://github.com/SXR3015/PDS}{PDS</a> GitHub Repository}</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_04949v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_04949v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04949v1">http://arxiv.org/abs/2511.04949v1</a><br>Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Fourier shapes to probe the geometric world of deep neural networks</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_04970v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_04970v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04970v1">http://arxiv.org/abs/2511.04970v1</a><br>While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model’s salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05034v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05034v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05034v1">http://arxiv.org/abs/2511.05034v1</a><br>Whole Slide Image (WSI) representation is critical for cancer subtyping, cancer recognition and mutation prediction.Training an end-to-end WSI representation model poses significant challenges, as a standard gigapixel slide can contain tens of thousands of image tiles, making it difficult to compute gradients of all tiles in a single mini-batch due to current GPU limitations. To address this challenge, we propose a method of dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI representation. Our approach utilizes a memory bank to store the features of tiles across all WSIs in the dataset. During training, a mini-batch usually contains multiple WSIs. For each WSI in the batch, a subset of tiles is randomly sampled and their features are computed using a tile encoder. Then, additional tile features from the same WSI are selected from the memory bank. The representation of each individual WSI is generated using a residual encoding technique that incorporates both the sampled features and those retrieved from the memory bank. Finally, the slide-level contrastive loss is computed based on the representations and histopathology reports ofthe WSIs within the mini-batch. Experiments conducted over cancer subtyping, cancer recognition, and mutation prediction tasks proved the effectiveness of the proposed DRE-SLCL method.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.4.9; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05055v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05055v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05055v1">http://arxiv.org/abs/2511.05055v1</a><br>Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05073v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05073v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05073v1">http://arxiv.org/abs/2511.05073v1</a><br>Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05150v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05150v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05150v1">http://arxiv.org/abs/2511.05150v1</a><br>AI-based biomarkers can infer molecular features directly from hematoxylin &amp; eosin (H&amp;E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05229v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05229v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05229v1">http://arxiv.org/abs/2511.05229v1</a><br>Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05250v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05250v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05250v1">http://arxiv.org/abs/2511.05250v1</a><br>Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05263v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05263v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05263v1">http://arxiv.org/abs/2511.05263v1</a><br>The analysis of character appearance frequency is essential for understanding narrative structure, character prominence, and story progression in anime. In this work, we introduce OregairuChar, a benchmark dataset designed for appearance frequency analysis in the anime series My Teen Romantic Comedy SNAFU. The dataset comprises 1600 manually selected frames from the third season, annotated with 2860 bounding boxes across 11 main characters. OregairuChar captures diverse visual challenges, including occlusion, pose variation, and inter-character similarity, providing a realistic basis for appearance-based studies. To enable quantitative research, we benchmark several object detection models on the dataset and leverage their predictions for fine-grained, episode-level analysis of character presence over time. This approach reveals patterns of character prominence and their evolution within the narrative. By emphasizing appearance frequency, OregairuChar serves as a valuable resource for exploring computational narrative dynamics and character-centric storytelling in stylized media.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepEyesV2: Toward Agentic Multimodal Model</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05271v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05271v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05271v1">http://arxiv.org/abs/2511.05271v1</a><br>Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05299v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05299v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05299v1">http://arxiv.org/abs/2511.05299v1</a><br>Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar’s state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at <a href="https://github.com/yzy-bupt/LiveStar">https://github.com/yzy-bupt/LiveStar</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05308v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05308v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05308v1">http://arxiv.org/abs/2511.05308v1</a><br>As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at <a href="https://github.com/matteo-bastico/DiffusionPointTransformer">https://github.com/matteo-bastico/DiffusionPointTransformer</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05394v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05394v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05394v1">http://arxiv.org/abs/2511.05394v1</a><br>We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
      <tag>H.5.2; H.5.1; I.4.8; I.2.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05404v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05404v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05404v1">http://arxiv.org/abs/2511.05404v1</a><br>Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at github.com&#x2F;DLR-RM&#x2F;MPRF.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.9; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On Flow Matching KL Divergence</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05480v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05480v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05480v1">http://arxiv.org/abs/2511.05480v1</a><br>We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\epsilon^2 &gt; 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</title>
    <link href="/2025/11/07/cs.AI/2025-11-07-2511_05489v1/"/>
    <url>/2025/11/07/cs.AI/2025-11-07-2511_05489v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05489v1">http://arxiv.org/abs/2511.05489v1</a><br>Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at <a href="https://github.com/Time-Search/TimeSearch-R">https://github.com/Time-Search/TimeSearch-R</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Quantifying the Risk of Transferred Black Box Attacks</title>
    <link href="/2025/11/07/cs.CR/2025-11-07-2511_05102v1/"/>
    <url>/2025/11/07/cs.CR/2025-11-07-2511_05102v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05102v1">http://arxiv.org/abs/2511.05102v1</a><br>Neural networks have become pervasive across various applications, including security-related products. However, their widespread adoption has heightened concerns regarding vulnerability to adversarial attacks. With emerging regulations and standards emphasizing security, organizations must reliably quantify risks associated with these attacks, particularly regarding transferred adversarial attacks, which remain challenging to evaluate accurately. This paper investigates the complexities involved in resilience testing against transferred adversarial attacks. Our analysis specifically addresses black-box evasion attacks, highlighting transfer-based attacks due to their practical significance and typically high transferability between neural network models. We underline the computational infeasibility of exhaustively exploring high-dimensional input spaces to achieve complete test coverage. As a result, comprehensive adversarial risk mapping is deemed impractical. To mitigate this limitation, we propose a targeted resilience testing framework that employs surrogate models strategically selected based on Centered Kernel Alignment (CKA) similarity. By leveraging surrogate models exhibiting both high and low CKA similarities relative to the target model, the proposed approach seeks to optimize coverage of adversarial subspaces. Risk estimation is conducted using regression-based estimators, providing organizations with realistic and actionable risk quantification.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$&#92;mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models</title>
    <link href="/2025/11/07/cs.CR/2025-11-07-2511_05319v1/"/>
    <url>/2025/11/07/cs.CR/2025-11-07-2511_05319v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05319v1">http://arxiv.org/abs/2511.05319v1</a><br>Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers. However, in the era of AIGC, the capacity of steganography is more critical than ever. In this work, we present Sentence-to-Image Steganography, an instance of Semantic Steganography, a novel task that enables the hiding of arbitrary sentence-level messages within a cover image. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages for evaluation. Finally, we present $\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large language models (LLMs) to embed high-level textual information, such as sentences or even paragraphs, into images. Unlike traditional bit-level counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich content through a newly designed pipeline in which the LLM is involved throughout the entire process. Both quantitative and qualitative experiments demonstrate that our method effectively unlocks new semantic steganographic capabilities for LLMs. The source code will be released soon.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title>
    <link href="/2025/11/07/cs.CL/2025-11-07-2511_05017v1/"/>
    <url>/2025/11/07/cs.CL/2025-11-07-2511_05017v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05017v1">http://arxiv.org/abs/2511.05017v1</a><br>In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations – and to show that refining textual embeddings with visual information mitigates this issue – we leave exploration of advanced fusion strategies for future work.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval</title>
    <link href="/2025/11/07/cs.GR/2025-11-07-2511_05020v1/"/>
    <url>/2025/11/07/cs.GR/2025-11-07-2511_05020v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05020v1">http://arxiv.org/abs/2511.05020v1</a><br>Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text. Most existing methods rely on a single model to perform feature fusion and similarity matching. However, this paradigm faces two major challenges. First, one model alone can’t see the whole picture and the tiny details at the same time; it has to handle different tasks with the same weights, so it often misses the small but important links between image and text. Second, the absence of dynamic weight allocation prevents adaptive leveraging of complementary model strengths, so the resulting embedding drifts away from the target and misleads the nearest-neighbor search in CIR. To address these limitations, we propose Dynamic Adaptive Fusion (DAFM) for multi-model collaboration in CIR. Rather than optimizing a single method in isolation, DAFM exploits the complementary strengths of heterogeneous models and adaptively rebalances their contributions. This not only maximizes retrieval accuracy but also ensures that the performance gains are independent of the fusion order, highlighting the robustness of our approach. Experiments on the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our method achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an average Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up to 4.5%. These results confirm that dynamic multi-model collaboration provides an effective and general solution for CIR.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neural Image Abstraction Using Long Smoothing B-Splines</title>
    <link href="/2025/11/07/cs.GR/2025-11-07-2511_05360v1/"/>
    <url>/2025/11/07/cs.GR/2025-11-07-2511_05360v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05360v1">http://arxiv.org/abs/2511.05360v1</a><br>We integrate smoothing B-splines into a standard differentiable vector graphics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vectorization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges</title>
    <link href="/2025/11/07/cs.GR/2025-11-07-2511_05152v1/"/>
    <url>/2025/11/07/cs.GR/2025-11-07-2511_05152v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05152v1">http://arxiv.org/abs/2511.05152v1</a><br>Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t&#x3D;0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. Code and video comparisons are available online: <a href="https://interims-git.github.io/">https://interims-git.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks</title>
    <link href="/2025/11/07/cs.HC/2025-11-07-2511_05250v1/"/>
    <url>/2025/11/07/cs.HC/2025-11-07-2511_05250v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05250v1">http://arxiv.org/abs/2511.05250v1</a><br>Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly</title>
    <link href="/2025/11/07/cs.HC/2025-11-07-2511_05394v1/"/>
    <url>/2025/11/07/cs.HC/2025-11-07-2511_05394v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05394v1">http://arxiv.org/abs/2511.05394v1</a><br>We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
      <tag>H.5.2; H.5.1; I.4.8; I.2.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Fourier shapes to probe the geometric world of deep neural networks</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_04970v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_04970v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04970v1">http://arxiv.org/abs/2511.04970v1</a><br>While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model’s salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Early Alzheimer&#39;s Disease Detection from Retinal OCT Images: A UK Biobank Study</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_05106v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_05106v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05106v1">http://arxiv.org/abs/2511.05106v1</a><br>Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer’s disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Another BRIXEL in the Wall: Towards Cheaper Dense Features</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_05168v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_05168v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05168v1">http://arxiv.org/abs/2511.05168v1</a><br>Vision foundation models achieve strong performance on both global and locally dense downstream tasks. Pretrained on large images, the recent DINOv3 model family is able to produce very fine-grained dense feature maps, enabling state-of-the-art performance. However, computing these feature maps requires the input image to be available at very high resolution, as well as large amounts of compute due to the squared complexity of the transformer architecture. To address these issues, we propose BRIXEL, a simple knowledge distillation approach that has the student learn to reproduce its own feature maps at higher resolution. Despite its simplicity, BRIXEL outperforms the baseline DINOv3 models by large margins on downstream tasks when the resolution is kept fixed. Moreover, it is able to produce feature maps that are very similar to those of the teacher at a fraction of the computational cost. Code and model weights are available at <a href="https://github.com/alexanderlappe/BRIXEL">https://github.com/alexanderlappe/BRIXEL</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>What&#39;s on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_05292v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_05292v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05292v1">http://arxiv.org/abs/2511.05292v1</a><br>Accurate food intake detection is vital for dietary monitoring and chronic disease prevention. Traditional self-report methods are prone to recall bias, while camera-based approaches raise concerns about privacy. Furthermore, existing wearable-based methods primarily focus on a limited number of food types, such as hamburgers and pizza, failing to address the vast diversity of Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that classifies Chinese food types by integrating hand motion cues from a smartwatch with head dynamics from smart glasses. To filter out irrelevant daily activities, we design a two-stage detection pipeline. The first stage identifies eating states by distinguishing characteristic temporal patterns from non-eating behaviors. The second stage then conducts fine-grained food type recognition based on the motions captured during food intake. To evaluate CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings across 11 food categories and 10 participants. Experiments demonstrate that CuisineSense achieves high accuracy in both eating state detection and food classification, offering a practical solution for unobtrusive, wearable-based dietary monitoring.The system code is publicly available at <a href="https://github.com/joeeeeyin/CuisineSense.git">https://github.com/joeeeeyin/CuisineSense.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_05308v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_05308v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05308v1">http://arxiv.org/abs/2511.05308v1</a><br>As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at <a href="https://github.com/matteo-bastico/DiffusionPointTransformer">https://github.com/matteo-bastico/DiffusionPointTransformer</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_05449v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_05449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05449v1">http://arxiv.org/abs/2511.05449v1</a><br>Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at <a href="https://gitmerge3d.github.io/">https://gitmerge3d.github.io</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_05462v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_05462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05462v1">http://arxiv.org/abs/2511.05462v1</a><br>Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demonstrate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On Flow Matching KL Divergence</title>
    <link href="/2025/11/07/cs.LG/2025-11-07-2511_05480v1/"/>
    <url>/2025/11/07/cs.LG/2025-11-07-2511_05480v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05480v1">http://arxiv.org/abs/2511.05480v1</a><br>We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\epsilon^2 &gt; 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder</title>
    <link href="/2025/11/07/cs.MM/2025-11-07-2511_04977v1/"/>
    <url>/2025/11/07/cs.MM/2025-11-07-2511_04977v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04977v1">http://arxiv.org/abs/2511.04977v1</a><br>Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.</p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges</title>
    <link href="/2025/11/07/cs.MM/2025-11-07-2511_05152v1/"/>
    <url>/2025/11/07/cs.MM/2025-11-07-2511_05152v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05152v1">http://arxiv.org/abs/2511.05152v1</a><br>Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t&#x3D;0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. Code and video comparisons are available online: <a href="https://interims-git.github.io/">https://interims-git.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</title>
    <link href="/2025/11/07/cs.RO/2025-11-07-2511_05397v1/"/>
    <url>/2025/11/07/cs.RO/2025-11-07-2511_05397v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05397v1">http://arxiv.org/abs/2511.05397v1</a><br>While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: <a href="https://everydayvla.github.io/">https://everydayvla.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Beta Distribution Learning for Reliable Roadway Crash Risk Assessment</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04886v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04886v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04886v1">http://arxiv.org/abs/2511.04886v1</a><br>Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions–a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04892v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04892v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04892v1">http://arxiv.org/abs/2511.04892v1</a><br>Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.BM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04920v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04920v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04920v1">http://arxiv.org/abs/2511.04920v1</a><br>Image restoration (IR) aims to recover clean images from degraded observations. Despite remarkable progress, most existing methods focus on a single degradation type, whereas real-world images often suffer from multiple coexisting degradations, such as rain, noise, and haze coexisting in a single image, which limits their practical effectiveness. In this paper, we propose an adaptive multi-degradation image restoration network that reconstructs images by leveraging decoupled representations of degradation ingredients to guide path selection. Specifically, we design a degradation ingredient decoupling block (DIDBlock) in the encoder to separate degradation ingredients statistically by integrating spatial and frequency domain information, enhancing the recognition of multiple degradation types and making their feature representations independent. In addition, we present fusion block (FBlock) to integrate degradation information across all levels using learnable matrices. In the decoder, we further introduce a task adaptation block (TABlock) that dynamically activates or fuses functional branches based on the multi-degradation representation, flexibly selecting optimal restoration paths under diverse degradation conditions. The resulting tightly integrated architecture, termed IMDNet, is extensively validated through experiments, showing superior performance on multi-degradation restoration while maintaining strong competitiveness on single-degradation tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A benchmark multimodal oro-dental dataset for large vision-language models</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04948v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04948v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04948v1">http://arxiv.org/abs/2511.04948v1</a><br>The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04949v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04949v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04949v1">http://arxiv.org/abs/2511.04949v1</a><br>Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04963v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04963v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04963v1">http://arxiv.org/abs/2511.04963v1</a><br>Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time&#x2F;gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR&#x2F;SSIM scores of 29.83 dB&#x2F;90.84% for fMRI synthesis (+1.54 dB&#x2F;+4.12% over baselines) and 30.00 dB&#x2F;77.55% for dMRI synthesis (+1.02 dB&#x2F;+2.2%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92%&#x2F;66.02%&#x2F;64.15% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \href{<a href="https://github.com/SXR3015/PDS%7D%7BPDS">https://github.com/SXR3015/PDS}{PDS</a> GitHub Repository}</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Fourier shapes to probe the geometric world of deep neural networks</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04970v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04970v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04970v1">http://arxiv.org/abs/2511.04970v1</a><br>While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model’s salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04951v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04951v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04951v1">http://arxiv.org/abs/2511.04951v1</a><br>3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU’s memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS’s memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>D.4; I.3.2; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04972v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04972v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04972v1">http://arxiv.org/abs/2511.04972v1</a><br>Topological Data Analysis (TDA) involves techniques of analyzing the underlying structure and connectivity of data. However, traditional methods like persistent homology can be computationally demanding, motivating the development of neural network-based estimators capable of reducing computational overhead and inference time. A key barrier to advancing these methods is the lack of labeled 3D data with class distributions and diversity tailored specifically for supervised learning in TDA tasks. To address this, we introduce a novel approach for systematically generating labeled 3D datasets using the Repulsive Surface algorithm, allowing control over topological invariants, such as hole count. The resulting dataset offers varied geometry with topological labeling, making it suitable for training and benchmarking neural network estimators. This paper uses a synthetic 3D dataset to train a genus estimator network, created using a 3D convolutional transformer architecture. An observed decrease in accuracy as deformations increase highlights the role of not just topological complexity, but also geometric complexity, when training generalized estimators. This dataset fills a gap in labeled 3D datasets and generation for training and evaluating models and techniques for TDA.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UHDRes: Ultra-High-Definition Image Restoration via Dual-Domain Decoupled Spectral Modulation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05009v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05009v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05009v1">http://arxiv.org/abs/2511.05009v1</a><br>Ultra-high-definition (UHD) images often suffer from severe degradations such as blur, haze, rain, or low-light conditions, which pose significant challenges for image restoration due to their high resolution and computational demands. In this paper, we propose UHDRes, a novel lightweight dual-domain decoupled spectral modulation framework for UHD image restoration. It explicitly models the amplitude spectrum via lightweight spectrum-domain modulation, while restoring phase implicitly through spatial-domain refinement. We introduce the spatio-spectral fusion mechanism, which first employs a multi-scale context aggregator to extract local and global spatial features, and then performs spectral modulation in a decoupled manner. It explicitly enhances amplitude features in the frequency domain while implicitly restoring phase information through spatial refinement. Additionally, a shared gated feed-forward network is designed to efficiently promote feature interaction through shared-parameter convolutions and adaptive gating mechanisms. Extensive experimental comparisons on five public UHD benchmarks demonstrate that our UHDRes achieves the state-of-the-art restoration performance with only 400K parameters, while significantly reducing inference latency and memory usage. The codes and models are available at <a href="https://github.com/Zhao0100/UHDRes">https://github.com/Zhao0100/UHDRes</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05017v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05017v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05017v1">http://arxiv.org/abs/2511.05017v1</a><br>In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations – and to show that refining textual embeddings with visual information mitigates this issue – we leave exploration of advanced fusion strategies for future work.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_04977v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_04977v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04977v1">http://arxiv.org/abs/2511.04977v1</a><br>Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05020v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05020v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05020v1">http://arxiv.org/abs/2511.05020v1</a><br>Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text. Most existing methods rely on a single model to perform feature fusion and similarity matching. However, this paradigm faces two major challenges. First, one model alone can’t see the whole picture and the tiny details at the same time; it has to handle different tasks with the same weights, so it often misses the small but important links between image and text. Second, the absence of dynamic weight allocation prevents adaptive leveraging of complementary model strengths, so the resulting embedding drifts away from the target and misleads the nearest-neighbor search in CIR. To address these limitations, we propose Dynamic Adaptive Fusion (DAFM) for multi-model collaboration in CIR. Rather than optimizing a single method in isolation, DAFM exploits the complementary strengths of heterogeneous models and adaptively rebalances their contributions. This not only maximizes retrieval accuracy but also ensures that the performance gains are independent of the fusion order, highlighting the robustness of our approach. Experiments on the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our method achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an average Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up to 4.5%. These results confirm that dynamic multi-model collaboration provides an effective and general solution for CIR.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05034v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05034v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05034v1">http://arxiv.org/abs/2511.05034v1</a><br>Whole Slide Image (WSI) representation is critical for cancer subtyping, cancer recognition and mutation prediction.Training an end-to-end WSI representation model poses significant challenges, as a standard gigapixel slide can contain tens of thousands of image tiles, making it difficult to compute gradients of all tiles in a single mini-batch due to current GPU limitations. To address this challenge, we propose a method of dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI representation. Our approach utilizes a memory bank to store the features of tiles across all WSIs in the dataset. During training, a mini-batch usually contains multiple WSIs. For each WSI in the batch, a subset of tiles is randomly sampled and their features are computed using a tile encoder. Then, additional tile features from the same WSI are selected from the memory bank. The representation of each individual WSI is generated using a residual encoding technique that incorporates both the sampled features and those retrieved from the memory bank. Finally, the slide-level contrastive loss is computed based on the representations and histopathology reports ofthe WSIs within the mini-batch. Experiments conducted over cancer subtyping, cancer recognition, and mutation prediction tasks proved the effectiveness of the proposed DRE-SLCL method.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.4.9; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05038v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05038v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05038v1">http://arxiv.org/abs/2511.05038v1</a><br>We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Medical Referring Image Segmentation via Next-Token Mask Prediction</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05044v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05044v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05044v1">http://arxiv.org/abs/2511.05044v1</a><br>Medical Referring Image Segmentation (MRIS) involves segmenting target regions in medical images based on natural language descriptions. While achieving promising results, recent approaches usually involve complex design of multimodal fusion or multi-stage decoders. In this work, we propose NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive next-token prediction task over a unified multimodal sequence of tokenized image, text, and mask representations. This formulation streamlines model design by eliminating the need for modality-specific fusion and external segmentation models, supports a unified architecture for end-to-end training. It also enables the use of pretrained tokenizers from emerging large-scale multimodal models, enhancing generalization and adaptability. More importantly, to address challenges under this formulation-such as exposure bias, long-tail token distributions, and fine-grained lesion edges-we propose three novel strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance boundary sensitivity and mitigate long-tail distribution effects, and (3) a memory-based Hard Error Token (HET) optimization strategy that emphasizes difficult tokens during training. Extensive experiments on the QaTa-COV19 and MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art performance, offering a streamlined and effective alternative to traditional MRIS pipelines.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05055v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05055v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05055v1">http://arxiv.org/abs/2511.05055v1</a><br>Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05057v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05057v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05057v1">http://arxiv.org/abs/2511.05057v1</a><br>The effectiveness of Contrastive Language-Image Pre-training (CLIP) models critically depends on the semantic diversity and quality of their training data. However, while existing synthetic data generation methods primarily focus on increasing data volume, such emphasis often leads to limited semantic diversity and redundant or shallow captions. To address this limitation, we propose Role-SynthCLIP, a novel data synthesis framework that leverages multi-perspective role-playing prompts (e.g., a compositional analyst, an interpreter of image context) to guide Multimodal Large Language Models (MLLMs) in generating semantically diverse captions from distinct viewpoints. This mechanism enhances the semantic diversity and fine-grained image-text alignment of synthetic pairs, thereby improving caption expressiveness and accuracy while keeping the total number of image-text pairs unchanged. Experimental results demonstrate the effectiveness and efficiency of our method. A CLIP-B&#x2F;16 model trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on the MS COCO validation set, surpassing the best existing synthetic data baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained models are released at <a href="https://github.com/huangfu170/Role-SynthCLIP">https://github.com/huangfu170/Role-SynthCLIP</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05073v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05073v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05073v1">http://arxiv.org/abs/2511.05073v1</a><br>Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05092v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05092v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05092v1">http://arxiv.org/abs/2511.05092v1</a><br>With growing concerns over data privacy, researchers have started using virtual data as an alternative to sensitive real-world images for training person re-identification (Re-ID) models. However, existing virtual datasets produced by game engines still face challenges such as complex construction and poor domain generalization, making them difficult to apply in real scenarios. To address these challenges, we propose a Dual-stage Prompt-driven Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich prompts incorporating multi-dimensional attributes such as pedestrian appearance, illumination, and viewpoint that drive the diffusion model to synthesize diverse data end-to-end, building a large-scale virtual dataset named GenePerson with 130,519 images of 6,641 identities. In the second stage, we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn domain-invariant generalization features. With the aid of contrastive learning, we employ two textual inversion networks to map images into pseudo-words representing style and content, respectively, thereby constructing style-disentangled content prompts to guide the model in learning domain-invariant content features at the image level. Experiments demonstrate that models trained on GenePerson with PDM achieve state-of-the-art generalization performance, surpassing those on popular real and virtual Re-ID datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05095v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05095v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05095v1">http://arxiv.org/abs/2511.05095v1</a><br>Adverse weather severely impairs real-world visual perception, while existing vision models trained on synthetic data with fixed parameters struggle to generalize to complex degradations. To address this, we first construct HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse weather phenomena, and then design a dual-level reinforcement learning framework initialized with HFLS-Weather for cold-start training. Within this framework, at the local level, weather-specific restoration models are refined through perturbation-driven image quality optimization, enabling reward-based learning without paired supervision; at the global level, a meta-controller dynamically orchestrates model selection and execution order according to scene degradation. This framework enables continuous adaptation to real-world conditions and achieves state-of-the-art performance across a wide range of adverse weather scenarios. Code is available at <a href="https://github.com/xxclfy/AgentRL-Real-Weather">https://github.com/xxclfy/AgentRL-Real-Weather</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05059v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05059v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05059v1">http://arxiv.org/abs/2511.05059v1</a><br>During laparoscopic surgery, smoke generated by tissue cauterization can significantly degrade the visual quality of endoscopic frames, increasing the risk of surgical errors and hindering both clinical decision-making and computer-assisted visual analysis. Consequently, removing surgical smoke is critical to ensuring patient safety and maintaining operative efficiency. In this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical smoke removal. SurgiATM statistically bridges a physics-based atmospheric model and data-driven deep learning models, combining the superior generalizability of the former with the high accuracy of the latter. Furthermore, SurgiATM is designed as a lightweight, plug-and-play module that can be seamlessly integrated into diverse surgical desmoking architectures to enhance their accuracy and stability, better meeting clinical requirements. It introduces only two hyperparameters and no additional trainable weights, preserving the original network architecture with minimal computational and modification overhead. We conduct extensive experiments on three public surgical datasets with ten desmoking methods, involving multiple network architectures and covering diverse procedures, including cholecystectomy, partial nephrectomy, and diaphragm dissection. The results demonstrate that incorporating SurgiATM commonly reduces the restoration errors of existing models and relatively enhances their generalizability, without adding any trainable layers or weights. This highlights the convenience, low cost, effectiveness, and generalizability of the proposed method. The code for SurgiATM is released at <a href="https://github.com/MingyuShengSMY/SurgiATM">https://github.com/MingyuShengSMY/SurgiATM</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Quantifying the Risk of Transferred Black Box Attacks</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05102v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05102v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05102v1">http://arxiv.org/abs/2511.05102v1</a><br>Neural networks have become pervasive across various applications, including security-related products. However, their widespread adoption has heightened concerns regarding vulnerability to adversarial attacks. With emerging regulations and standards emphasizing security, organizations must reliably quantify risks associated with these attacks, particularly regarding transferred adversarial attacks, which remain challenging to evaluate accurately. This paper investigates the complexities involved in resilience testing against transferred adversarial attacks. Our analysis specifically addresses black-box evasion attacks, highlighting transfer-based attacks due to their practical significance and typically high transferability between neural network models. We underline the computational infeasibility of exhaustively exploring high-dimensional input spaces to achieve complete test coverage. As a result, comprehensive adversarial risk mapping is deemed impractical. To mitigate this limitation, we propose a targeted resilience testing framework that employs surrogate models strategically selected based on Centered Kernel Alignment (CKA) similarity. By leveraging surrogate models exhibiting both high and low CKA similarities relative to the target model, the proposed approach seeks to optimize coverage of adversarial subspaces. Risk estimation is conducted using regression-based estimators, providing organizations with realistic and actionable risk quantification.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Early Alzheimer&#39;s Disease Detection from Retinal OCT Images: A UK Biobank Study</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05106v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05106v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05106v1">http://arxiv.org/abs/2511.05106v1</a><br>Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer’s disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05108v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05108v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05108v1">http://arxiv.org/abs/2511.05108v1</a><br>Lane detection for autonomous driving in snow-covered environments remains a major challenge due to the frequent absence or occlusion of lane markings. In this paper, we present a novel, robust and realtime capable approach that bypasses the reliance on traditional lane markings by detecting roadside features,specifically vertical roadside posts called delineators, as indirect lane indicators. Our method first perceives these posts, then fits a smooth lane trajectory using a parameterized Bezier curve model, leveraging spatial consistency and road geometry. To support training and evaluation in these challenging scenarios, we introduce SnowyLane, a new synthetic dataset containing 80,000 annotated frames capture winter driving conditions, with varying snow coverage, and lighting conditions. Compared to state-of-the-art lane detection systems, our approach demonstrates significantly improved robustness in adverse weather, particularly in cases with heavy snow occlusion. This work establishes a strong foundation for reliable lane detection in winter scenarios and contributes a valuable resource for future research in all-weather autonomous driving. The dataset is available at <a href="https://ekut-es.github.io/snowy-lane">https://ekut-es.github.io/snowy-lane</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05150v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05150v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05150v1">http://arxiv.org/abs/2511.05150v1</a><br>AI-based biomarkers can infer molecular features directly from hematoxylin &amp; eosin (H&amp;E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05152v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05152v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05152v1">http://arxiv.org/abs/2511.05152v1</a><br>Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t&#x3D;0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. Code and video comparisons are available online: <a href="https://interims-git.github.io/">https://interims-git.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05170v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05170v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05170v1">http://arxiv.org/abs/2511.05170v1</a><br>Nucleus detection and classification (NDC) in histopathology analysis is a fundamental task that underpins a wide range of high-level pathology applications. However, existing methods heavily rely on labor-intensive nucleus-level annotations and struggle to fully exploit large-scale unlabeled data for learning discriminative nucleus representations. In this work, we propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised learning method tailored for NDC. At its core is NuLo (Nucleus-based Local self-distillation), a coordinate-guided mechanism that enables flexible local self-distillation based on predicted nucleus positions. By removing the need for strict spatial alignment between augmented views, NuLo allows critical cross-scale alignment, thus unlocking the capacity of models for fine-grained nucleus-level representation. To support MUSE, we design a simple yet effective encoder-decoder architecture and a large field-of-view semi-supervised fine-tuning strategy that together maximize the value of unlabeled pathology images. Extensive experiments on three widely used benchmarks demonstrate that MUSE effectively addresses the core challenges of histopathological NDC. The resulting models not only surpass state-of-the-art supervised baselines but also outperform generic pathology foundation models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Another BRIXEL in the Wall: Towards Cheaper Dense Features</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05168v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05168v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05168v1">http://arxiv.org/abs/2511.05168v1</a><br>Vision foundation models achieve strong performance on both global and locally dense downstream tasks. Pretrained on large images, the recent DINOv3 model family is able to produce very fine-grained dense feature maps, enabling state-of-the-art performance. However, computing these feature maps requires the input image to be available at very high resolution, as well as large amounts of compute due to the squared complexity of the transformer architecture. To address these issues, we propose BRIXEL, a simple knowledge distillation approach that has the student learn to reproduce its own feature maps at higher resolution. Despite its simplicity, BRIXEL outperforms the baseline DINOv3 models by large margins on downstream tasks when the resolution is kept fixed. Moreover, it is able to produce feature maps that are very similar to those of the teacher at a fraction of the computational cost. Code and model weights are available at <a href="https://github.com/alexanderlappe/BRIXEL">https://github.com/alexanderlappe/BRIXEL</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PySlyde: A Lightweight, Open-Source Toolkit for Pathology Preprocessing</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05183v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05183v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05183v1">http://arxiv.org/abs/2511.05183v1</a><br>The integration of artificial intelligence (AI) into pathology is advancing precision medicine by improving diagnosis, treatment planning, and patient outcomes. Digitised whole-slide images (WSIs) capture rich spatial and morphological information vital for understanding disease biology, yet their gigapixel scale and variability pose major challenges for standardisation and analysis. Robust preprocessing, covering tissue detection, tessellation, stain normalisation, and annotation parsing is critical but often limited by fragmented and inconsistent workflows. We present PySlyde, a lightweight, open-source Python toolkit built on OpenSlide to simplify and standardise WSI preprocessing. PySlyde provides an intuitive API for slide loading, annotation management, tissue detection, tiling, and feature extraction, compatible with modern pathology foundation models. By unifying these processes, it streamlines WSI preprocessing, enhances reproducibility, and accelerates the generation of AI-ready datasets, enabling researchers to focus on model development and downstream analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Walk the Lines 2: Contour Tracking for Detailed Segmentation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05210v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05210v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05210v1">http://arxiv.org/abs/2511.05210v1</a><br>This paper presents Walk the Lines 2 (WtL2), a unique contour tracking algorithm specifically adapted for detailed segmentation of infrared (IR) ships and various objects in RGB.1 This extends the original Walk the Lines (WtL) [12], which focused solely on detailed ship segmentation in color. These innovative WtLs can replace the standard non-maximum suppression (NMS) by using contour tracking to refine the object contour until a 1-pixel-wide closed shape can be binarized, forming a segmentable area in foreground-background scenarios. WtL2 broadens the application range of WtL beyond its original scope, adapting to IR and expanding to diverse objects within the RGB context. To achieve IR segmentation, we adapt its input, the object contour detector, to IR ships. In addition, the algorithm is enhanced to process a wide range of RGB objects, outperforming the latest generation of contour-based methods when achieving a closed object contour, offering high peak Intersection over Union (IoU) with impressive details. This positions WtL2 as a compelling method for specialized applications that require detailed segmentation or high-quality samples, potentially accelerating progress in several niche areas of image segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05229v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05229v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05229v1">http://arxiv.org/abs/2511.05229v1</a><br>Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05245v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05245v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05245v1">http://arxiv.org/abs/2511.05245v1</a><br>The current mainstream and state-of-the-art anomaly detection (AD) methods are substantially established on pretrained feature networks yielded by ImageNet pretraining. However, regardless of supervised or self-supervised pretraining, the pretraining process on ImageNet does not match the goal of anomaly detection (i.e., pretraining in natural images doesn’t aim to distinguish between normal and abnormal). Moreover, natural images and industrial image data in AD scenarios typically have the distribution shift. The two issues can cause ImageNet-pretrained features to be suboptimal for AD tasks. To further promote the development of the AD field, pretrained representations specially for AD tasks are eager and very valuable. To this end, we propose a novel AD representation learning framework specially designed for learning robust and discriminative pretrained representations for industrial anomaly detection. Specifically, closely surrounding the goal of anomaly detection (i.e., focus on discrepancies between normals and anomalies), we propose angle- and norm-oriented contrastive losses to maximize the angle size and norm difference between normal and abnormal features simultaneously. To avoid the distribution shift from natural images to AD images, our pretraining is performed on a large-scale AD dataset, RealIAD. To further alleviate the potential shift between pretraining data and downstream AD datasets, we learn the pretrained AD representations based on the class-generalizable representation, residual features. For evaluation, based on five embedding-based AD methods, we simply replace their original features with our pretrained representations. Extensive experiments on five AD datasets and five backbones consistently show the superiority of our pretrained features. The code is available at <a href="https://github.com/xcyao00/ADPretrain">https://github.com/xcyao00/ADPretrain</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05250v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05250v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05250v1">http://arxiv.org/abs/2511.05250v1</a><br>Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05253v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05253v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05253v1">http://arxiv.org/abs/2511.05253v1</a><br>Introduction: Accurate intraoperative delineation of colorectal liver metastases (CRLM) is crucial for achieving negative resection margins but remains challenging using intraoperative ultrasound (iUS) due to low contrast, noise, and operator dependency. Automated segmentation could enhance precision and efficiency in ultrasound-based navigation workflows.   Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two variants were compared: one trained on full iUS volumes and another on cropped regions around tumors. Segmentation accuracy was assessed using Dice Similarity Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference (RVD) on retrospective and prospective datasets. The workflow was integrated into 3D Slicer for real-time intraoperative use.   Results: The cropped-volume model significantly outperformed the full-volume model across all metrics (AUC-ROC &#x3D; 0.898 vs 0.718). It achieved median DSC &#x3D; 0.74, recall &#x3D; 0.79, and HDist. &#x3D; 17.1 mm comparable to semi-automatic segmentation but with <del>4x faster execution (</del> 1 min). Prospective intraoperative testing confirmed robust and consistent performance, with clinically acceptable accuracy for real-time surgical guidance.   Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net provides reliable, near real-time results with minimal operator input. The method enables efficient, registration-free ultrasound-based navigation for hepatic surgery, approaching expert-level accuracy while substantially reducing manual workload and procedure time.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05263v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05263v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05263v1">http://arxiv.org/abs/2511.05263v1</a><br>The analysis of character appearance frequency is essential for understanding narrative structure, character prominence, and story progression in anime. In this work, we introduce OregairuChar, a benchmark dataset designed for appearance frequency analysis in the anime series My Teen Romantic Comedy SNAFU. The dataset comprises 1600 manually selected frames from the third season, annotated with 2860 bounding boxes across 11 main characters. OregairuChar captures diverse visual challenges, including occlusion, pose variation, and inter-character similarity, providing a realistic basis for appearance-based studies. To enable quantitative research, we benchmark several object detection models on the dataset and leverage their predictions for fine-grained, episode-level analysis of character presence over time. This approach reveals patterns of character prominence and their evolution within the narrative. By emphasizing appearance frequency, OregairuChar serves as a valuable resource for exploring computational narrative dynamics and character-centric storytelling in stylized media.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05219v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05219v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05219v1">http://arxiv.org/abs/2511.05219v1</a><br>Controlling the spatial and semantic structure of diffusion-generated images remains a challenge. Existing methods like ControlNet rely on handcrafted condition maps and retraining, limiting flexibility and generalization. Inversion-based approaches offer stronger alignment but incur high inference cost due to dual-path denoising. We present FreeControl, a training-free framework for semantic structural control in diffusion models. Unlike prior methods that extract attention across multiple timesteps, FreeControl performs one-step attention extraction from a single, optimally chosen key timestep and reuses it throughout denoising. This enables efficient structural guidance without inversion or retraining. To further improve quality and stability, we introduce Latent-Condition Decoupling (LCD): a principled separation of the key timestep and the noised latent used in attention extraction. LCD provides finer control over attention quality and eliminates structural artifacts. FreeControl also supports compositional control via reference images assembled from multiple sources - enabling intuitive scene layout design and stronger prompt alignment. FreeControl introduces a new paradigm for test-time control, enabling structurally and semantically aligned, visually coherent generation directly from raw images, with the flexibility for intuitive compositional design and compatibility with modern diffusion models at approximately 5 percent additional cost.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepEyesV2: Toward Agentic Multimodal Model</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05271v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05271v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05271v1">http://arxiv.org/abs/2511.05271v1</a><br>Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>What&#39;s on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05292v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05292v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05292v1">http://arxiv.org/abs/2511.05292v1</a><br>Accurate food intake detection is vital for dietary monitoring and chronic disease prevention. Traditional self-report methods are prone to recall bias, while camera-based approaches raise concerns about privacy. Furthermore, existing wearable-based methods primarily focus on a limited number of food types, such as hamburgers and pizza, failing to address the vast diversity of Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that classifies Chinese food types by integrating hand motion cues from a smartwatch with head dynamics from smart glasses. To filter out irrelevant daily activities, we design a two-stage detection pipeline. The first stage identifies eating states by distinguishing characteristic temporal patterns from non-eating behaviors. The second stage then conducts fine-grained food type recognition based on the motions captured during food intake. To evaluate CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings across 11 food categories and 10 participants. Experiments demonstrate that CuisineSense achieves high accuracy in both eating state detection and food classification, offering a practical solution for unobtrusive, wearable-based dietary monitoring.The system code is publicly available at <a href="https://github.com/joeeeeyin/CuisineSense.git">https://github.com/joeeeeyin/CuisineSense.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cross-domain EEG-based Emotion Recognition with Contrastive Learning</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05293v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05293v1">http://arxiv.org/abs/2511.05293v1</a><br>Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05299v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05299v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05299v1">http://arxiv.org/abs/2511.05299v1</a><br>Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar’s state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at <a href="https://github.com/yzy-bupt/LiveStar">https://github.com/yzy-bupt/LiveStar</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$&#92;mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05319v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05319v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05319v1">http://arxiv.org/abs/2511.05319v1</a><br>Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers. However, in the era of AIGC, the capacity of steganography is more critical than ever. In this work, we present Sentence-to-Image Steganography, an instance of Semantic Steganography, a novel task that enables the hiding of arbitrary sentence-level messages within a cover image. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages for evaluation. Finally, we present $\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large language models (LLMs) to embed high-level textual information, such as sentences or even paragraphs, into images. Unlike traditional bit-level counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich content through a newly designed pipeline in which the LLM is involved throughout the entire process. Both quantitative and qualitative experiments demonstrate that our method effectively unlocks new semantic steganographic capabilities for LLMs. The source code will be released soon.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05356v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05356v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05356v1">http://arxiv.org/abs/2511.05356v1</a><br>Articulated object perception presents significant challenges in computer vision, particularly because most existing methods ignore temporal dynamics despite the inherently dynamic nature of such objects. The use of 4D temporal data has not been thoroughly explored in articulated object perception and remains unexamined for panoptic segmentation. The lack of a benchmark dataset further hurt this field. To this end, we introduce Artic4D as a new dataset derived from PartNet Mobility and augmented with synthetic sensor data, featuring 4D panoptic annotations and articulation parameters. Building on this dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework. This approach explicitly estimates per-frame offsets mapping observed object parts to a learned canonical space, thereby enhancing part-level segmentation. The framework employs this canonical representation to achieve consistent alignment of object parts across sequential frames. Comprehensive experiments on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the art approaches in panoptic segmentation accuracy in more complex scenarios. These findings highlight the effectiveness of temporal modeling and canonical alignment in dynamic object understanding, and pave the way for future advances in 4D articulated object perception.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.2.10; I.4.6; I.5.1; I.5.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05308v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05308v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05308v1">http://arxiv.org/abs/2511.05308v1</a><br>As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at <a href="https://github.com/matteo-bastico/DiffusionPointTransformer">https://github.com/matteo-bastico/DiffusionPointTransformer</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neural Image Abstraction Using Long Smoothing B-Splines</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05360v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05360v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05360v1">http://arxiv.org/abs/2511.05360v1</a><br>We integrate smoothing B-splines into a standard differentiable vector graphics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vectorization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dense Motion Captioning</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05369v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05369v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05369v1">http://arxiv.org/abs/2511.05369v1</a><br>Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.2.10; I.4.8; I.5.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05393v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05393v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05393v1">http://arxiv.org/abs/2511.05393v1</a><br>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05394v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05394v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05394v1">http://arxiv.org/abs/2511.05394v1</a><br>We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
      <tag>H.5.2; H.5.1; I.4.8; I.2.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05397v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05397v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05397v1">http://arxiv.org/abs/2511.05397v1</a><br>While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: <a href="https://everydayvla.github.io/">https://everydayvla.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05403v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05403v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05403v1">http://arxiv.org/abs/2511.05403v1</a><br>The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM’s scale and diversity make it a valuable real-world resource for hand modeling and related research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05404v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05404v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05404v1">http://arxiv.org/abs/2511.05404v1</a><br>Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at github.com&#x2F;DLR-RM&#x2F;MPRF.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.9; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05421v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05421v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05421v1">http://arxiv.org/abs/2511.05421v1</a><br>Continual learning is an emerging topic in the field of deep learning, where a model is expected to learn continuously for new upcoming tasks without forgetting previous experiences. This field has witnessed numerous advancements, but few works have been attempted in the direction of image restoration. Handling large image sizes and the divergent nature of various degradation poses a unique challenge in the restoration domain. However, existing works require heavily engineered architectural modifications for new task adaptation, resulting in significant computational overhead. Regularization-based methods are unsuitable for restoration, as different restoration challenges require different kinds of feature processing. In this direction, we propose a simple modification of the convolution layer to adapt the knowledge from previous restoration tasks without touching the main backbone architecture. Therefore, it can be seamlessly applied to any deep architecture without any structural modifications. Unlike other approaches, we demonstrate that our model can increase the number of trainable parameters without significantly increasing computational overhead or inference time. Experimental validation demonstrates that new restoration tasks can be introduced without compromising the performance of existing tasks. We also show that performance on new restoration tasks improves by adapting the knowledge from the knowledge base created by previous restoration tasks. The code is available at <a href="https://github.com/aupendu/continual-restore">https://github.com/aupendu/continual-restore</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05432v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05432v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05432v1">http://arxiv.org/abs/2511.05432v1</a><br>We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05449v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05449v1">http://arxiv.org/abs/2511.05449v1</a><br>Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at <a href="https://gitmerge3d.github.io/">https://gitmerge3d.github.io</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05461v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05461v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05461v1">http://arxiv.org/abs/2511.05461v1</a><br>Natural disasters demand rapid damage assessment to guide humanitarian response. Here, we investigate whether medium-resolution Earth observation images from the Copernicus program can support building damage assessment, complementing very-high resolution imagery with often limited availability. We introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the established xBD benchmark. In a series of experiments, we demonstrate that building damage can be detected and mapped rather well in many disaster scenarios, despite the moderate 10$,$m ground sampling distance. We also find that, for damage mapping at that resolution, architectural sophistication does not seem to bring much advantage: more complex model architectures tend to struggle with generalization to unseen disasters, and geospatial foundation models bring little practical benefit. Our results suggest that Copernicus images are a viable data source for rapid, wide-area damage assessment and could play an important role alongside VHR imagery. We release the xBD-S12 dataset, code, and trained models to support further research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Photo Dating by Facial Age Aggregation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05464v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05464v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05464v1">http://arxiv.org/abs/2511.05464v1</a><br>We introduce a novel method for Photo Dating which estimates the year a photograph was taken by leveraging information from the faces of people present in the image. To facilitate this research, we publicly release CSFD-1.6M, a new dataset containing over 1.6 million annotated faces, primarily from movie stills, with identity and birth year annotations. Uniquely, our dataset provides annotations for multiple individuals within a single image, enabling the study of multi-face information aggregation. We propose a probabilistic framework that formally combines visual evidence from modern face recognition and age estimation models, and career-based temporal priors to infer the photo capture year. Our experiments demonstrate that aggregating evidence from multiple faces consistently improves the performance and the approach significantly outperforms strong, scene-based baselines, particularly for images containing several identifiable individuals.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05467v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05467v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05467v1">http://arxiv.org/abs/2511.05467v1</a><br>Flow boiling is an efficient heat transfer mechanism capable of dissipating high heat loads with minimal temperature variation, making it an ideal thermal management method. However, sudden shifts between flow regimes can disrupt thermal performance and system reliability, highlighting the need for accurate and low-latency real-time monitoring. Conventional optical imaging methods are limited by high computational demands and insufficient temporal resolution, making them inadequate for capturing transient flow behavior. To address this, we propose a real-time framework based on signals from neuromorphic sensors for flow regime classification. Neuromorphic sensors detect changes in brightness at individual pixels, which typically correspond to motion at edges, enabling fast and efficient detection without full-frame reconstruction, providing event-based information. We develop five classification models using both traditional image data and event-based data, demonstrating that models leveraging event data outperform frame-based approaches due to their sensitivity to dynamic flow features. Among these models, the event-based long short-term memory model provides the best balance between accuracy and speed, achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our asynchronous processing pipeline supports continuous, low-latency predictions and delivers stable output through a majority voting mechanisms, enabling reliable real-time feedback for experimental control and intelligent thermal management.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>76T10, 68T07</tag>
      
      <tag>I.2.10; I.4.8; I.4.9</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05462v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05462v1">http://arxiv.org/abs/2511.05462v1</a><br>Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demonstrate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05474v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05474v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05474v1">http://arxiv.org/abs/2511.05474v1</a><br>This paper introduces a cutting-edge approach to cross-modal interaction for tiny object detection by combining semantic-guided natural language processing with advanced visual recognition backbones. The proposed method integrates the BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By employing lemmatization and fine-tuning techniques, the system aligns semantic cues from textual inputs with visual features, enhancing detection precision for small and complex objects. Experimental validation using the COCO and Objects365 datasets demonstrates that the model achieves superior performance. On the COCO2017 validation set, it attains a 52.6% average precision (AP), outperforming YOLO-World significantly while maintaining half the parameter consumption of Transformer-based models like GLIP. Several test on different of backbones such ELAN, MSP, and CSP further enable efficient handling of multi-scale objects, ensuring scalability and robustness in resource-constrained environments. This study underscores the potential of integrating natural language understanding with advanced backbone architectures, setting new benchmarks in object detection accuracy, efficiency, and adaptability to real-world challenges.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05477v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05477v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05477v1">http://arxiv.org/abs/2511.05477v1</a><br>Medical image segmentation requires models that are accurate, lightweight, and interpretable. Convolutional architectures lack adaptive nonlinearity and transparent decision-making, whereas Transformer architectures are hindered by quadratic complexity and opaque attention mechanisms. U-KAN addresses these challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than both convolutional and attention-based methods, fewer parameters than Transformer variants, and improved interpretability compared to conventional approaches. However, its O(C^2) complexity due to full-channel transformations limits its scalability as the number of channels increases. To overcome this, we introduce GroupKAN, a lightweight segmentation network that incorporates two novel, structured functional modules: (1) Grouped KAN Transform, which partitions channels into G groups for multivariate spline mappings, reducing complexity to O(C^2&#x2F;G), and (2) Grouped KAN Activation, which applies shared spline-based mappings within each channel group for efficient, token-wise nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC), GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11 percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M), and shows improved interpretability.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On Flow Matching KL Divergence</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05480v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05480v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05480v1">http://arxiv.org/abs/2511.05480v1</a><br>We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\epsilon^2 &gt; 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05489v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05489v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05489v1">http://arxiv.org/abs/2511.05489v1</a><br>Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at <a href="https://github.com/Time-Search/TimeSearch-R">https://github.com/Time-Search/TimeSearch-R</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Visual Spatial Tuning</title>
    <link href="/2025/11/07/cs.CV/2025-11-07-2511_05491v1/"/>
    <url>/2025/11/07/cs.CV/2025-11-07-2511_05491v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05491v1">http://arxiv.org/abs/2511.05491v1</a><br>Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including $34.8%$ on MMSI-Bench and $61.2%$ on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation</title>
    <link href="/2025/11/07/eess.IV/2025-11-07-2511_04892v1/"/>
    <url>/2025/11/07/eess.IV/2025-11-07-2511_04892v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04892v1">http://arxiv.org/abs/2511.04892v1</a><br>Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.BM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UHDRes: Ultra-High-Definition Image Restoration via Dual-Domain Decoupled Spectral Modulation</title>
    <link href="/2025/11/07/eess.IV/2025-11-07-2511_05009v1/"/>
    <url>/2025/11/07/eess.IV/2025-11-07-2511_05009v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05009v1">http://arxiv.org/abs/2511.05009v1</a><br>Ultra-high-definition (UHD) images often suffer from severe degradations such as blur, haze, rain, or low-light conditions, which pose significant challenges for image restoration due to their high resolution and computational demands. In this paper, we propose UHDRes, a novel lightweight dual-domain decoupled spectral modulation framework for UHD image restoration. It explicitly models the amplitude spectrum via lightweight spectrum-domain modulation, while restoring phase implicitly through spatial-domain refinement. We introduce the spatio-spectral fusion mechanism, which first employs a multi-scale context aggregator to extract local and global spatial features, and then performs spectral modulation in a decoupled manner. It explicitly enhances amplitude features in the frequency domain while implicitly restoring phase information through spatial refinement. Additionally, a shared gated feed-forward network is designed to efficiently promote feature interaction through shared-parameter convolutions and adaptive gating mechanisms. Extensive experimental comparisons on five public UHD benchmarks demonstrate that our UHDRes achieves the state-of-the-art restoration performance with only 400K parameters, while significantly reducing inference latency and memory usage. The codes and models are available at <a href="https://github.com/Zhao0100/UHDRes">https://github.com/Zhao0100/UHDRes</a>.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PySlyde: A Lightweight, Open-Source Toolkit for Pathology Preprocessing</title>
    <link href="/2025/11/07/eess.IV/2025-11-07-2511_05183v1/"/>
    <url>/2025/11/07/eess.IV/2025-11-07-2511_05183v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05183v1">http://arxiv.org/abs/2511.05183v1</a><br>The integration of artificial intelligence (AI) into pathology is advancing precision medicine by improving diagnosis, treatment planning, and patient outcomes. Digitised whole-slide images (WSIs) capture rich spatial and morphological information vital for understanding disease biology, yet their gigapixel scale and variability pose major challenges for standardisation and analysis. Robust preprocessing, covering tissue detection, tessellation, stain normalisation, and annotation parsing is critical but often limited by fragmented and inconsistent workflows. We present PySlyde, a lightweight, open-source Python toolkit built on OpenSlide to simplify and standardise WSI preprocessing. PySlyde provides an intuitive API for slide loading, annotation management, tissue detection, tiling, and feature extraction, compatible with modern pathology foundation models. By unifying these processes, it streamlines WSI preprocessing, enhances reproducibility, and accelerates the generation of AI-ready datasets, enabling researchers to focus on model development and downstream analysis.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection</title>
    <link href="/2025/11/07/eess.IV/2025-11-07-2511_05253v1/"/>
    <url>/2025/11/07/eess.IV/2025-11-07-2511_05253v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05253v1">http://arxiv.org/abs/2511.05253v1</a><br>Introduction: Accurate intraoperative delineation of colorectal liver metastases (CRLM) is crucial for achieving negative resection margins but remains challenging using intraoperative ultrasound (iUS) due to low contrast, noise, and operator dependency. Automated segmentation could enhance precision and efficiency in ultrasound-based navigation workflows.   Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two variants were compared: one trained on full iUS volumes and another on cropped regions around tumors. Segmentation accuracy was assessed using Dice Similarity Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference (RVD) on retrospective and prospective datasets. The workflow was integrated into 3D Slicer for real-time intraoperative use.   Results: The cropped-volume model significantly outperformed the full-volume model across all metrics (AUC-ROC &#x3D; 0.898 vs 0.718). It achieved median DSC &#x3D; 0.74, recall &#x3D; 0.79, and HDist. &#x3D; 17.1 mm comparable to semi-automatic segmentation but with <del>4x faster execution (</del> 1 min). Prospective intraoperative testing confirmed robust and consistent performance, with clinically acceptable accuracy for real-time surgical guidance.   Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net provides reliable, near real-time results with minimal operator input. The method enables efficient, registration-free ultrasound-based navigation for hepatic surgery, approaching expert-level accuracy while substantially reducing manual workload and procedure time.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Beta Distribution Learning for Reliable Roadway Crash Risk Assessment</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04886v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04886v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04886v1">http://arxiv.org/abs/2511.04886v1</a><br>Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions–a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04892v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04892v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04892v1">http://arxiv.org/abs/2511.04892v1</a><br>Nuclei segmentation is the cornerstone task in histology image reading, shedding light on the underlying molecular patterns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physicians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are expensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing operations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses { no manually annotated training data} or domain adaptation, it maintains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly supervised methods while having a competitive standing among fully supervised methods. Remarkably, every module within LG-NuSegHop is transparent and explainable to physicians.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.BM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04920v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04920v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04920v1">http://arxiv.org/abs/2511.04920v1</a><br>Image restoration (IR) aims to recover clean images from degraded observations. Despite remarkable progress, most existing methods focus on a single degradation type, whereas real-world images often suffer from multiple coexisting degradations, such as rain, noise, and haze coexisting in a single image, which limits their practical effectiveness. In this paper, we propose an adaptive multi-degradation image restoration network that reconstructs images by leveraging decoupled representations of degradation ingredients to guide path selection. Specifically, we design a degradation ingredient decoupling block (DIDBlock) in the encoder to separate degradation ingredients statistically by integrating spatial and frequency domain information, enhancing the recognition of multiple degradation types and making their feature representations independent. In addition, we present fusion block (FBlock) to integrate degradation information across all levels using learnable matrices. In the decoder, we further introduce a task adaptation block (TABlock) that dynamically activates or fuses functional branches based on the multi-degradation representation, flexibly selecting optimal restoration paths under diverse degradation conditions. The resulting tightly integrated architecture, termed IMDNet, is extensively validated through experiments, showing superior performance on multi-degradation restoration while maintaining strong competitiveness on single-degradation tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A benchmark multimodal oro-dental dataset for large vision-language models</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04948v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04948v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04948v1">http://arxiv.org/abs/2511.04948v1</a><br>The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04949v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04949v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04949v1">http://arxiv.org/abs/2511.04949v1</a><br>Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04951v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04951v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04951v1">http://arxiv.org/abs/2511.04951v1</a><br>3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU’s memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS’s memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>D.4; I.3.2; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Fourier shapes to probe the geometric world of deep neural networks</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04970v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04970v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04970v1">http://arxiv.org/abs/2511.04970v1</a><br>While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model’s salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04963v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04963v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04963v1">http://arxiv.org/abs/2511.04963v1</a><br>Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time&#x2F;gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR&#x2F;SSIM scores of 29.83 dB&#x2F;90.84% for fMRI synthesis (+1.54 dB&#x2F;+4.12% over baselines) and 30.00 dB&#x2F;77.55% for dMRI synthesis (+1.02 dB&#x2F;+2.2%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92%&#x2F;66.02%&#x2F;64.15% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \href{<a href="https://github.com/SXR3015/PDS%7D%7BPDS">https://github.com/SXR3015/PDS}{PDS</a> GitHub Repository}</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04972v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04972v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04972v1">http://arxiv.org/abs/2511.04972v1</a><br>Topological Data Analysis (TDA) involves techniques of analyzing the underlying structure and connectivity of data. However, traditional methods like persistent homology can be computationally demanding, motivating the development of neural network-based estimators capable of reducing computational overhead and inference time. A key barrier to advancing these methods is the lack of labeled 3D data with class distributions and diversity tailored specifically for supervised learning in TDA tasks. To address this, we introduce a novel approach for systematically generating labeled 3D datasets using the Repulsive Surface algorithm, allowing control over topological invariants, such as hole count. The resulting dataset offers varied geometry with topological labeling, making it suitable for training and benchmarking neural network estimators. This paper uses a synthetic 3D dataset to train a genus estimator network, created using a 3D convolutional transformer architecture. An observed decrease in accuracy as deformations increase highlights the role of not just topological complexity, but also geometric complexity, when training generalized estimators. This dataset fills a gap in labeled 3D datasets and generation for training and evaluating models and techniques for TDA.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_04977v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_04977v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04977v1">http://arxiv.org/abs/2511.04977v1</a><br>Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UHDRes: Ultra-High-Definition Image Restoration via Dual-Domain Decoupled Spectral Modulation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05009v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05009v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05009v1">http://arxiv.org/abs/2511.05009v1</a><br>Ultra-high-definition (UHD) images often suffer from severe degradations such as blur, haze, rain, or low-light conditions, which pose significant challenges for image restoration due to their high resolution and computational demands. In this paper, we propose UHDRes, a novel lightweight dual-domain decoupled spectral modulation framework for UHD image restoration. It explicitly models the amplitude spectrum via lightweight spectrum-domain modulation, while restoring phase implicitly through spatial-domain refinement. We introduce the spatio-spectral fusion mechanism, which first employs a multi-scale context aggregator to extract local and global spatial features, and then performs spectral modulation in a decoupled manner. It explicitly enhances amplitude features in the frequency domain while implicitly restoring phase information through spatial refinement. Additionally, a shared gated feed-forward network is designed to efficiently promote feature interaction through shared-parameter convolutions and adaptive gating mechanisms. Extensive experimental comparisons on five public UHD benchmarks demonstrate that our UHDRes achieves the state-of-the-art restoration performance with only 400K parameters, while significantly reducing inference latency and memory usage. The codes and models are available at <a href="https://github.com/Zhao0100/UHDRes">https://github.com/Zhao0100/UHDRes</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05017v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05017v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05017v1">http://arxiv.org/abs/2511.05017v1</a><br>In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations – and to show that refining textual embeddings with visual information mitigates this issue – we leave exploration of advanced fusion strategies for future work.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05020v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05020v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05020v1">http://arxiv.org/abs/2511.05020v1</a><br>Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text. Most existing methods rely on a single model to perform feature fusion and similarity matching. However, this paradigm faces two major challenges. First, one model alone can’t see the whole picture and the tiny details at the same time; it has to handle different tasks with the same weights, so it often misses the small but important links between image and text. Second, the absence of dynamic weight allocation prevents adaptive leveraging of complementary model strengths, so the resulting embedding drifts away from the target and misleads the nearest-neighbor search in CIR. To address these limitations, we propose Dynamic Adaptive Fusion (DAFM) for multi-model collaboration in CIR. Rather than optimizing a single method in isolation, DAFM exploits the complementary strengths of heterogeneous models and adaptively rebalances their contributions. This not only maximizes retrieval accuracy but also ensures that the performance gains are independent of the fusion order, highlighting the robustness of our approach. Experiments on the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our method achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an average Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up to 4.5%. These results confirm that dynamic multi-model collaboration provides an effective and general solution for CIR.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05034v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05034v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05034v1">http://arxiv.org/abs/2511.05034v1</a><br>Whole Slide Image (WSI) representation is critical for cancer subtyping, cancer recognition and mutation prediction.Training an end-to-end WSI representation model poses significant challenges, as a standard gigapixel slide can contain tens of thousands of image tiles, making it difficult to compute gradients of all tiles in a single mini-batch due to current GPU limitations. To address this challenge, we propose a method of dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI representation. Our approach utilizes a memory bank to store the features of tiles across all WSIs in the dataset. During training, a mini-batch usually contains multiple WSIs. For each WSI in the batch, a subset of tiles is randomly sampled and their features are computed using a tile encoder. Then, additional tile features from the same WSI are selected from the memory bank. The representation of each individual WSI is generated using a residual encoding technique that incorporates both the sampled features and those retrieved from the memory bank. Finally, the slide-level contrastive loss is computed based on the representations and histopathology reports ofthe WSIs within the mini-batch. Experiments conducted over cancer subtyping, cancer recognition, and mutation prediction tasks proved the effectiveness of the proposed DRE-SLCL method.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.4.9; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05038v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05038v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05038v1">http://arxiv.org/abs/2511.05038v1</a><br>We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05055v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05055v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05055v1">http://arxiv.org/abs/2511.05055v1</a><br>Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05057v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05057v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05057v1">http://arxiv.org/abs/2511.05057v1</a><br>The effectiveness of Contrastive Language-Image Pre-training (CLIP) models critically depends on the semantic diversity and quality of their training data. However, while existing synthetic data generation methods primarily focus on increasing data volume, such emphasis often leads to limited semantic diversity and redundant or shallow captions. To address this limitation, we propose Role-SynthCLIP, a novel data synthesis framework that leverages multi-perspective role-playing prompts (e.g., a compositional analyst, an interpreter of image context) to guide Multimodal Large Language Models (MLLMs) in generating semantically diverse captions from distinct viewpoints. This mechanism enhances the semantic diversity and fine-grained image-text alignment of synthetic pairs, thereby improving caption expressiveness and accuracy while keeping the total number of image-text pairs unchanged. Experimental results demonstrate the effectiveness and efficiency of our method. A CLIP-B&#x2F;16 model trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on the MS COCO validation set, surpassing the best existing synthetic data baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained models are released at <a href="https://github.com/huangfu170/Role-SynthCLIP">https://github.com/huangfu170/Role-SynthCLIP</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05059v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05059v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05059v1">http://arxiv.org/abs/2511.05059v1</a><br>During laparoscopic surgery, smoke generated by tissue cauterization can significantly degrade the visual quality of endoscopic frames, increasing the risk of surgical errors and hindering both clinical decision-making and computer-assisted visual analysis. Consequently, removing surgical smoke is critical to ensuring patient safety and maintaining operative efficiency. In this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical smoke removal. SurgiATM statistically bridges a physics-based atmospheric model and data-driven deep learning models, combining the superior generalizability of the former with the high accuracy of the latter. Furthermore, SurgiATM is designed as a lightweight, plug-and-play module that can be seamlessly integrated into diverse surgical desmoking architectures to enhance their accuracy and stability, better meeting clinical requirements. It introduces only two hyperparameters and no additional trainable weights, preserving the original network architecture with minimal computational and modification overhead. We conduct extensive experiments on three public surgical datasets with ten desmoking methods, involving multiple network architectures and covering diverse procedures, including cholecystectomy, partial nephrectomy, and diaphragm dissection. The results demonstrate that incorporating SurgiATM commonly reduces the restoration errors of existing models and relatively enhances their generalizability, without adding any trainable layers or weights. This highlights the convenience, low cost, effectiveness, and generalizability of the proposed method. The code for SurgiATM is released at <a href="https://github.com/MingyuShengSMY/SurgiATM">https://github.com/MingyuShengSMY/SurgiATM</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05092v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05092v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05092v1">http://arxiv.org/abs/2511.05092v1</a><br>With growing concerns over data privacy, researchers have started using virtual data as an alternative to sensitive real-world images for training person re-identification (Re-ID) models. However, existing virtual datasets produced by game engines still face challenges such as complex construction and poor domain generalization, making them difficult to apply in real scenarios. To address these challenges, we propose a Dual-stage Prompt-driven Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich prompts incorporating multi-dimensional attributes such as pedestrian appearance, illumination, and viewpoint that drive the diffusion model to synthesize diverse data end-to-end, building a large-scale virtual dataset named GenePerson with 130,519 images of 6,641 identities. In the second stage, we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn domain-invariant generalization features. With the aid of contrastive learning, we employ two textual inversion networks to map images into pseudo-words representing style and content, respectively, thereby constructing style-disentangled content prompts to guide the model in learning domain-invariant content features at the image level. Experiments demonstrate that models trained on GenePerson with PDM achieve state-of-the-art generalization performance, surpassing those on popular real and virtual Re-ID datasets.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05073v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05073v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05073v1">http://arxiv.org/abs/2511.05073v1</a><br>Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05095v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05095v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05095v1">http://arxiv.org/abs/2511.05095v1</a><br>Adverse weather severely impairs real-world visual perception, while existing vision models trained on synthetic data with fixed parameters struggle to generalize to complex degradations. To address this, we first construct HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse weather phenomena, and then design a dual-level reinforcement learning framework initialized with HFLS-Weather for cold-start training. Within this framework, at the local level, weather-specific restoration models are refined through perturbation-driven image quality optimization, enabling reward-based learning without paired supervision; at the global level, a meta-controller dynamically orchestrates model selection and execution order according to scene degradation. This framework enables continuous adaptation to real-world conditions and achieves state-of-the-art performance across a wide range of adverse weather scenarios. Code is available at <a href="https://github.com/xxclfy/AgentRL-Real-Weather">https://github.com/xxclfy/AgentRL-Real-Weather</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Quantifying the Risk of Transferred Black Box Attacks</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05102v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05102v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05102v1">http://arxiv.org/abs/2511.05102v1</a><br>Neural networks have become pervasive across various applications, including security-related products. However, their widespread adoption has heightened concerns regarding vulnerability to adversarial attacks. With emerging regulations and standards emphasizing security, organizations must reliably quantify risks associated with these attacks, particularly regarding transferred adversarial attacks, which remain challenging to evaluate accurately. This paper investigates the complexities involved in resilience testing against transferred adversarial attacks. Our analysis specifically addresses black-box evasion attacks, highlighting transfer-based attacks due to their practical significance and typically high transferability between neural network models. We underline the computational infeasibility of exhaustively exploring high-dimensional input spaces to achieve complete test coverage. As a result, comprehensive adversarial risk mapping is deemed impractical. To mitigate this limitation, we propose a targeted resilience testing framework that employs surrogate models strategically selected based on Centered Kernel Alignment (CKA) similarity. By leveraging surrogate models exhibiting both high and low CKA similarities relative to the target model, the proposed approach seeks to optimize coverage of adversarial subspaces. Risk estimation is conducted using regression-based estimators, providing organizations with realistic and actionable risk quantification.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Early Alzheimer&#39;s Disease Detection from Retinal OCT Images: A UK Biobank Study</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05106v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05106v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05106v1">http://arxiv.org/abs/2511.05106v1</a><br>Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer’s disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05108v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05108v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05108v1">http://arxiv.org/abs/2511.05108v1</a><br>Lane detection for autonomous driving in snow-covered environments remains a major challenge due to the frequent absence or occlusion of lane markings. In this paper, we present a novel, robust and realtime capable approach that bypasses the reliance on traditional lane markings by detecting roadside features,specifically vertical roadside posts called delineators, as indirect lane indicators. Our method first perceives these posts, then fits a smooth lane trajectory using a parameterized Bezier curve model, leveraging spatial consistency and road geometry. To support training and evaluation in these challenging scenarios, we introduce SnowyLane, a new synthetic dataset containing 80,000 annotated frames capture winter driving conditions, with varying snow coverage, and lighting conditions. Compared to state-of-the-art lane detection systems, our approach demonstrates significantly improved robustness in adverse weather, particularly in cases with heavy snow occlusion. This work establishes a strong foundation for reliable lane detection in winter scenarios and contributes a valuable resource for future research in all-weather autonomous driving. The dataset is available at <a href="https://ekut-es.github.io/snowy-lane">https://ekut-es.github.io/snowy-lane</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05150v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05150v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05150v1">http://arxiv.org/abs/2511.05150v1</a><br>AI-based biomarkers can infer molecular features directly from hematoxylin &amp; eosin (H&amp;E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Medical Referring Image Segmentation via Next-Token Mask Prediction</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05044v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05044v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05044v1">http://arxiv.org/abs/2511.05044v1</a><br>Medical Referring Image Segmentation (MRIS) involves segmenting target regions in medical images based on natural language descriptions. While achieving promising results, recent approaches usually involve complex design of multimodal fusion or multi-stage decoders. In this work, we propose NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive next-token prediction task over a unified multimodal sequence of tokenized image, text, and mask representations. This formulation streamlines model design by eliminating the need for modality-specific fusion and external segmentation models, supports a unified architecture for end-to-end training. It also enables the use of pretrained tokenizers from emerging large-scale multimodal models, enhancing generalization and adaptability. More importantly, to address challenges under this formulation-such as exposure bias, long-tail token distributions, and fine-grained lesion edges-we propose three novel strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance boundary sensitivity and mitigate long-tail distribution effects, and (3) a memory-based Hard Error Token (HET) optimization strategy that emphasizes difficult tokens during training. Extensive experiments on the QaTa-COV19 and MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art performance, offering a streamlined and effective alternative to traditional MRIS pipelines.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05152v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05152v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05152v1">http://arxiv.org/abs/2511.05152v1</a><br>Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t&#x3D;0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. Code and video comparisons are available online: <a href="https://interims-git.github.io/">https://interims-git.github.io/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Another BRIXEL in the Wall: Towards Cheaper Dense Features</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05168v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05168v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05168v1">http://arxiv.org/abs/2511.05168v1</a><br>Vision foundation models achieve strong performance on both global and locally dense downstream tasks. Pretrained on large images, the recent DINOv3 model family is able to produce very fine-grained dense feature maps, enabling state-of-the-art performance. However, computing these feature maps requires the input image to be available at very high resolution, as well as large amounts of compute due to the squared complexity of the transformer architecture. To address these issues, we propose BRIXEL, a simple knowledge distillation approach that has the student learn to reproduce its own feature maps at higher resolution. Despite its simplicity, BRIXEL outperforms the baseline DINOv3 models by large margins on downstream tasks when the resolution is kept fixed. Moreover, it is able to produce feature maps that are very similar to those of the teacher at a fraction of the computational cost. Code and model weights are available at <a href="https://github.com/alexanderlappe/BRIXEL">https://github.com/alexanderlappe/BRIXEL</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05170v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05170v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05170v1">http://arxiv.org/abs/2511.05170v1</a><br>Nucleus detection and classification (NDC) in histopathology analysis is a fundamental task that underpins a wide range of high-level pathology applications. However, existing methods heavily rely on labor-intensive nucleus-level annotations and struggle to fully exploit large-scale unlabeled data for learning discriminative nucleus representations. In this work, we propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised learning method tailored for NDC. At its core is NuLo (Nucleus-based Local self-distillation), a coordinate-guided mechanism that enables flexible local self-distillation based on predicted nucleus positions. By removing the need for strict spatial alignment between augmented views, NuLo allows critical cross-scale alignment, thus unlocking the capacity of models for fine-grained nucleus-level representation. To support MUSE, we design a simple yet effective encoder-decoder architecture and a large field-of-view semi-supervised fine-tuning strategy that together maximize the value of unlabeled pathology images. Extensive experiments on three widely used benchmarks demonstrate that MUSE effectively addresses the core challenges of histopathological NDC. The resulting models not only surpass state-of-the-art supervised baselines but also outperform generic pathology foundation models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Walk the Lines 2: Contour Tracking for Detailed Segmentation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05210v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05210v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05210v1">http://arxiv.org/abs/2511.05210v1</a><br>This paper presents Walk the Lines 2 (WtL2), a unique contour tracking algorithm specifically adapted for detailed segmentation of infrared (IR) ships and various objects in RGB.1 This extends the original Walk the Lines (WtL) [12], which focused solely on detailed ship segmentation in color. These innovative WtLs can replace the standard non-maximum suppression (NMS) by using contour tracking to refine the object contour until a 1-pixel-wide closed shape can be binarized, forming a segmentable area in foreground-background scenarios. WtL2 broadens the application range of WtL beyond its original scope, adapting to IR and expanding to diverse objects within the RGB context. To achieve IR segmentation, we adapt its input, the object contour detector, to IR ships. In addition, the algorithm is enhanced to process a wide range of RGB objects, outperforming the latest generation of contour-based methods when achieving a closed object contour, offering high peak Intersection over Union (IoU) with impressive details. This positions WtL2 as a compelling method for specialized applications that require detailed segmentation or high-quality samples, potentially accelerating progress in several niche areas of image segmentation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05219v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05219v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05219v1">http://arxiv.org/abs/2511.05219v1</a><br>Controlling the spatial and semantic structure of diffusion-generated images remains a challenge. Existing methods like ControlNet rely on handcrafted condition maps and retraining, limiting flexibility and generalization. Inversion-based approaches offer stronger alignment but incur high inference cost due to dual-path denoising. We present FreeControl, a training-free framework for semantic structural control in diffusion models. Unlike prior methods that extract attention across multiple timesteps, FreeControl performs one-step attention extraction from a single, optimally chosen key timestep and reuses it throughout denoising. This enables efficient structural guidance without inversion or retraining. To further improve quality and stability, we introduce Latent-Condition Decoupling (LCD): a principled separation of the key timestep and the noised latent used in attention extraction. LCD provides finer control over attention quality and eliminates structural artifacts. FreeControl also supports compositional control via reference images assembled from multiple sources - enabling intuitive scene layout design and stronger prompt alignment. FreeControl introduces a new paradigm for test-time control, enabling structurally and semantically aligned, visually coherent generation directly from raw images, with the flexibility for intuitive compositional design and compatibility with modern diffusion models at approximately 5 percent additional cost.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PySlyde: A Lightweight, Open-Source Toolkit for Pathology Preprocessing</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05183v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05183v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05183v1">http://arxiv.org/abs/2511.05183v1</a><br>The integration of artificial intelligence (AI) into pathology is advancing precision medicine by improving diagnosis, treatment planning, and patient outcomes. Digitised whole-slide images (WSIs) capture rich spatial and morphological information vital for understanding disease biology, yet their gigapixel scale and variability pose major challenges for standardisation and analysis. Robust preprocessing, covering tissue detection, tessellation, stain normalisation, and annotation parsing is critical but often limited by fragmented and inconsistent workflows. We present PySlyde, a lightweight, open-source Python toolkit built on OpenSlide to simplify and standardise WSI preprocessing. PySlyde provides an intuitive API for slide loading, annotation management, tissue detection, tiling, and feature extraction, compatible with modern pathology foundation models. By unifying these processes, it streamlines WSI preprocessing, enhances reproducibility, and accelerates the generation of AI-ready datasets, enabling researchers to focus on model development and downstream analysis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05229v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05229v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05229v1">http://arxiv.org/abs/2511.05229v1</a><br>Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05245v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05245v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05245v1">http://arxiv.org/abs/2511.05245v1</a><br>The current mainstream and state-of-the-art anomaly detection (AD) methods are substantially established on pretrained feature networks yielded by ImageNet pretraining. However, regardless of supervised or self-supervised pretraining, the pretraining process on ImageNet does not match the goal of anomaly detection (i.e., pretraining in natural images doesn’t aim to distinguish between normal and abnormal). Moreover, natural images and industrial image data in AD scenarios typically have the distribution shift. The two issues can cause ImageNet-pretrained features to be suboptimal for AD tasks. To further promote the development of the AD field, pretrained representations specially for AD tasks are eager and very valuable. To this end, we propose a novel AD representation learning framework specially designed for learning robust and discriminative pretrained representations for industrial anomaly detection. Specifically, closely surrounding the goal of anomaly detection (i.e., focus on discrepancies between normals and anomalies), we propose angle- and norm-oriented contrastive losses to maximize the angle size and norm difference between normal and abnormal features simultaneously. To avoid the distribution shift from natural images to AD images, our pretraining is performed on a large-scale AD dataset, RealIAD. To further alleviate the potential shift between pretraining data and downstream AD datasets, we learn the pretrained AD representations based on the class-generalizable representation, residual features. For evaluation, based on five embedding-based AD methods, we simply replace their original features with our pretrained representations. Extensive experiments on five AD datasets and five backbones consistently show the superiority of our pretrained features. The code is available at <a href="https://github.com/xcyao00/ADPretrain">https://github.com/xcyao00/ADPretrain</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05253v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05253v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05253v1">http://arxiv.org/abs/2511.05253v1</a><br>Introduction: Accurate intraoperative delineation of colorectal liver metastases (CRLM) is crucial for achieving negative resection margins but remains challenging using intraoperative ultrasound (iUS) due to low contrast, noise, and operator dependency. Automated segmentation could enhance precision and efficiency in ultrasound-based navigation workflows.   Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two variants were compared: one trained on full iUS volumes and another on cropped regions around tumors. Segmentation accuracy was assessed using Dice Similarity Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference (RVD) on retrospective and prospective datasets. The workflow was integrated into 3D Slicer for real-time intraoperative use.   Results: The cropped-volume model significantly outperformed the full-volume model across all metrics (AUC-ROC &#x3D; 0.898 vs 0.718). It achieved median DSC &#x3D; 0.74, recall &#x3D; 0.79, and HDist. &#x3D; 17.1 mm comparable to semi-automatic segmentation but with <del>4x faster execution (</del> 1 min). Prospective intraoperative testing confirmed robust and consistent performance, with clinically acceptable accuracy for real-time surgical guidance.   Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net provides reliable, near real-time results with minimal operator input. The method enables efficient, registration-free ultrasound-based navigation for hepatic surgery, approaching expert-level accuracy while substantially reducing manual workload and procedure time.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05263v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05263v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05263v1">http://arxiv.org/abs/2511.05263v1</a><br>The analysis of character appearance frequency is essential for understanding narrative structure, character prominence, and story progression in anime. In this work, we introduce OregairuChar, a benchmark dataset designed for appearance frequency analysis in the anime series My Teen Romantic Comedy SNAFU. The dataset comprises 1600 manually selected frames from the third season, annotated with 2860 bounding boxes across 11 main characters. OregairuChar captures diverse visual challenges, including occlusion, pose variation, and inter-character similarity, providing a realistic basis for appearance-based studies. To enable quantitative research, we benchmark several object detection models on the dataset and leverage their predictions for fine-grained, episode-level analysis of character presence over time. This approach reveals patterns of character prominence and their evolution within the narrative. By emphasizing appearance frequency, OregairuChar serves as a valuable resource for exploring computational narrative dynamics and character-centric storytelling in stylized media.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DeepEyesV2: Toward Agentic Multimodal Model</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05271v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05271v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05271v1">http://arxiv.org/abs/2511.05271v1</a><br>Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>What&#39;s on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05292v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05292v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05292v1">http://arxiv.org/abs/2511.05292v1</a><br>Accurate food intake detection is vital for dietary monitoring and chronic disease prevention. Traditional self-report methods are prone to recall bias, while camera-based approaches raise concerns about privacy. Furthermore, existing wearable-based methods primarily focus on a limited number of food types, such as hamburgers and pizza, failing to address the vast diversity of Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that classifies Chinese food types by integrating hand motion cues from a smartwatch with head dynamics from smart glasses. To filter out irrelevant daily activities, we design a two-stage detection pipeline. The first stage identifies eating states by distinguishing characteristic temporal patterns from non-eating behaviors. The second stage then conducts fine-grained food type recognition based on the motions captured during food intake. To evaluate CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings across 11 food categories and 10 participants. Experiments demonstrate that CuisineSense achieves high accuracy in both eating state detection and food classification, offering a practical solution for unobtrusive, wearable-based dietary monitoring.The system code is publicly available at <a href="https://github.com/joeeeeyin/CuisineSense.git">https://github.com/joeeeeyin/CuisineSense.git</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05299v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05299v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05299v1">http://arxiv.org/abs/2511.05299v1</a><br>Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar’s state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at <a href="https://github.com/yzy-bupt/LiveStar">https://github.com/yzy-bupt/LiveStar</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cross-domain EEG-based Emotion Recognition with Contrastive Learning</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05293v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05293v1">http://arxiv.org/abs/2511.05293v1</a><br>Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05308v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05308v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05308v1">http://arxiv.org/abs/2511.05308v1</a><br>As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at <a href="https://github.com/matteo-bastico/DiffusionPointTransformer">https://github.com/matteo-bastico/DiffusionPointTransformer</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$&#92;mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05319v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05319v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05319v1">http://arxiv.org/abs/2511.05319v1</a><br>Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers. However, in the era of AIGC, the capacity of steganography is more critical than ever. In this work, we present Sentence-to-Image Steganography, an instance of Semantic Steganography, a novel task that enables the hiding of arbitrary sentence-level messages within a cover image. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages for evaluation. Finally, we present $\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large language models (LLMs) to embed high-level textual information, such as sentences or even paragraphs, into images. Unlike traditional bit-level counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich content through a newly designed pipeline in which the LLM is involved throughout the entire process. Both quantitative and qualitative experiments demonstrate that our method effectively unlocks new semantic steganographic capabilities for LLMs. The source code will be released soon.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05356v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05356v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05356v1">http://arxiv.org/abs/2511.05356v1</a><br>Articulated object perception presents significant challenges in computer vision, particularly because most existing methods ignore temporal dynamics despite the inherently dynamic nature of such objects. The use of 4D temporal data has not been thoroughly explored in articulated object perception and remains unexamined for panoptic segmentation. The lack of a benchmark dataset further hurt this field. To this end, we introduce Artic4D as a new dataset derived from PartNet Mobility and augmented with synthetic sensor data, featuring 4D panoptic annotations and articulation parameters. Building on this dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework. This approach explicitly estimates per-frame offsets mapping observed object parts to a learned canonical space, thereby enhancing part-level segmentation. The framework employs this canonical representation to achieve consistent alignment of object parts across sequential frames. Comprehensive experiments on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the art approaches in panoptic segmentation accuracy in more complex scenarios. These findings highlight the effectiveness of temporal modeling and canonical alignment in dynamic object understanding, and pave the way for future advances in 4D articulated object perception.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.2.10; I.4.6; I.5.1; I.5.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neural Image Abstraction Using Long Smoothing B-Splines</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05360v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05360v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05360v1">http://arxiv.org/abs/2511.05360v1</a><br>We integrate smoothing B-splines into a standard differentiable vector graphics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vectorization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dense Motion Captioning</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05369v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05369v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05369v1">http://arxiv.org/abs/2511.05369v1</a><br>Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.2.10; I.4.8; I.5.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05393v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05393v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05393v1">http://arxiv.org/abs/2511.05393v1</a><br>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05397v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05397v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05397v1">http://arxiv.org/abs/2511.05397v1</a><br>While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: <a href="https://everydayvla.github.io/">https://everydayvla.github.io/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05394v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05394v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05394v1">http://arxiv.org/abs/2511.05394v1</a><br>We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
      <tag>H.5.2; H.5.1; I.4.8; I.2.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05250v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05250v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05250v1">http://arxiv.org/abs/2511.05250v1</a><br>Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05403v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05403v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05403v1">http://arxiv.org/abs/2511.05403v1</a><br>The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM’s scale and diversity make it a valuable real-world resource for hand modeling and related research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05432v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05432v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05432v1">http://arxiv.org/abs/2511.05432v1</a><br>We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05404v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05404v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05404v1">http://arxiv.org/abs/2511.05404v1</a><br>Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at github.com&#x2F;DLR-RM&#x2F;MPRF.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.9; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05421v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05421v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05421v1">http://arxiv.org/abs/2511.05421v1</a><br>Continual learning is an emerging topic in the field of deep learning, where a model is expected to learn continuously for new upcoming tasks without forgetting previous experiences. This field has witnessed numerous advancements, but few works have been attempted in the direction of image restoration. Handling large image sizes and the divergent nature of various degradation poses a unique challenge in the restoration domain. However, existing works require heavily engineered architectural modifications for new task adaptation, resulting in significant computational overhead. Regularization-based methods are unsuitable for restoration, as different restoration challenges require different kinds of feature processing. In this direction, we propose a simple modification of the convolution layer to adapt the knowledge from previous restoration tasks without touching the main backbone architecture. Therefore, it can be seamlessly applied to any deep architecture without any structural modifications. Unlike other approaches, we demonstrate that our model can increase the number of trainable parameters without significantly increasing computational overhead or inference time. Experimental validation demonstrates that new restoration tasks can be introduced without compromising the performance of existing tasks. We also show that performance on new restoration tasks improves by adapting the knowledge from the knowledge base created by previous restoration tasks. The code is available at <a href="https://github.com/aupendu/continual-restore">https://github.com/aupendu/continual-restore</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05449v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05449v1">http://arxiv.org/abs/2511.05449v1</a><br>Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at <a href="https://gitmerge3d.github.io/">https://gitmerge3d.github.io</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05461v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05461v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05461v1">http://arxiv.org/abs/2511.05461v1</a><br>Natural disasters demand rapid damage assessment to guide humanitarian response. Here, we investigate whether medium-resolution Earth observation images from the Copernicus program can support building damage assessment, complementing very-high resolution imagery with often limited availability. We introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the established xBD benchmark. In a series of experiments, we demonstrate that building damage can be detected and mapped rather well in many disaster scenarios, despite the moderate 10$,$m ground sampling distance. We also find that, for damage mapping at that resolution, architectural sophistication does not seem to bring much advantage: more complex model architectures tend to struggle with generalization to unseen disasters, and geospatial foundation models bring little practical benefit. Our results suggest that Copernicus images are a viable data source for rapid, wide-area damage assessment and could play an important role alongside VHR imagery. We release the xBD-S12 dataset, code, and trained models to support further research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05462v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05462v1">http://arxiv.org/abs/2511.05462v1</a><br>Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demonstrate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Photo Dating by Facial Age Aggregation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05464v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05464v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05464v1">http://arxiv.org/abs/2511.05464v1</a><br>We introduce a novel method for Photo Dating which estimates the year a photograph was taken by leveraging information from the faces of people present in the image. To facilitate this research, we publicly release CSFD-1.6M, a new dataset containing over 1.6 million annotated faces, primarily from movie stills, with identity and birth year annotations. Uniquely, our dataset provides annotations for multiple individuals within a single image, enabling the study of multi-face information aggregation. We propose a probabilistic framework that formally combines visual evidence from modern face recognition and age estimation models, and career-based temporal priors to infer the photo capture year. Our experiments demonstrate that aggregating evidence from multiple faces consistently improves the performance and the approach significantly outperforms strong, scene-based baselines, particularly for images containing several identifiable individuals.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05467v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05467v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05467v1">http://arxiv.org/abs/2511.05467v1</a><br>Flow boiling is an efficient heat transfer mechanism capable of dissipating high heat loads with minimal temperature variation, making it an ideal thermal management method. However, sudden shifts between flow regimes can disrupt thermal performance and system reliability, highlighting the need for accurate and low-latency real-time monitoring. Conventional optical imaging methods are limited by high computational demands and insufficient temporal resolution, making them inadequate for capturing transient flow behavior. To address this, we propose a real-time framework based on signals from neuromorphic sensors for flow regime classification. Neuromorphic sensors detect changes in brightness at individual pixels, which typically correspond to motion at edges, enabling fast and efficient detection without full-frame reconstruction, providing event-based information. We develop five classification models using both traditional image data and event-based data, demonstrating that models leveraging event data outperform frame-based approaches due to their sensitivity to dynamic flow features. Among these models, the event-based long short-term memory model provides the best balance between accuracy and speed, achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our asynchronous processing pipeline supports continuous, low-latency predictions and delivers stable output through a majority voting mechanisms, enabling reliable real-time feedback for experimental control and intelligent thermal management.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>76T10, 68T07</tag>
      
      <tag>I.2.10; I.4.8; I.4.9</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05474v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05474v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05474v1">http://arxiv.org/abs/2511.05474v1</a><br>This paper introduces a cutting-edge approach to cross-modal interaction for tiny object detection by combining semantic-guided natural language processing with advanced visual recognition backbones. The proposed method integrates the BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By employing lemmatization and fine-tuning techniques, the system aligns semantic cues from textual inputs with visual features, enhancing detection precision for small and complex objects. Experimental validation using the COCO and Objects365 datasets demonstrates that the model achieves superior performance. On the COCO2017 validation set, it attains a 52.6% average precision (AP), outperforming YOLO-World significantly while maintaining half the parameter consumption of Transformer-based models like GLIP. Several test on different of backbones such ELAN, MSP, and CSP further enable efficient handling of multi-scale objects, ensuring scalability and robustness in resource-constrained environments. This study underscores the potential of integrating natural language understanding with advanced backbone architectures, setting new benchmarks in object detection accuracy, efficiency, and adaptability to real-world challenges.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05477v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05477v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05477v1">http://arxiv.org/abs/2511.05477v1</a><br>Medical image segmentation requires models that are accurate, lightweight, and interpretable. Convolutional architectures lack adaptive nonlinearity and transparent decision-making, whereas Transformer architectures are hindered by quadratic complexity and opaque attention mechanisms. U-KAN addresses these challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than both convolutional and attention-based methods, fewer parameters than Transformer variants, and improved interpretability compared to conventional approaches. However, its O(C^2) complexity due to full-channel transformations limits its scalability as the number of channels increases. To overcome this, we introduce GroupKAN, a lightweight segmentation network that incorporates two novel, structured functional modules: (1) Grouped KAN Transform, which partitions channels into G groups for multivariate spline mappings, reducing complexity to O(C^2&#x2F;G), and (2) Grouped KAN Activation, which applies shared spline-based mappings within each channel group for efficient, token-wise nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC), GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11 percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M), and shows improved interpretability.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On Flow Matching KL Divergence</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05480v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05480v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05480v1">http://arxiv.org/abs/2511.05480v1</a><br>We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\epsilon^2 &gt; 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05489v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05489v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05489v1">http://arxiv.org/abs/2511.05489v1</a><br>Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at <a href="https://github.com/Time-Search/TimeSearch-R">https://github.com/Time-Search/TimeSearch-R</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Visual Spatial Tuning</title>
    <link href="/2025/11/07/highlights/2025-11-07-2511_05491v1/"/>
    <url>/2025/11/07/highlights/2025-11-07-2511_05491v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.05491v1">http://arxiv.org/abs/2511.05491v1</a><br>Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including $34.8%$ on MMSI-Bench and $61.2%$ on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Nemotron Nano V2 VL</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_03929v2/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_03929v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03929v2">http://arxiv.org/abs/2511.03929v2</a><br>We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_03950v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_03950v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03950v1">http://arxiv.org/abs/2511.03950v1</a><br>Reconstructing real-world objects from multi-view images is essential for applications in 3D editing, AR&#x2F;VR, and digital content creation. Existing methods typically prioritize either geometric accuracy (Multi-View Stereo) or photorealistic rendering (Novel View Synthesis), often decoupling geometry and appearance optimization, which hinders downstream editing tasks. This paper advocates an unified treatment on geometry and appearance optimization for seamless Gaussian-mesh joint optimization. More specifically, we propose a novel framework that simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, leveraging photometric consistency from input images and geometric regularization from normal and depth maps. The obtained high-quality 3D reconstruction can be further exploit in down-stream editing tasks, such as relighting and shape deformation. The code will be publicly available upon acceptance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04126v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04126v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04126v1">http://arxiv.org/abs/2511.04126v1</a><br>This study presents a complete pipeline for automated tennis match analysis. Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference. Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times. The experimental results demonstrate robust performance in varying court conditions and match scenarios. The model outputs an annotated video along with detailed performance metrics, enabling coaches, broadcasters, and players to gain actionable insights into the dynamics of the game.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04128v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04128v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04128v1">http://arxiv.org/abs/2511.04128v1</a><br>Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: <a href="https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-">https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning from Online Videos at Inference Time for Computer-Use Agents</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04137v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04137v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04137v1">http://arxiv.org/abs/2511.04137v1</a><br>Computer-use agents can operate computers and automate laborious tasks, but despite recent rapid progress, they still lag behind human users, especially when tasks require domain-specific procedural knowledge about particular applications, platforms, and multi-step workflows. Humans can bridge this gap by watching video tutorials: we search, skim, and selectively imitate short segments that match our current subgoal. In this paper, we study how to enable computer-use agents to learn from online videos at inference time effectively. We propose a framework that retrieves and filters tutorial videos, converts them into structured demonstration trajectories, and dynamically selects trajectories as in-context guidance during execution. Particularly, using a VLM, we infer UI actions, segment videos into short subsequences of actions, and assign each subsequence a textual objective. At inference time, a two-stage selection mechanism dynamically chooses a single trajectory to add in context at each step, focusing the agent on the most helpful local guidance for its next decision. Experiments on two widely used benchmarks show that our framework consistently outperforms strong base agents and variants that use only textual tutorials or transcripts. Analyses highlight the importance of trajectory segmentation and selection, action filtering, and visual information, suggesting that abundant online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time. Our code is available at <a href="https://github.com/UCSB-NLP-Chang/video_demo">https://github.com/UCSB-NLP-Chang/video_demo</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04171v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04171v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04171v1">http://arxiv.org/abs/2511.04171v1</a><br>Image registration refers to the process of spatially aligning two or more images by mapping them into a common coordinate system, so that corresponding anatomical or tissue structures are matched across images. In digital pathology, registration enables direct comparison and integration of information from different stains or imaging modalities, sup-porting applications such as biomarker analysis and tissue reconstruction. Accurate registration of images from different modalities is an essential step in digital pathology. In this study, we investigated how various color transformation techniques affect image registration between hematoxylin and eosin (H&amp;E) stained images and non-linear multimodal images. We used a dataset of 20 tissue sample pairs, with each pair undergoing several preprocessing steps, including different color transformation (CycleGAN, Macenko, Reinhard, Vahadane), inversion, contrast adjustment, intensity normalization, and denoising. All images were registered using the VALIS registration method, which first applies rigid registration and then performs non-rigid registration in two steps on both low and high-resolution images. Registration performance was evaluated using the relative Target Registration Error (rTRE). We reported the median of median rTRE values (MMrTRE) and the average of median rTRE values (AMrTRE) for each method. In addition, we performed a custom point-based evaluation using ten manually selected key points. Registration was done separately for two scenarios, using either the original or inverted multimodal images. In both scenarios, CycleGAN color transformation achieved the lowest registration errors, while the other methods showed higher errors. These findings show that applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AStF: Motion Style Transfer via Adaptive Statistics Fusor</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04192v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04192v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04192v1">http://arxiv.org/abs/2511.04192v1</a><br>Human motion style transfer allows characters to appear less rigidity and more realism with specific style. Traditional arbitrary image style transfer typically process mean and variance which is proved effective. Meanwhile, similar methods have been adapted for motion style transfer. However, due to the fundamental differences between images and motion, relying on mean and variance is insufficient to fully capture the complex dynamic patterns and spatiotemporal coherence properties of motion data. Building upon this, our key insight is to bring two more coefficient, skewness and kurtosis, into the analysis of motion style. Specifically, we propose a novel Adaptive Statistics Fusor (AStF) which consists of Style Disentanglement Module (SDM) and High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in conjunction with a Motion Consistency Regularization (MCR) discriminator. Experimental results show that, by providing a more comprehensive model of the spatiotemporal statistical patterns inherent in dynamic styles, our proposed AStF shows proficiency superiority in motion style transfers over state-of-the-arts. Our code and model are available at <a href="https://github.com/CHMimilanlan/AStF">https://github.com/CHMimilanlan/AStF</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04255v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04255v1">http://arxiv.org/abs/2511.04255v1</a><br>This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at <a href="https://github.com/xmed-lab/MedSapiens">https://github.com/xmed-lab/MedSapiens</a> .</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04260v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04260v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04260v1">http://arxiv.org/abs/2511.04260v1</a><br>The growing sophistication of synthetic image and deepfake generation models has turned source attribution and authenticity verification into a critical challenge for modern computer vision systems. Recent studies suggest that diffusion pipelines unintentionally imprint persistent statistical traces, known as signal leaks, within their outputs, particularly in latent representations. Building on this observation, we propose Proto-LeakNet, a signal-leak-aware and interpretable attribution framework that integrates closed-set classification with a density-based open-set evaluation on the learned embeddings, enabling analysis of unseen generators without retraining. Operating in the latent domain of diffusion models, our method re-simulates partial forward diffusion to expose residual generator-specific cues. A temporal attention encoder aggregates multi-step latent features, while a feature-weighted prototype head structures the embedding space and enables transparent attribution. Trained solely on closed data and achieving a Macro AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under post-processing, surpassing state-of-the-art methods, and achieves strong separability between known and unseen generators. These results demonstrate that modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics. The code for the whole work will be available upon submission.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04304v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04304v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04304v1">http://arxiv.org/abs/2511.04304v1</a><br>The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On the Equivalence of Regression and Classification</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04422v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04422v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04422v1">http://arxiv.org/abs/2511.04422v1</a><br>A formal link between regression and classification has been tenuous. Even though the margin maximization term $|w|$ is used in support vector regression, it has at best been justified as a regularizer. We show that a regression problem with $M$ samples lying on a hyperplane has a one-to-one equivalence with a linearly separable classification task with $2M$ samples. We show that margin maximization on the equivalent classification task leads to a different regression formulation than traditionally used. Using the equivalence, we demonstrate a &#96;&#96;regressability’’ measure, that can be used to estimate the difficulty of regressing a dataset, without needing to first learn a model for it. We use the equivalence to train neural networks to learn a linearizing map, that transforms input variables into a space where a linear regressor is adequate.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T05, 68T10, 68Q32</tag>
      
      <tag>I.2.6; I.5.1; I.5.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04583v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04583v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04583v1">http://arxiv.org/abs/2511.04583v1</a><br>Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04671v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04671v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04671v1">http://arxiv.org/abs/2511.04671v1</a><br>Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at <a href="https://portal-cornell.github.io/X-Diffusion/">https://portal-cornell.github.io/X-Diffusion/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04718v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04718v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04718v1">http://arxiv.org/abs/2511.04718v1</a><br>Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks   by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations,   treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within   specific frequency bands, limiting diagnostic sensitivity and specificity.   While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be   optimal for capturing individual variability or disease-specific alterations.   To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each   brain region and Frequency-Coupled Connectivity Learning to capture   both intra- and nuanced cross-band interactions in a unified functional   network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations   for diagnostic prediction. Experimental results on the ADNI and ABIDE   datasets demonstrate superior performance over existing methods. The   code is available at <a href="https://github.com/XXYY20221234/Ada-FCN">https://github.com/XXYY20221234/Ada-FCN</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04727v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04727v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04727v1">http://arxiv.org/abs/2511.04727v1</a><br>Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Knowledge-based anomaly detection for identifying network-induced shape artifacts</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04729v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04729v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04729v1">http://arxiv.org/abs/2511.04729v1</a><br>Synthetic data provides a promising approach to address data scarcity for training machine learning models; however, adoption without proper quality assessments may introduce artifacts, distortions, and unrealistic features that compromise model performance and clinical utility. This work introduces a novel knowledge-based anomaly detection method for detecting network-induced shape artifacts in synthetic images. The introduced method utilizes a two-stage framework comprising (i) a novel feature extractor that constructs a specialized feature space by analyzing the per-image distribution of angle gradients along anatomical boundaries, and (ii) an isolation forest-based anomaly detector. We demonstrate the effectiveness of the method for identifying network-induced shape artifacts in two synthetic mammography datasets from models trained on CSAW-M and VinDr-Mammo patient datasets respectively. Quantitative evaluation shows that the method successfully concentrates artifacts in the most anomalous partition (1st percentile), with AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study involving three imaging scientists confirmed that images identified by the method as containing network-induced shape artifacts were also flagged by human readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the most anomalous partition, approximately 1.5-2 times higher than the least anomalous partition. Kendall-Tau correlations between algorithmic and human rankings were 0.45 and 0.43 for the two datasets, indicating reasonable agreement despite the challenging nature of subtle artifact detection. This method is a step forward in the responsible use of synthetic data, as it allows developers to evaluate synthetic images for known anatomic constraints and pinpoint and address specific issues to improve the overall quality of a synthetic dataset.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CPO: Condition Preference Optimization for Controllable Image Generation</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04753v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04753v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04753v1">http://arxiv.org/abs/2511.04753v1</a><br>To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t &lt; 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win–lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10%$ error rate reduction in segmentation, $70$–$80%$ in human pose, and consistent $2$–$5%$ reductions in edge and depth maps.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04803v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04803v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04803v1">http://arxiv.org/abs/2511.04803v1</a><br>Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at <a href="https://github.com/MMV-Lab/biomedseg-efficiency">https://github.com/MMV-Lab/biomedseg-efficiency</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.10; I.4.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04811v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04811v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04811v1">http://arxiv.org/abs/2511.04811v1</a><br>Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net’s self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at <a href="https://github.com/MMV-Lab/AL_BioMed_img_seg">https://github.com/MMV-Lab/AL_BioMed_img_seg</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07, 68U10</tag>
      
      <tag>I.2.10; I.4.6; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title>
    <link href="/2025/11/06/cs.AI/2025-11-06-2511_04834v1/"/>
    <url>/2025/11/06/cs.AI/2025-11-06-2511_04834v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04834v1">http://arxiv.org/abs/2511.04834v1</a><br>Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing Straight: Document Orientation Detection for Efficient OCR</title>
    <link href="/2025/11/06/cs.CL/2025-11-06-2511_04161v1/"/>
    <url>/2025/11/06/cs.CL/2025-11-06-2511_04161v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04161v1">http://arxiv.org/abs/2511.04161v1</a><br>Despite significant advances in document understanding, determining the correct orientation of scanned or photographed documents remains a critical pre-processing step in the real world settings. Accurate rotation correction is essential for enhancing the performance of downstream tasks such as Optical Character Recognition (OCR) where misalignment commonly arises due to user errors, particularly incorrect base orientations of the camera during capture. In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from rotation-transformed structured and free-form English OCR datasets, and (ii) ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource languages. We also present a fast, robust and lightweight rotation classification pipeline built on the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned specifically for 4-class rotation task in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy on identifying the rotations respectively on both the datasets. Beyond classification, we demonstrate the critical role of our module in boosting OCR performance: closed-source (up to 14%) and open-weights models (up to 4x) in the simulated real-world setting.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</title>
    <link href="/2025/11/06/cs.CL/2025-11-06-2511_04570v1/"/>
    <url>/2025/11/06/cs.CL/2025-11-06-2511_04570v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04570v1">http://arxiv.org/abs/2511.04570v1</a><br>“Thinking with Text” and “Thinking with Images” paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce “Thinking with Video”, a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2’s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions “thinking with video” as a unified multimodal reasoning paradigm.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
    <link href="/2025/11/06/cs.CL/2025-11-06-2511_04583v1/"/>
    <url>/2025/11/06/cs.CL/2025-11-06-2511_04583v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04583v1">http://arxiv.org/abs/2511.04583v1</a><br>Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Faithful Contouring: Near-Lossless 3D Voxel Representation Free from Iso-surface</title>
    <link href="/2025/11/06/cs.GR/2025-11-06-2511_04029v2/"/>
    <url>/2025/11/06/cs.GR/2025-11-06-2511_04029v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04029v2">http://arxiv.org/abs/2511.04029v2</a><br>Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93% reduction in Chamfer Distance and a 35% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</title>
    <link href="/2025/11/06/cs.HC/2025-11-06-2511_04679v1/"/>
    <url>/2025/11/06/cs.HC/2025-11-06-2511_04679v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04679v1">http://arxiv.org/abs/2511.04679v1</a><br>Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Nemotron Nano V2 VL</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_03929v2/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_03929v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03929v2">http://arxiv.org/abs/2511.03929v2</a><br>We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04255v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04255v1">http://arxiv.org/abs/2511.04255v1</a><br>This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at <a href="https://github.com/xmed-lab/MedSapiens">https://github.com/xmed-lab/MedSapiens</a> .</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04334v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04334v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04334v1">http://arxiv.org/abs/2511.04334v1</a><br>The accurate delineation of tumours in radiological images like Computed Tomography is a very specialised and time-consuming task, and currently a bottleneck preventing quantitative analyses to be performed routinely in the clinical setting. For this reason, developing methods for the automated segmentation of tumours in medical imaging is of the utmost importance and has driven significant efforts in recent years. However, challenges regarding the impracticality of 3D scans, given the large amount of voxels to be analysed, usually requires the downsampling of such images or using patches thereof when applying traditional convolutional neural networks. To overcome this problem, in this paper we propose a new methodology that uses, divided into two stages, voxel sparsification and submanifold sparse convolutional networks. This method allows segmentations to be performed with high-resolution inputs and a native 3D model architecture, obtaining state-of-the-art accuracies while significantly reducing the computational resources needed in terms of GPU memory and time. We studied the deployment of this methodology in the context of Computed Tomography images of renal cancer patients from the KiTS23 challenge, and our method achieved results competitive with the challenge winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7% for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also offers significant computational improvements, achieving up to a 60% reduction in inference time and up to a 75% reduction in VRAM usage compared to an equivalent dense architecture, across both CPU and various GPU cards tested.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04384v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04384v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04384v1">http://arxiv.org/abs/2511.04384v1</a><br>We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding. The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks. This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable. Extensive evaluation demonstrates that our approach substantially improves over single-task baselines in both answer accuracy and visual localization, highlighting the effectiveness of grounded multi-task learning for medical VQA applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On the Equivalence of Regression and Classification</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04422v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04422v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04422v1">http://arxiv.org/abs/2511.04422v1</a><br>A formal link between regression and classification has been tenuous. Even though the margin maximization term $|w|$ is used in support vector regression, it has at best been justified as a regularizer. We show that a regression problem with $M$ samples lying on a hyperplane has a one-to-one equivalence with a linearly separable classification task with $2M$ samples. We show that margin maximization on the equivalent classification task leads to a different regression formulation than traditionally used. Using the equivalence, we demonstrate a &#96;&#96;regressability’’ measure, that can be used to estimate the difficulty of regressing a dataset, without needing to first learn a model for it. We use the equivalence to train neural networks to learn a linearizing map, that transforms input variables into a space where a linear regressor is adequate.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T05, 68T10, 68Q32</tag>
      
      <tag>I.2.6; I.5.1; I.5.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04494v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04494v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04494v1">http://arxiv.org/abs/2511.04494v1</a><br>Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer’s output distribution, which can be expressed as $\lVert (W - \widetilde{W}) \Sigma^{1&#x2F;2}\rVert_F$ where $\Sigma^{1&#x2F;2}$ is the square root of the covariance matrix of the layer’s input and $W$, $\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18&#x2F;50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04583v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04583v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04583v1">http://arxiv.org/abs/2511.04583v1</a><br>Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04665v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04665v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04665v1">http://arxiv.org/abs/2511.04665v1</a><br>Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: <a href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04718v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04718v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04718v1">http://arxiv.org/abs/2511.04718v1</a><br>Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks   by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations,   treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within   specific frequency bands, limiting diagnostic sensitivity and specificity.   While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be   optimal for capturing individual variability or disease-specific alterations.   To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each   brain region and Frequency-Coupled Connectivity Learning to capture   both intra- and nuanced cross-band interactions in a unified functional   network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations   for diagnostic prediction. Experimental results on the ADNI and ABIDE   datasets demonstrate superior performance over existing methods. The   code is available at <a href="https://github.com/XXYY20221234/Ada-FCN">https://github.com/XXYY20221234/Ada-FCN</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04727v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04727v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04727v1">http://arxiv.org/abs/2511.04727v1</a><br>Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CPO: Condition Preference Optimization for Controllable Image Generation</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04753v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04753v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04753v1">http://arxiv.org/abs/2511.04753v1</a><br>To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t &lt; 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win–lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10%$ error rate reduction in segmentation, $70$–$80%$ in human pose, and consistent $2$–$5%$ reductions in edge and depth maps.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04803v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04803v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04803v1">http://arxiv.org/abs/2511.04803v1</a><br>Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at <a href="https://github.com/MMV-Lab/biomedseg-efficiency">https://github.com/MMV-Lab/biomedseg-efficiency</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.10; I.4.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04811v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04811v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04811v1">http://arxiv.org/abs/2511.04811v1</a><br>Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net’s self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at <a href="https://github.com/MMV-Lab/AL_BioMed_img_seg">https://github.com/MMV-Lab/AL_BioMed_img_seg</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07, 68U10</tag>
      
      <tag>I.2.10; I.4.6; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title>
    <link href="/2025/11/06/cs.LG/2025-11-06-2511_04834v1/"/>
    <url>/2025/11/06/cs.LG/2025-11-06-2511_04834v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04834v1">http://arxiv.org/abs/2511.04834v1</a><br>Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning</title>
    <link href="/2025/11/06/cs.MM/2025-11-06-2511_04601v1/"/>
    <url>/2025/11/06/cs.MM/2025-11-06-2511_04601v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04601v1">http://arxiv.org/abs/2511.04601v1</a><br>While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the model’s fine-grained vision-language alignment. However, the inherent token length limitation of CLIP’s text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIP’s original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies</title>
    <link href="/2025/11/06/cs.RO/2025-11-06-2511_04357v1/"/>
    <url>/2025/11/06/cs.RO/2025-11-06-2511_04357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04357v1">http://arxiv.org/abs/2511.04357v1</a><br>Deploying autonomous robots that can learn new skills from demonstrations is an important challenge of modern robotics. Existing solutions often apply end-to-end imitation learning with Vision-Language Action (VLA) models or symbolic approaches with Action Model Learning (AML). On the one hand, current VLA models are limited by the lack of high-level symbolic planning, which hinders their abilities in long-horizon tasks. On the other hand, symbolic approaches in AML lack generalization and scalability perspectives. In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of human demonstrations. This representation is used to generate new planning domains during inference and serves as an orchestrator for low-level VLA policies, scaling up the number of actions that can be reproduced in a row. Our results show that GraSP-VLA is effective for modeling symbolic representations on the task of automatic planning domain generation from observations. In addition, results on real-world experiments show the potential of our Continuous Scene Graph representation to orchestrate low-level VLA policies in long-horizon tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems</title>
    <link href="/2025/11/06/cs.RO/2025-11-06-2511_04388v1/"/>
    <url>/2025/11/06/cs.RO/2025-11-06-2511_04388v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04388v1">http://arxiv.org/abs/2511.04388v1</a><br>Depth estimation is one of the key technologies for realizing 3D perception in unmanned systems. Monocular depth estimation has been widely researched because of its low-cost advantage, but the existing methods face the challenges of poor depth estimation performance and blurred object boundaries on embedded systems. In this paper, we propose a novel monocular depth estimation model, BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate depth maps on embedded systems and significantly improves boundary quality. Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which adaptively fuses depth features to enhance boundary detail representation. Secondly, we integrate semantic knowledge into the encoder to improve the object recognition and boundary perception capabilities. Finally, BoRe-Depth is deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We demonstrate that the proposed model significantly outperforms previous lightweight models on multiple challenging datasets, and we provide detailed ablation studies for the proposed methods. The code is available at <a href="https://github.com/liangxiansheng093/BoRe-Depth">https://github.com/liangxiansheng093/BoRe-Depth</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</title>
    <link href="/2025/11/06/cs.RO/2025-11-06-2511_04555v1/"/>
    <url>/2025/11/06/cs.RO/2025-11-06-2511_04555v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04555v1">http://arxiv.org/abs/2511.04555v1</a><br>Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title>
    <link href="/2025/11/06/cs.RO/2025-11-06-2511_04665v1/"/>
    <url>/2025/11/06/cs.RO/2025-11-06-2511_04665v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04665v1">http://arxiv.org/abs/2511.04665v1</a><br>Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: <a href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title>
    <link href="/2025/11/06/cs.RO/2025-11-06-2511_04671v1/"/>
    <url>/2025/11/06/cs.RO/2025-11-06-2511_04671v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04671v1">http://arxiv.org/abs/2511.04671v1</a><br>Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at <a href="https://portal-cornell.github.io/X-Diffusion/">https://portal-cornell.github.io/X-Diffusion/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</title>
    <link href="/2025/11/06/cs.RO/2025-11-06-2511_04679v1/"/>
    <url>/2025/11/06/cs.RO/2025-11-06-2511_04679v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04679v1">http://arxiv.org/abs/2511.04679v1</a><br>Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Nemotron Nano V2 VL</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03929v2/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03929v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03929v2">http://arxiv.org/abs/2511.03929v2</a><br>We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03943v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03943v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03943v1">http://arxiv.org/abs/2511.03943v1</a><br>Temporal action localization requires precise boundary detection; however, current methods apply uniform computation despite significant variations in difficulty across boundaries. We present two complementary contributions. First, Boundary Distance Regression (BDR) provides information-theoretically optimal localization through signed-distance regression rather than classification, achieving 43% sharper boundary peaks. BDR retrofits to existing methods with approximately 50 lines of code, yielding consistent 1.8 to 3.1% <a href="mailto:&#x6d;&#x41;&#x50;&#64;&#x30;&#x2e;&#55;">mAP@0.7</a> improvements across diverse architectures. Second, Adaptive Temporal Refinement (ATR) allocates computation via continuous depth selection $\tau \in [0,1]$, enabling end-to-end differentiable optimization without reinforcement learning. On THUMOS14, ATR achieves 56.5% <a href="mailto:&#x6d;&#x41;&#80;&#64;&#48;&#x2e;&#55;">mAP@0.7</a> at 162G FLOPs, compared to 53.6% at 198G for uniform processing, providing a 2.9% improvement with 18% less compute. Gains scale with boundary heterogeneity, showing 4.2% improvement on short actions. Training cost is mitigated via knowledge distillation, with lightweight students retaining 99% performance at baseline cost. Results are validated across four benchmarks with rigorous statistical testing.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03950v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03950v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03950v1">http://arxiv.org/abs/2511.03950v1</a><br>Reconstructing real-world objects from multi-view images is essential for applications in 3D editing, AR&#x2F;VR, and digital content creation. Existing methods typically prioritize either geometric accuracy (Multi-View Stereo) or photorealistic rendering (Novel View Synthesis), often decoupling geometry and appearance optimization, which hinders downstream editing tasks. This paper advocates an unified treatment on geometry and appearance optimization for seamless Gaussian-mesh joint optimization. More specifically, we propose a novel framework that simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, leveraging photometric consistency from input images and geometric regularization from normal and depth maps. The obtained high-quality 3D reconstruction can be further exploit in down-stream editing tasks, such as relighting and shape deformation. The code will be publicly available upon acceptance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Linear Fractional Transformation Model and Calibration Method for Light Field Camera</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03962v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03962v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03962v1">http://arxiv.org/abs/2511.03962v1</a><br>Accurate calibration of internal parameters is a crucial yet challenging prerequisite for 3D reconstruction using light field cameras. In this paper, we propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled the main lens and micro lens array (MLA). The proposed method includes an analytical solution based on least squares, followed by nonlinear refinement. The method for detecting features from the raw images is also introduced. Experimental results on both physical and simulated data have verified the performance of proposed method. Based on proposed model, the simulation of raw light field images becomes faster, which is crucial for data-driven deep learning methods. The corresponding code can be obtained from the author’s website.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03970v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03970v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03970v1">http://arxiv.org/abs/2511.03970v1</a><br>Modern scene reconstruction methods are able to accurately recover 3D surfaces that are visible in one or more images. However, this leads to incomplete reconstructions, missing all occluded surfaces. While much progress has been made on reconstructing entire objects given partial observations using generative models, the structural elements of a scene, like the walls, floors and ceilings, have received less attention. We argue that these scene elements should be relatively easy to predict, since they are typically planar, repetitive and simple, and so less costly approaches may be suitable. In this work, we present a synthetic dataset – Room Envelopes – that facilitates progress on this task by providing a set of RGB images and two associated pointmaps for each image: one capturing the visible surface and one capturing the first surface once fittings and fixtures are removed, that is, the structural layout. As we show, this enables direct supervision for feed-forward monocular geometry estimators that predict both the first visible surface and the first layout surface. This confers an understanding of the scene’s extent, as well as the shape and location of its objects.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Simple 3D Pose Features Support Human and Machine Social Scene Understanding</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03988v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03988v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03988v1">http://arxiv.org/abs/2511.03988v1</a><br>Humans can quickly and effortlessly extract a variety of information about others’ social interactions from visual input, ranging from visuospatial cues like whether two people are facing each other to higher-level information. Yet, the computations supporting these abilities remain poorly understood, and social interaction recognition continues to challenge even the most advanced AI vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose information to make social interaction judgments, which is absent in most AI vision models. To test this, we combined state-of-the-art pose and depth estimation algorithms to extract 3D joint positions of people in short video clips depicting everyday human actions and compared their ability to predict human social interaction judgments with current AI vision models. Strikingly, 3D joint positions outperformed most current AI vision models, revealing that key social information is available in explicit body position but not in the learned features of most vision models, including even the layer-wise embeddings of the pose models used to extract joint positions. To uncover the critical pose features humans use to make social judgments, we derived a compact set of 3D social pose features describing only the 3D position and direction of faces in the videos. We found that these minimal descriptors matched the predictive strength of the full set of 3D joints and significantly improved the performance of off-the-shelf AI vision models when combined with their embeddings. Moreover, the degree to which 3D social pose features were represented in each off-the-shelf AI vision model predicted the model’s ability to match human social judgments. Together, our findings provide strong evidence that human social scene understanding relies on explicit representations of 3D pose and can be supported by simple, structured visuospatial primitives.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>q-bio.NC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03992v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03992v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03992v1">http://arxiv.org/abs/2511.03992v1</a><br>Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR&#x2F;VR interaction, and autonomous perception.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04008v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04008v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04008v1">http://arxiv.org/abs/2511.04008v1</a><br>Domain generalization (DG) seeks robust Vision Transformer (ViT) performance on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging; standard fine-tuning is costly and can impair generalization. We propose GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT, SAGE) operates on inter-patch graphs to dynamically assign patches to specialized experts. This context-aware GNN routing leverages inter-patch relationships for better adaptation to domain shifts. GNN-MoE achieves state-of-the-art or competitive DG benchmark performance with high parameter efficiency, highlighting the utility of graph-based contextual routing for robust, lightweight DG.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_03997v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_03997v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03997v1">http://arxiv.org/abs/2511.03997v1</a><br>Recent advances in text-to-video generation have achieved impressive perceptual quality, yet generated content often violates fundamental principles of physical plausibility - manifesting as implausible object dynamics, incoherent interactions, and unrealistic motion patterns. Such failures hinder the deployment of video generation models in embodied AI, robotics, and simulation-intensive domains. To bridge this gap, we propose PhysCorr, a unified framework for modeling, evaluating, and optimizing physical consistency in video generation. Specifically, we introduce PhysicsRM, the first dual-dimensional reward model that quantifies both intra-object stability and inter-object interactions. On this foundation, we develop PhyDPO, a novel direct preference optimization pipeline that leverages contrastive feedback and physics-aware reweighting to guide generation toward physically coherent outputs. Our approach is model-agnostic and scalable, enabling seamless integration into a wide range of video diffusion and transformer-based backbones. Extensive experiments across multiple benchmarks demonstrate that PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment. This work takes a critical step toward physically grounded and trustworthy video generation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Faithful Contouring: Near-Lossless 3D Voxel Representation Free from Iso-surface</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04029v2/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04029v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04029v2">http://arxiv.org/abs/2511.04029v2</a><br>Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93% reduction in Chamfer Distance and a 35% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04016v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04016v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04016v1">http://arxiv.org/abs/2511.04016v1</a><br>The performance of vision models in medical imaging is often hindered by the prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain natural images. To address this fundamental domain gap, we propose MedDChest, a new foundational Vision Transformer (ViT) model optimized specifically for thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated, multimodal dataset of over 1.2 million images, encompassing different modalities including Chest X-ray and Computed Tomography (CT) compiled from 10 public sources. A core technical contribution of our work is Guided Random Resized Crops, a novel content-aware data augmentation strategy that biases sampling towards anatomically relevant regions, overcoming the inefficiency of standard cropping techniques on medical scans. We validate our model’s effectiveness by fine-tuning it on a diverse set of downstream diagnostic tasks. Comprehensive experiments empirically demonstrate that MedDChest significantly outperforms strong, publicly available ImageNet-pretrained models. By establishing the superiority of large-scale, in-domain pre-training combined with domain-specific data augmentation, MedDChest provides a powerful and robust feature extractor that serves as a significantly better starting point for a wide array of thoracic diagnostic tasks. The model weights will be made publicly available to foster future research and applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04037v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04037v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04037v1">http://arxiv.org/abs/2511.04037v1</a><br>Photoplethysmography (PPG) signals, which measure changes in blood volume in the skin using light, have recently gained attention in biometric authentication because of their non-invasive acquisition, inherent liveness detection, and suitability for low-cost wearable devices. However, PPG signal quality is challenged by motion artifacts, illumination changes, and inter-subject physiological variability, making robust feature extraction and classification crucial. This study proposes a lightweight and cost-effective biometric authentication framework based on PPG signals extracted from low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The raw PPG signals undergo a standard preprocessing pipeline involving baseline drift removal, motion artifact suppression using Principal Component Analysis (PCA), bandpass filtering, Fourier-based resampling, and amplitude normalization. To generate robust representations, each one-dimensional PPG segment is converted into a two-dimensional time-frequency scalogram via the Continuous Wavelet Transform (CWT), effectively capturing transient cardiovascular dynamics. We developed a hybrid deep learning model, termed CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision Transformer (CVT) and ConvMixer branches with temporal features from a Long Short-Term Memory network (LSTM). The experimental results on 46 subjects demonstrate an authentication accuracy of 98%, validating the robustness of the model to noise and variability between subjects. Due to its efficiency, scalability, and inherent liveness detection capability, the proposed system is well-suited for real-world mobile and embedded biometric security applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.SP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04078v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04078v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04078v1">http://arxiv.org/abs/2511.04078v1</a><br>Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI remains a fundamental challenge due to subject variability and the entangled nature of visual features. Existing approaches primarily align neural activity directly with visual embeddings, but visual-only representations often fail to capture latent semantic dimensions, limiting interpretability and deep robustness. To address these limitations, we propose Bratrix, the first end-to-end framework to achieve multimodal Language-Anchored Vision-Brain alignment. Bratrix decouples visual stimuli into hierarchical visual and linguistic semantic components, and projects both visual and brain representations into a shared latent space, enabling the formation of aligned visual-language and brain-language embeddings. To emulate human-like perceptual reliability and handle noisy neural signals, Bratrix incorporates a novel uncertainty perception module that applies uncertainty-aware weighting during alignment. By leveraging learnable language-anchored semantic matrices to enhance cross-modal correlations and employing a two-stage training strategy of single-modality pretraining followed by multimodal fine-tuning, Bratrix-M improves alignment precision. Extensive experiments on EEG, MEG, and fMRI benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and captioning performance compared to state-of-the-art methods, specifically surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04084v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04084v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04084v1">http://arxiv.org/abs/2511.04084v1</a><br>Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: <a href="https://github.com/nsapkota417/UKAST">https://github.com/nsapkota417/UKAST</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04083v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04083v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04083v1">http://arxiv.org/abs/2511.04083v1</a><br>We study CT image denoising in the unpaired and self-supervised regimes by evaluating two strong, training-data-efficient paradigms: a CycleGAN-based residual translator and a Noise2Score (N2S) score-matching denoiser. Under a common evaluation protocol, a configuration sweep identifies a simple standard U-Net backbone within CycleGAN (lambda_cycle &#x3D; 30, lambda_iden &#x3D; 2, ngf &#x3D; ndf &#x3D; 64) as the most reliable setting; we then train it to convergence with a longer schedule. The selected CycleGAN improves the noisy input from 34.66 dB &#x2F; 0.9234 SSIM to 38.913 dB &#x2F; 0.971 SSIM and attains an estimated score of 1.9441 and an unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly behind in absolute PSNR &#x2F; SSIM, achieves large gains over very noisy inputs, highlighting its utility when clean pairs are unavailable. Overall, CycleGAN offers the strongest final image quality, whereas Noise2Score provides a robust pair-free alternative with competitive performance. Source code is available at <a href="https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score">https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SpatialLock: Precise Spatial Control in Text-to-Image Synthesis</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04112v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04112v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04112v1">http://arxiv.org/abs/2511.04112v1</a><br>Text-to-Image (T2I) synthesis has made significant advancements in recent years, driving applications such as generating datasets automatically. However, precise control over object localization in generated images remains a challenge. Existing methods fail to fully utilize positional information, leading to an inadequate understanding of object spatial layouts. To address this issue, we propose SpatialLock, a novel framework that leverages perception signals and grounding information to jointly control the generation of spatial locations. SpatialLock incorporates two components: Position-Engaged Injection (PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial information through an attention layer, encouraging the model to learn the grounding information effectively. PoG employs perception-based supervision to further refine object localization. Together, these components enable the model to generate objects with precise spatial arrangements and improve the visual quality of the generated images. Experiments show that SpatialLock sets a new state-of-the-art for precise object positioning, achieving IOU scores above 0.9 across multiple datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04117v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04117v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04117v1">http://arxiv.org/abs/2511.04117v1</a><br>In this paper, we propose Tortoise and Hare Guidance (THG), a training-free strategy that accelerates diffusion sampling while maintaining high-fidelity generation. We demonstrate that the noise estimate and the additional guidance term exhibit markedly different sensitivity to numerical error by reformulating the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our error-bound analysis shows that the additional guidance branch is more robust to approximation, revealing substantial redundancy that conventional solvers fail to exploit. Building on this insight, THG significantly reduces the computation of the additional guidance: the noise estimate is integrated with the tortoise equation on the original, fine-grained timestep grid, while the additional guidance is integrated with the hare equation only on a coarse grid. We also introduce (i) an error-bound-aware timestep sampler that adaptively selects step sizes and (ii) a guidance-scale scheduler that stabilizes large extrapolation spans. THG reduces the number of function evaluations (NFE) by up to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward $\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free accelerators under identical computation budgets. Our findings highlight the potential of multirate formulations for diffusion solvers, paving the way for real-time high-quality image synthesis without any model retraining. The source code is available at <a href="https://github.com/yhlee-add/THG">https://github.com/yhlee-add/THG</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text to Sketch Generation with Multi-Styles</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04123v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04123v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04123v1">http://arxiv.org/abs/2511.04123v1</a><br>Recent advances in vision-language models have facilitated progress in sketch generation. However, existing specialized methods primarily focus on generic synthesis and lack mechanisms for precise control over sketch styles. In this work, we propose a training-free framework based on diffusion models that enables explicit style guidance via textual prompts and referenced style sketches. Unlike previous style transfer methods that overwrite key and value matrices in self-attention, we incorporate the reference features as auxiliary information with linear smoothing and leverage a style-content guidance mechanism. This design effectively reduces content leakage from reference sketches and enhances synthesis quality, especially in cases with low structural similarity between reference and target sketches. Furthermore, we extend our framework to support controllable multi-style generation by integrating features from multiple reference sketches, coordinated via a joint AdaIN module. Extensive experiments demonstrate that our approach achieves high-quality sketch generation with accurate style alignment and improved flexibility in style control. The official implementation of M3S is available at <a href="https://github.com/CMACH508/M3S">https://github.com/CMACH508/M3S</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04126v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04126v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04126v1">http://arxiv.org/abs/2511.04126v1</a><br>This study presents a complete pipeline for automated tennis match analysis. Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference. Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times. The experimental results demonstrate robust performance in varying court conditions and match scenarios. The model outputs an annotated video along with detailed performance metrics, enabling coaches, broadcasters, and players to gain actionable insights into the dynamics of the game.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04128v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04128v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04128v1">http://arxiv.org/abs/2511.04128v1</a><br>Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: <a href="https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-">https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning from Online Videos at Inference Time for Computer-Use Agents</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04137v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04137v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04137v1">http://arxiv.org/abs/2511.04137v1</a><br>Computer-use agents can operate computers and automate laborious tasks, but despite recent rapid progress, they still lag behind human users, especially when tasks require domain-specific procedural knowledge about particular applications, platforms, and multi-step workflows. Humans can bridge this gap by watching video tutorials: we search, skim, and selectively imitate short segments that match our current subgoal. In this paper, we study how to enable computer-use agents to learn from online videos at inference time effectively. We propose a framework that retrieves and filters tutorial videos, converts them into structured demonstration trajectories, and dynamically selects trajectories as in-context guidance during execution. Particularly, using a VLM, we infer UI actions, segment videos into short subsequences of actions, and assign each subsequence a textual objective. At inference time, a two-stage selection mechanism dynamically chooses a single trajectory to add in context at each step, focusing the agent on the most helpful local guidance for its next decision. Experiments on two widely used benchmarks show that our framework consistently outperforms strong base agents and variants that use only textual tutorials or transcripts. Analyses highlight the importance of trajectory segmentation and selection, action filtering, and visual information, suggesting that abundant online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time. Our code is available at <a href="https://github.com/UCSB-NLP-Chang/video_demo">https://github.com/UCSB-NLP-Chang/video_demo</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing Straight: Document Orientation Detection for Efficient OCR</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04161v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04161v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04161v1">http://arxiv.org/abs/2511.04161v1</a><br>Despite significant advances in document understanding, determining the correct orientation of scanned or photographed documents remains a critical pre-processing step in the real world settings. Accurate rotation correction is essential for enhancing the performance of downstream tasks such as Optical Character Recognition (OCR) where misalignment commonly arises due to user errors, particularly incorrect base orientations of the camera during capture. In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from rotation-transformed structured and free-form English OCR datasets, and (ii) ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource languages. We also present a fast, robust and lightweight rotation classification pipeline built on the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned specifically for 4-class rotation task in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy on identifying the rotations respectively on both the datasets. Beyond classification, we demonstrate the critical role of our module in boosting OCR performance: closed-source (up to 14%) and open-weights models (up to 4x) in the simulated real-world setting.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04171v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04171v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04171v1">http://arxiv.org/abs/2511.04171v1</a><br>Image registration refers to the process of spatially aligning two or more images by mapping them into a common coordinate system, so that corresponding anatomical or tissue structures are matched across images. In digital pathology, registration enables direct comparison and integration of information from different stains or imaging modalities, sup-porting applications such as biomarker analysis and tissue reconstruction. Accurate registration of images from different modalities is an essential step in digital pathology. In this study, we investigated how various color transformation techniques affect image registration between hematoxylin and eosin (H&amp;E) stained images and non-linear multimodal images. We used a dataset of 20 tissue sample pairs, with each pair undergoing several preprocessing steps, including different color transformation (CycleGAN, Macenko, Reinhard, Vahadane), inversion, contrast adjustment, intensity normalization, and denoising. All images were registered using the VALIS registration method, which first applies rigid registration and then performs non-rigid registration in two steps on both low and high-resolution images. Registration performance was evaluated using the relative Target Registration Error (rTRE). We reported the median of median rTRE values (MMrTRE) and the average of median rTRE values (AMrTRE) for each method. In addition, we performed a custom point-based evaluation using ten manually selected key points. Registration was done separately for two scenarios, using either the original or inverted multimodal images. In both scenarios, CycleGAN color transformation achieved the lowest registration errors, while the other methods showed higher errors. These findings show that applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04190v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04190v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04190v1">http://arxiv.org/abs/2511.04190v1</a><br>Covariance descriptors capture second-order statistics of image features. They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging. We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices. We propose constructing covariance descriptors from features extracted by pre-trained general vision encoders (GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and MedSAM - are evaluated across eleven binary and multi-class datasets from the MedMNSIT benchmark. Our results show that covariance descriptors derived from GVE features consistently outperform those derived from handcrafted features. Moreover, SPDNet yields superior performance to state-of-the-art methods when combined with DINOv2 features. Our findings highlight the potential of combining covariance descriptors with powerful pretrained vision encoders for medical image analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AStF: Motion Style Transfer via Adaptive Statistics Fusor</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04192v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04192v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04192v1">http://arxiv.org/abs/2511.04192v1</a><br>Human motion style transfer allows characters to appear less rigidity and more realism with specific style. Traditional arbitrary image style transfer typically process mean and variance which is proved effective. Meanwhile, similar methods have been adapted for motion style transfer. However, due to the fundamental differences between images and motion, relying on mean and variance is insufficient to fully capture the complex dynamic patterns and spatiotemporal coherence properties of motion data. Building upon this, our key insight is to bring two more coefficient, skewness and kurtosis, into the analysis of motion style. Specifically, we propose a novel Adaptive Statistics Fusor (AStF) which consists of Style Disentanglement Module (SDM) and High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in conjunction with a Motion Consistency Regularization (MCR) discriminator. Experimental results show that, by providing a more comprehensive model of the spatiotemporal statistical patterns inherent in dynamic styles, our proposed AStF shows proficiency superiority in motion style transfers over state-of-the-arts. Our code and model are available at <a href="https://github.com/CHMimilanlan/AStF">https://github.com/CHMimilanlan/AStF</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04255v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04255v1">http://arxiv.org/abs/2511.04255v1</a><br>This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at <a href="https://github.com/xmed-lab/MedSapiens">https://github.com/xmed-lab/MedSapiens</a> .</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04260v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04260v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04260v1">http://arxiv.org/abs/2511.04260v1</a><br>The growing sophistication of synthetic image and deepfake generation models has turned source attribution and authenticity verification into a critical challenge for modern computer vision systems. Recent studies suggest that diffusion pipelines unintentionally imprint persistent statistical traces, known as signal leaks, within their outputs, particularly in latent representations. Building on this observation, we propose Proto-LeakNet, a signal-leak-aware and interpretable attribution framework that integrates closed-set classification with a density-based open-set evaluation on the learned embeddings, enabling analysis of unseen generators without retraining. Operating in the latent domain of diffusion models, our method re-simulates partial forward diffusion to expose residual generator-specific cues. A temporal attention encoder aggregates multi-step latent features, while a feature-weighted prototype head structures the embedding space and enables transparent attribution. Trained solely on closed data and achieving a Macro AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under post-processing, surpassing state-of-the-art methods, and achieves strong separability between known and unseen generators. These results demonstrate that modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics. The code for the whole work will be available upon submission.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04281v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04281v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04281v1">http://arxiv.org/abs/2511.04281v1</a><br>Video-based Visible-Infrared person re-identification (VVI-ReID) aims to retrieve the same pedestrian across visible and infrared modalities from video sequences. Existing methods tend to exploit modality-invariant visual features but largely overlook gait features, which are not only modality-invariant but also rich in temporal dynamics, thus limiting their ability to model the spatiotemporal consistency essential for cross-modal video matching. To address these challenges, we propose a DINOv2-Driven Gait Representation Learning (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn gait features complementary to appearance cues, facilitating robust sequence-level representations for cross-modal retrieval. Specifically, we introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which generates and enhances silhouette representations with general-purpose semantic priors from DINOv2 and jointly optimizes them with the ReID objective to achieve semantically enriched and task-adaptive gait feature learning. Furthermore, we develop a Progressive Bidirectional Multi-Granularity Enhancement (PBMGE) module, which progressively refines feature representations by enabling bidirectional interactions between gait and appearance streams across multiple spatial granularities, fully leveraging their complementarity to enhance global representations with rich local details and produce highly discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate the superiority of our approach, significantly outperforming existing state-of-the-art methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FastGS: Training 3D Gaussian Splatting in 100 Seconds</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04283v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04283v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04283v1">http://arxiv.org/abs/2511.04283v1</a><br>The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &amp; Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at <a href="https://fastgs.github.io/">https://fastgs.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T40(Primary)68T45, 68U99 (Secondary)</tag>
      
      <tag>I.4.8; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04288v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04288v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04288v1">http://arxiv.org/abs/2511.04288v1</a><br>Herbicide field trials require accurate identification of plant species and assessment of herbicide-induced damage across diverse environments. While general-purpose vision foundation models have shown promising results in complex visual domains, their performance can be limited in agriculture, where fine-grained distinctions between species and damage types are critical.   In this work, we adapt a general-purpose vision foundation model to herbicide trial characterization. Trained using a self-supervised learning approach on a large, curated agricultural dataset, the model learns rich and transferable representations optimized for herbicide trials images.   Our domain-specific model significantly outperforms the best general-purpose foundation model in both species identification (F1 score improvement from 0.91 to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions (new locations and other time), it achieves even greater gains (species identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In domain-shift scenarios, such as drone imagery, it maintains strong performance (species classification from 0.49 to 0.60).   Additionally, we show that domain-specific pretraining enhances segmentation accuracy, particularly in low-annotation regimes. An annotation-efficiency analysis reveals that, under unseen conditions, the domain-specific model achieves 5.4% higher F1 score than the general-purpose model, while using 80% fewer labeled samples.   These results demonstrate the generalization capabilities of domain-specific foundation models and their potential to significantly reduce manual annotation efforts, offering a scalable and automated solution for herbicide trial analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04304v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04304v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04304v1">http://arxiv.org/abs/2511.04304v1</a><br>The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04317v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04317v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04317v1">http://arxiv.org/abs/2511.04317v1</a><br>Most text-to-video(T2V) diffusion models depend on pre-trained text encoders for semantic alignment, yet they often fail to maintain video quality when provided with concise prompts rather than well-designed ones. The primary issue lies in their limited textual semantics understanding. Moreover, these text encoders cannot rephrase prompts online to better align with user intentions, which limits both the scalability and usability of the models, To address these challenges, we introduce RISE-T2V, which uniquely integrates the processes of prompt rephrasing and semantic feature extraction into a single and seamless step instead of two separate steps. RISE-T2V is universal and can be applied to various pre-trained LLMs and video diffusion models(VDMs), significantly enhancing their capabilities for T2V tasks. We propose an innovative module called the Rephrasing Adapter, enabling diffusion models to utilize text hidden states during the next token prediction of the LLM as a condition for video generation. By employing a Rephrasing Adapter, the video generation model can implicitly rephrase basic prompts into more comprehensive representations that better match the user’s intent. Furthermore, we leverage the powerful capabilities of LLMs to enable video generation models to accomplish a broader range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a versatile framework applicable to different video diffusion model architectures, significantly enhancing the ability of T2V models to generate high-quality videos that align with user intent. Visual results are available on the webpage at <a href="https://rise-t2v.github.io/">https://rise-t2v.github.io</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04334v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04334v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04334v1">http://arxiv.org/abs/2511.04334v1</a><br>The accurate delineation of tumours in radiological images like Computed Tomography is a very specialised and time-consuming task, and currently a bottleneck preventing quantitative analyses to be performed routinely in the clinical setting. For this reason, developing methods for the automated segmentation of tumours in medical imaging is of the utmost importance and has driven significant efforts in recent years. However, challenges regarding the impracticality of 3D scans, given the large amount of voxels to be analysed, usually requires the downsampling of such images or using patches thereof when applying traditional convolutional neural networks. To overcome this problem, in this paper we propose a new methodology that uses, divided into two stages, voxel sparsification and submanifold sparse convolutional networks. This method allows segmentations to be performed with high-resolution inputs and a native 3D model architecture, obtaining state-of-the-art accuracies while significantly reducing the computational resources needed in terms of GPU memory and time. We studied the deployment of this methodology in the context of Computed Tomography images of renal cancer patients from the KiTS23 challenge, and our method achieved results competitive with the challenge winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7% for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also offers significant computational improvements, achieving up to a 60% reduction in inference time and up to a 75% reduction in VRAM usage compared to an equivalent dense architecture, across both CPU and various GPU cards tested.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04344v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04344v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04344v1">http://arxiv.org/abs/2511.04344v1</a><br>This paper presents a comprehensive evaluation of nine convolutional neural network architectures for binary classification of horses and motorcycles in the VOC 2008 dataset. We address the significant class imbalance problem by implementing minority-class augmentation techniques. Our experiments compare modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and Vision Transformer across multiple performance metrics. Results demonstrate substantial performance variations, with ConvNeXt-Tiny achieving the highest Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle detection. We observe that data augmentation significantly improves minority class detection, particularly benefiting deeper architectures. This study provides insights into architecture selection for imbalanced binary classification tasks and quantifies the impact of data augmentation strategies in mitigating class imbalance issues in object detection.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04347v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04347v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04347v1">http://arxiv.org/abs/2511.04347v1</a><br>Accurate 3D object detection is essential for automated vehicles to navigate safely in complex real-world environments. Bird’s Eye View (BEV) representations, which project multi-sensor data into a top-down spatial format, have emerged as a powerful approach for robust perception. Although BEV-based fusion architectures have demonstrated strong performance through multimodal integration, the effects of sensor occlusions, caused by environmental conditions such as fog, haze, or physical obstructions, on 3D detection accuracy remain underexplored. In this work, we investigate the impact of occlusions on both camera and Light Detection and Ranging (LiDAR) outputs using the BEVFusion architecture, evaluated on the nuScenes dataset. Detection performance is measured using mean Average Precision (mAP) and the nuScenes Detection Score (NDS). Our results show that moderate camera occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is based only on the camera. On the other hand, LiDAR sharply drops in performance only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%), with a severe impact on long-range detection. In fused settings, the effect depends on which sensor is occluded: occluding the camera leads to a minor 4.1% drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8% drop (to 50.1%), revealing the model’s stronger reliance on LiDAR for the task of 3D object detection. Our results highlight the need for future research into occlusion-aware evaluation methods and improved sensor fusion techniques that can maintain detection accuracy in the presence of partial sensor failure or degradation due to adverse environmental conditions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04349v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04349v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04349v1">http://arxiv.org/abs/2511.04349v1</a><br>Background In analytical chemistry, spatial information about materials is commonly captured through imaging techniques, such as traditional color cameras or with advanced hyperspectral cameras and microscopes. However, efficiently extracting and analyzing this spatial information for exploratory and predictive purposes remains a challenge, especially when using traditional chemometric methods. Recent advances in deep learning and artificial intelligence have significantly enhanced image processing capabilities, enabling the extraction of multiscale deep features that are otherwise challenging to capture with conventional image processing techniques. Despite the wide availability of open-source deep learning models, adoption in analytical chemistry remains limited because of the absence of structured, step-by-step guidance for implementing these models.   Results This tutorial aims to bridge this gap by providing a step-by-step guide for applying deep learning approaches to extract spatial information from imaging data and integrating it with other data sources, such as spectral information. Importantly, the focus of this work is not on training deep learning models for image processing but on using existing open source models to extract deep features from imaging data.   Significance The tutorial provides MATLAB code tutorial demonstrations, showcasing the processing of imaging data from various imaging modalities commonly encountered in analytical chemistry. Readers must run the tutorial steps on their own datasets using the codes presented in this tutorial.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04357v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04357v1">http://arxiv.org/abs/2511.04357v1</a><br>Deploying autonomous robots that can learn new skills from demonstrations is an important challenge of modern robotics. Existing solutions often apply end-to-end imitation learning with Vision-Language Action (VLA) models or symbolic approaches with Action Model Learning (AML). On the one hand, current VLA models are limited by the lack of high-level symbolic planning, which hinders their abilities in long-horizon tasks. On the other hand, symbolic approaches in AML lack generalization and scalability perspectives. In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of human demonstrations. This representation is used to generate new planning domains during inference and serves as an orchestrator for low-level VLA policies, scaling up the number of actions that can be reproduced in a row. Our results show that GraSP-VLA is effective for modeling symbolic representations on the task of automatic planning domain generation from observations. In addition, results on real-world experiments show the potential of our Continuous Scene Graph representation to orchestrate low-level VLA policies in long-horizon tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04384v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04384v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04384v1">http://arxiv.org/abs/2511.04384v1</a><br>We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding. The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks. This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable. Extensive evaluation demonstrates that our approach substantially improves over single-task baselines in both answer accuracy and visual localization, highlighting the effectiveness of grounded multi-task learning for medical VQA applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04388v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04388v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04388v1">http://arxiv.org/abs/2511.04388v1</a><br>Depth estimation is one of the key technologies for realizing 3D perception in unmanned systems. Monocular depth estimation has been widely researched because of its low-cost advantage, but the existing methods face the challenges of poor depth estimation performance and blurred object boundaries on embedded systems. In this paper, we propose a novel monocular depth estimation model, BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate depth maps on embedded systems and significantly improves boundary quality. Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which adaptively fuses depth features to enhance boundary detail representation. Secondly, we integrate semantic knowledge into the encoder to improve the object recognition and boundary perception capabilities. Finally, BoRe-Depth is deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We demonstrate that the proposed model significantly outperforms previous lightweight models on multiple challenging datasets, and we provide detailed ablation studies for the proposed methods. The code is available at <a href="https://github.com/liangxiansheng093/BoRe-Depth">https://github.com/liangxiansheng093/BoRe-Depth</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04394v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04394v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04394v1">http://arxiv.org/abs/2511.04394v1</a><br>DORAEMON is an open-source PyTorch library that unifies visual object modeling and representation learning across diverse scales. A single YAML-driven workflow covers classification, retrieval and metric learning; more than 1000 pretrained backbones are exposed through a timm-compatible interface, together with modular losses, augmentations and distributed-training utilities. Reproducible recipes match or exceed reference results on ImageNet-1K, MS-Celeb-1M and Stanford online products, while one-command export to ONNX or HuggingFace bridges research and deployment. By consolidating datasets, models, and training techniques into one platform, DORAEMON offers a scalable foundation for rapid experimentation in visual recognition and representation learning, enabling efficient transfer of research advances to real-world applications. The repository is available at <a href="https://github.com/wuji3/DORAEMON">https://github.com/wuji3/DORAEMON</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04426v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04426v1">http://arxiv.org/abs/2511.04426v1</a><br>Analyzing octopuses in their natural habitats is challenging due to their camouflage capability, rapid changes in skin texture and color, non-rigid body deformations, and frequent occlusions, all of which are compounded by variable underwater lighting and turbidity. Addressing the lack of large-scale annotated datasets, this paper introduces HideAndSeg, a novel, minimally supervised AI-based tool for segmenting videos of octopuses. It establishes a quantitative baseline for this task. HideAndSeg integrates SAM2 with a custom-trained YOLOv11 object detector. First, the user provides point coordinates to generate the initial segmentation masks with SAM2. These masks serve as training data for the YOLO model. After that, our approach fully automates the pipeline by providing a bounding box prompt to SAM2, eliminating the need for further manual intervention. We introduce two unsupervised metrics - temporal consistency $DICE_t$ and new component count $NC_t$ - to quantitatively evaluate segmentation quality and guide mask refinement in the absence of ground-truth data, i.e., real-world information that serves to train, validate, and test AI models. Results show that HideAndSeg achieves satisfactory performance, reducing segmentation noise compared to the manually prompted approach. Our method can re-identify and segment the octopus even after periods of complete occlusion in natural environments, a scenario in which the manually prompted model fails. By reducing the need for manual analysis in real-world scenarios, this work provides a practical tool that paves the way for more efficient behavioral studies of wild cephalopods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Solving Convex Partition Visual Jigsaw Puzzles</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04450v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04450v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04450v1">http://arxiv.org/abs/2511.04450v1</a><br>Jigsaw puzzle solving requires the rearrangement of unordered pieces into their original pose in order to reconstruct a coherent whole, often an image, and is known to be an intractable problem. While the possible impact of automatic puzzle solvers can be disruptive in various application domains, most of the literature has focused on developing solvers for square jigsaw puzzles, severely limiting their practical use. In this work, we significantly expand the types of puzzles handled computationally, focusing on what is known as Convex Partitions, a major subset of polygonal puzzles whose pieces are convex. We utilize both geometrical and pictorial compatibilities, introduce a greedy solver, and report several performance measures next to the first benchmark dataset of such puzzles.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>V-Thinker: Interactive Thinking with Images</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04460v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04460v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04460v1">http://arxiv.org/abs/2511.04460v1</a><br>Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising “Thinking with Images” paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On the Equivalence of Regression and Classification</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04422v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04422v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04422v1">http://arxiv.org/abs/2511.04422v1</a><br>A formal link between regression and classification has been tenuous. Even though the margin maximization term $|w|$ is used in support vector regression, it has at best been justified as a regularizer. We show that a regression problem with $M$ samples lying on a hyperplane has a one-to-one equivalence with a linearly separable classification task with $2M$ samples. We show that margin maximization on the equivalent classification task leads to a different regression formulation than traditionally used. Using the equivalence, we demonstrate a &#96;&#96;regressability’’ measure, that can be used to estimate the difficulty of regressing a dataset, without needing to first learn a model for it. We use the equivalence to train neural networks to learn a linearizing map, that transforms input variables into a space where a linear regressor is adequate.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T05, 68T10, 68Q32</tag>
      
      <tag>I.2.6; I.5.1; I.5.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04474v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04474v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04474v1">http://arxiv.org/abs/2511.04474v1</a><br>Landslides cause severe damage to lives, infrastructure, and the environment, making accurate and timely mapping essential for disaster preparedness and response. However, conventional deep learning models often struggle when applied across different sensors, regions, or under conditions of limited training data. To address these challenges, we present a three-axis analytical framework of sensor, label, and domain for adapting geospatial foundation models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a series of experiments, we show that it consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE). The model, built on global pretraining, self-supervision, and adaptable fine-tuning, proved resilient to spectral variation, maintained accuracy under label scarcity, and generalized more reliably across diverse datasets and geographic settings. Alongside these strengths, we also highlight remaining challenges such as computational cost and the limited availability of reusable AI-ready training data for landslide research. Overall, our study positions GeoFMs as a step toward more robust and scalable approaches for landslide risk reduction and environmental monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04494v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04494v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04494v1">http://arxiv.org/abs/2511.04494v1</a><br>Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer’s output distribution, which can be expressed as $\lVert (W - \widetilde{W}) \Sigma^{1&#x2F;2}\rVert_F$ where $\Sigma^{1&#x2F;2}$ is the square root of the covariance matrix of the layer’s input and $W$, $\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18&#x2F;50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography via Implicit Neural Representation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04510v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04510v1">http://arxiv.org/abs/2511.04510v1</a><br>Fluorescence Molecular Tomography (FMT) is a promising technique for non-invasive 3D visualization of fluorescent probes, but its reconstruction remains challenging due to the inherent ill-posedness and reliance on inaccurate or often-unknown tissue optical properties. While deep learning methods have shown promise, their supervised nature limits generalization beyond training data. To address these problems, we propose $\mu$NeuFMT, a self-supervised FMT reconstruction framework that integrates implicit neural-based scene representation with explicit physical modeling of photon propagation. Its key innovation lies in jointly optimize both the fluorescence distribution and the optical properties ($\mu$) during reconstruction, eliminating the need for precise prior knowledge of tissue optics or pre-conditioned training data. We demonstrate that $\mu$NeuFMT robustly recovers accurate fluorophore distributions and optical coefficients even with severely erroneous initial values (0.5$\times$ to 2$\times$ of ground truth). Extensive numerical, phantom, and in vivo validations show that $\mu$NeuFMT outperforms conventional and supervised deep learning approaches across diverse heterogeneous scenarios. Our work establishes a new paradigm for robust and accurate FMT reconstruction, paving the way for more reliable molecular imaging in complex clinically related scenarios, such as fluorescence guided surgery.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>I.2.10; I.4.5</tag>
      
      <tag>physics.optics</tag>
      
      <tag>68T07, 78A46, 78A70, 92C55</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>THEval. Evaluation Framework for Talking Head Video Generation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04520v2/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04520v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04520v2">http://arxiv.org/abs/2511.04520v2</a><br>Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04525v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04525v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04525v1">http://arxiv.org/abs/2511.04525v1</a><br>Purpose: Accurate assessment of surgical complexity is essential in Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with longer operative times and increased risk of postoperative complications. The Parkland Grading Scale (PGS) provides a clinically validated framework for stratifying inflammation severity; however, its automation in surgical videos remains largely unexplored, particularly in realistic scenarios where complete videos must be analyzed without prior manual curation. Methods: In this work, we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity estimation in LC via the PGS, designed to operate under weak temporal supervision. Unlike prior methods limited to static images or manually trimmed clips, STC-Net operates directly on full videos. It jointly performs temporal localization and grading through a localization, window proposal, and grading module. We introduce a novel loss formulation combining hard and soft localization objectives and background-aware grading supervision. Results: Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by over 10% in both metrics and highlighting the effectiveness of weak supervision for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, making it promising for post-operative analysis and surgical training.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04555v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04555v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04555v1">http://arxiv.org/abs/2511.04555v1</a><br>Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04570v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04570v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04570v1">http://arxiv.org/abs/2511.04570v1</a><br>“Thinking with Text” and “Thinking with Images” paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce “Thinking with Video”, a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2’s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions “thinking with video” as a unified multimodal reasoning paradigm.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04583v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04583v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04583v1">http://arxiv.org/abs/2511.04583v1</a><br>Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04595v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04595v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04595v1">http://arxiv.org/abs/2511.04595v1</a><br>Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04601v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04601v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04601v1">http://arxiv.org/abs/2511.04601v1</a><br>While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the model’s fine-grained vision-language alignment. However, the inherent token length limitation of CLIP’s text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIP’s original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04615v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04615v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04615v1">http://arxiv.org/abs/2511.04615v1</a><br>Deep learning models can generate virtual immunohistochemistry (IHC) stains from hematoxylin and eosin (H&amp;E) images, offering a scalable and low-cost alternative to laboratory IHC. However, reliable evaluation of image quality remains a challenge as current texture- and distribution-based metrics quantify image fidelity rather than the accuracy of IHC staining. Here, we introduce an automated and accuracy grounded framework to determine image quality across sixteen paired or unpaired image translation models. Using color deconvolution, we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by each virtual IHC model. We use the segmented masks of real and virtual IHC to compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly quantify correct pixel - level labeling without needing expert manual annotations. Our results demonstrate that conventional image fidelity metrics, including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM), correlate poorly with stain accuracy and pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based models are less reliable in providing accurate IHC positive pixel labels. Moreover, whole-slide images (WSI) reveal performance declines that are invisible in patch-based evaluations, emphasizing the need for WSI-level benchmarks. Together, this framework defines a reproducible approach for assessing the quality of virtual IHC models, a critical step to accelerate translation towards routine use by pathologists.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04628v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04628v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04628v1">http://arxiv.org/abs/2511.04628v1</a><br>Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Polarization-resolved imaging improves eye tracking</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04652v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04652v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04652v1">http://arxiv.org/abs/2511.04652v1</a><br>Polarization-resolved near-infrared imaging adds a useful optical contrast mechanism to eye tracking by measuring the polarization state of light reflected by ocular tissues in addition to its intensity. In this paper we demonstrate how this contrast can be used to enable eye tracking. Specifically, we demonstrate that a polarization-enabled eye tracking (PET) system composed of a polarization–filter–array camera paired with a linearly polarized near-infrared illuminator can reveal trackable features across the sclera and gaze-informative patterns on the cornea, largely absent in intensity-only images. Across a cohort of 346 participants, convolutional neural network based machine learning models trained on data from PET reduced the median 95th-percentile absolute gaze error by 10–16% relative to capacity-matched intensity baselines under nominal conditions and in the presence of eyelid occlusions, eye-relief changes, and pupil-size variation. These results link light–tissue polarization effects to practical gains in human–computer interaction and position PET as a simple, robust sensing modality for future wearable devices.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>physics.optics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmark Designers Should &quot;Train on the Test Set&quot; to Expose Exploitable Non-Visual Shortcuts</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04655v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04655v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04655v1">http://arxiv.org/abs/2511.04655v1</a><br>Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to <code>game&#39;&#39; their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly </code>training on the test set’’ – probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a <code>Test-set Stress-Test&#39;&#39; (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via $k$-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score $s(x)$. We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an </code>Iterative Bias Pruning’’ (IBP) procedure. Applying this framework to four benchmarks – VSI-Bench, CV-Bench, MMMU, and VideoMME – we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04665v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04665v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04665v1">http://arxiv.org/abs/2511.04665v1</a><br>Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: <a href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04668v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04668v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04668v1">http://arxiv.org/abs/2511.04668v1</a><br>Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V – a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cambrian-S: Towards Spatial Supersensing in Video</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04670v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04670v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04670v1">http://arxiv.org/abs/2511.04670v1</a><br>We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04671v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04671v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04671v1">http://arxiv.org/abs/2511.04671v1</a><br>Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at <a href="https://portal-cornell.github.io/X-Diffusion/">https://portal-cornell.github.io/X-Diffusion/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04675v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04675v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04675v1">http://arxiv.org/abs/2511.04675v1</a><br>We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tracking and Understanding Object Transformations</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04678v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04678v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04678v1">http://arxiv.org/abs/2511.04678v1</a><br>Real-world objects frequently undergo state transformations. From an apple being cut into pieces to a butterfly emerging from its cocoon, tracking through these changes is important for understanding real-world objects and dynamics. However, existing methods often lose track of the target object after transformation, due to significant changes in object appearance. To address this limitation, we introduce the task of Track Any State: tracking objects through transformations while detecting and describing state changes, accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we present TubeletGraph, a zero-shot system that recovers missing objects after transformation and maps out how object states are evolving over time. TubeletGraph first identifies potentially overlooked tracks, and determines whether they should be integrated based on semantic and proximity priors. Then, it reasons about the added tracks and generates a state graph describing each observed transformation. TubeletGraph achieves state-of-the-art tracking performance under transformations, while demonstrating deeper understanding of object transformations and promising capabilities in temporal grounding and semantic reasoning for complex object transformations. Code, additional results, and the benchmark dataset are available at <a href="https://tubelet-graph.github.io/">https://tubelet-graph.github.io</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04679v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04679v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04679v1">http://arxiv.org/abs/2511.04679v1</a><br>Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04680v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04680v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04680v1">http://arxiv.org/abs/2511.04680v1</a><br>Automatic image cropping is a method for maximizing the human-perceived quality of cropped regions in photographs. Although several works have proposed techniques for producing singular crops, little work has addressed the problem of producing multiple, distinct crops with aesthetic appeal. In this paper, we motivate the problem with a discussion on modern social media applications, introduce a dataset of 277 relevant images and human labels, and evaluate the efficacy of several single-crop models with an image partitioning algorithm as a pre-processing step. The dataset is available at <a href="https://github.com/RafeLoya/carousel">https://github.com/RafeLoya/carousel</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04718v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04718v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04718v1">http://arxiv.org/abs/2511.04718v1</a><br>Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks   by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations,   treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within   specific frequency bands, limiting diagnostic sensitivity and specificity.   While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be   optimal for capturing individual variability or disease-specific alterations.   To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each   brain region and Frequency-Coupled Connectivity Learning to capture   both intra- and nuanced cross-band interactions in a unified functional   network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations   for diagnostic prediction. Experimental results on the ADNI and ABIDE   datasets demonstrate superior performance over existing methods. The   code is available at <a href="https://github.com/XXYY20221234/Ada-FCN">https://github.com/XXYY20221234/Ada-FCN</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04727v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04727v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04727v1">http://arxiv.org/abs/2511.04727v1</a><br>Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Knowledge-based anomaly detection for identifying network-induced shape artifacts</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04729v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04729v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04729v1">http://arxiv.org/abs/2511.04729v1</a><br>Synthetic data provides a promising approach to address data scarcity for training machine learning models; however, adoption without proper quality assessments may introduce artifacts, distortions, and unrealistic features that compromise model performance and clinical utility. This work introduces a novel knowledge-based anomaly detection method for detecting network-induced shape artifacts in synthetic images. The introduced method utilizes a two-stage framework comprising (i) a novel feature extractor that constructs a specialized feature space by analyzing the per-image distribution of angle gradients along anatomical boundaries, and (ii) an isolation forest-based anomaly detector. We demonstrate the effectiveness of the method for identifying network-induced shape artifacts in two synthetic mammography datasets from models trained on CSAW-M and VinDr-Mammo patient datasets respectively. Quantitative evaluation shows that the method successfully concentrates artifacts in the most anomalous partition (1st percentile), with AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study involving three imaging scientists confirmed that images identified by the method as containing network-induced shape artifacts were also flagged by human readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the most anomalous partition, approximately 1.5-2 times higher than the least anomalous partition. Kendall-Tau correlations between algorithmic and human rankings were 0.45 and 0.43 for the two datasets, indicating reasonable agreement despite the challenging nature of subtle artifact detection. This method is a step forward in the responsible use of synthetic data, as it allows developers to evaluate synthetic images for known anatomic constraints and pinpoint and address specific issues to improve the overall quality of a synthetic dataset.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CPO: Condition Preference Optimization for Controllable Image Generation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04753v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04753v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04753v1">http://arxiv.org/abs/2511.04753v1</a><br>To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t &lt; 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win–lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10%$ error rate reduction in segmentation, $70$–$80%$ in human pose, and consistent $2$–$5%$ reductions in edge and depth maps.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04766v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04766v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04766v1">http://arxiv.org/abs/2511.04766v1</a><br>Foundation models (FMs) offer powerful representations for geospatial analysis, but adapting them effectively remains challenging. Standard adaptation methods, whether full fine-tuning or efficient frozen-backbone approaches, typically employ decoders with fixed regularization strategies, failing to account for the significant heterogeneity in satellite imagery. We introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture designed to address this limitation. DARN integrates three key innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and (3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide theoretical justifications linking DARN’s optimization to stationary point convergence and its mechanism to adaptive information bottlenecks. Empirically, DARN demonstrates exceptional performance across both major adaptation paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering substantial advantages crucial for real-world deployment: superior out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms), enhanced robustness (17% relative reduction in corruption error), and improved performance on minority classes. DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Global 3D Reconstruction of Clouds &amp; Tropical Cyclones</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04773v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04773v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04773v1">http://arxiv.org/abs/2511.04773v1</a><br>Accurate forecasting of tropical cyclones (TCs) remains challenging due to limited satellite observations probing TC structure and difficulties in resolving cloud properties involved in TC intensification. Recent research has demonstrated the capabilities of machine learning methods for 3D cloud reconstruction from satellite observations. However, existing approaches have been restricted to regions where TCs are uncommon, and are poorly validated for intense storms. We introduce a new framework, based on a pre-training–fine-tuning pipeline, that learns from multiple satellites with global coverage to translate 2D satellite imagery into 3D cloud maps of relevant cloud properties. We apply our model to a custom-built TC dataset to evaluate performance in the most challenging and relevant conditions. We show that we can - for the first time - create global instantaneous 3D cloud maps and accurately reconstruct the 3D structure of intense storms. Our model not only extends available satellite observations but also provides estimates when observations are missing entirely. This is crucial for advancing our understanding of TC intensification and improving forecasts.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>physics.ao-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3D Gaussian Point Encoders</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04797v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04797v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04797v1">http://arxiv.org/abs/2511.04797v1</a><br>In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04779v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04779v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04779v1">http://arxiv.org/abs/2511.04779v1</a><br>Event-based cameras are becoming a popular solution for efficient, low-power eye tracking. Due to the sparse and asynchronous nature of event data, they require less processing power and offer latencies in the microsecond range. However, many existing solutions are limited to validation on powerful GPUs, with no deployment on real embedded devices. In this paper, we present EETnet, a convolutional neural network designed for eye tracking using purely event-based data, capable of running on microcontrollers with limited resources. Additionally, we outline a methodology to train, evaluate, and quantize the network using a public dataset. Finally, we propose two versions of the architecture: a classification model that detects the pupil on a grid superimposed on the original image, and a regression model that operates at the pixel level.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04803v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04803v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04803v1">http://arxiv.org/abs/2511.04803v1</a><br>Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at <a href="https://github.com/MMV-Lab/biomedseg-efficiency">https://github.com/MMV-Lab/biomedseg-efficiency</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.10; I.4.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04811v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04811v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04811v1">http://arxiv.org/abs/2511.04811v1</a><br>Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net’s self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at <a href="https://github.com/MMV-Lab/AL_BioMed_img_seg">https://github.com/MMV-Lab/AL_BioMed_img_seg</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07, 68U10</tag>
      
      <tag>I.2.10; I.4.6; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04834v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04834v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04834v1">http://arxiv.org/abs/2511.04834v1</a><br>Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Geometry Denoising with Preferred Normal Vectors</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04848v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04848v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04848v1">http://arxiv.org/abs/2511.04848v1</a><br>We introduce a new paradigm for geometry denoising using prior knowledge about the surface normal vector. This prior knowledge comes in the form of a set of preferred normal vectors, which we refer to as label vectors. A segmentation problem is naturally embedded in the denoising process. The segmentation is based on the similarity of the normal vector to the elements of the set of label vectors. Regularization is achieved by a total variation term. We formulate a split Bregman (ADMM) approach to solve the resulting optimization problem. The vertex update step is based on second-order shape calculus.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>math.OC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04864v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04864v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04864v1">http://arxiv.org/abs/2511.04864v1</a><br>Recovering high-quality surfaces from irregular point cloud is ill-posed unless strong geometric priors are available. We introduce an implicit self-prior approach that distills a shape-specific prior directly from the input point cloud itself and embeds it within an implicit neural representation. This is achieved by jointly training a small dictionary of learnable embeddings with an implicit distance field; at every query location, the field attends to the dictionary via cross-attention, enabling the network to capture and reuse repeating structures and long-range correlations inherent to the shape. Optimized solely with self-supervised point cloud reconstruction losses, our approach requires no external training data. To effectively integrate this learned prior while preserving input fidelity, the trained field is then sampled to extract densely distributed points and analytic normals via automatic differentiation. We integrate the resulting dense point cloud and corresponding normals into a robust implicit moving least squares (RIMLS) formulation. We show this hybrid strategy preserves fine geometric details in the input data, while leveraging the learned prior to regularize sparse regions. Experiments show that our method outperforms both classical and learning-based approaches in generating high-fidelity surfaces with superior detail preservation and robustness to common data degradations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04871v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04871v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04871v1">http://arxiv.org/abs/2511.04871v1</a><br>Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps are effective for assessing neurodegenerative diseases and microstructural properties of white matter in large number of brain conditions. However, DW-MRI inherently limits the combination of data from multiple acquisition sites without harmonization to mitigate scanner-specific biases. While the widely used ComBAT method reduces site effects in research, its reliance on linear covariate relationships, homogeneous populations, fixed site numbers, and well populated sites constrains its clinical use. To overcome these limitations, we propose Clinical-ComBAT, a method designed for real-world clinical scenarios. Clinical-ComBAT harmonizes each site independently, enabling flexibility as new data and clinics are introduced. It incorporates a non-linear polynomial data model, site-specific harmonization referenced to a normative site, and variance priors adaptable to small cohorts. It further includes hyperparameter tuning and a goodness-of-fit metric for harmonization assessment. We demonstrate its effectiveness on simulated and real data, showing improved alignment of diffusion metrics and enhanced applicability for normative modeling.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects</title>
    <link href="/2025/11/06/cs.CV/2025-11-06-2511_04872v1/"/>
    <url>/2025/11/06/cs.CV/2025-11-06-2511_04872v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04872v1">http://arxiv.org/abs/2511.04872v1</a><br>This study evaluates the efficacy of vision transformer models, specifically Swin transformers, in enhancing the diagnostic accuracy of ear diseases compared to traditional convolutional neural networks. With a reported 27% misdiagnosis rate among specialist otolaryngologists, improving diagnostic accuracy is crucial. The research utilised a real-world dataset from the Department of Otolaryngology at the Clinical Hospital of the Universidad de Chile, comprising otoscopic videos of ear examinations depicting various middle and external ear conditions. Frames were selected based on the Laplacian and Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively, marginally outperforming the ResNet model (99.5%). These results surpassed metrics reported in related studies. However, the evaluation uncovered a critical data leakage issue in the preprocessing step, affecting both this study and related research using the same raw dataset. After mitigating the data leakage, model performance decreased significantly. Corrected accuracies were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This finding highlights the importance of rigorous data handling in machine learning studies, especially in medical applications. The findings indicate that while vision transformers show promise, it is essential to find an optimal balance between the benefits of advanced model architectures and those derived from effective data preprocessing. This balance is key to developing a reliable machine learning model for diagnosing ear diseases.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data</title>
    <link href="/2025/11/06/eess.IV/2025-11-06-2511_04304v1/"/>
    <url>/2025/11/06/eess.IV/2025-11-06-2511_04304v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04304v1">http://arxiv.org/abs/2511.04304v1</a><br>The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography via Implicit Neural Representation</title>
    <link href="/2025/11/06/eess.IV/2025-11-06-2511_04510v1/"/>
    <url>/2025/11/06/eess.IV/2025-11-06-2511_04510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04510v1">http://arxiv.org/abs/2511.04510v1</a><br>Fluorescence Molecular Tomography (FMT) is a promising technique for non-invasive 3D visualization of fluorescent probes, but its reconstruction remains challenging due to the inherent ill-posedness and reliance on inaccurate or often-unknown tissue optical properties. While deep learning methods have shown promise, their supervised nature limits generalization beyond training data. To address these problems, we propose $\mu$NeuFMT, a self-supervised FMT reconstruction framework that integrates implicit neural-based scene representation with explicit physical modeling of photon propagation. Its key innovation lies in jointly optimize both the fluorescence distribution and the optical properties ($\mu$) during reconstruction, eliminating the need for precise prior knowledge of tissue optics or pre-conditioned training data. We demonstrate that $\mu$NeuFMT robustly recovers accurate fluorophore distributions and optical coefficients even with severely erroneous initial values (0.5$\times$ to 2$\times$ of ground truth). Extensive numerical, phantom, and in vivo validations show that $\mu$NeuFMT outperforms conventional and supervised deep learning approaches across diverse heterogeneous scenarios. Our work establishes a new paradigm for robust and accurate FMT reconstruction, paving the way for more reliable molecular imaging in complex clinically related scenarios, such as fluorescence guided surgery.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>I.2.10; I.4.5</tag>
      
      <tag>physics.optics</tag>
      
      <tag>68T07, 78A46, 78A70, 92C55</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals</title>
    <link href="/2025/11/06/eess.SP/2025-11-06-2511_04037v1/"/>
    <url>/2025/11/06/eess.SP/2025-11-06-2511_04037v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04037v1">http://arxiv.org/abs/2511.04037v1</a><br>Photoplethysmography (PPG) signals, which measure changes in blood volume in the skin using light, have recently gained attention in biometric authentication because of their non-invasive acquisition, inherent liveness detection, and suitability for low-cost wearable devices. However, PPG signal quality is challenged by motion artifacts, illumination changes, and inter-subject physiological variability, making robust feature extraction and classification crucial. This study proposes a lightweight and cost-effective biometric authentication framework based on PPG signals extracted from low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The raw PPG signals undergo a standard preprocessing pipeline involving baseline drift removal, motion artifact suppression using Principal Component Analysis (PCA), bandpass filtering, Fourier-based resampling, and amplitude normalization. To generate robust representations, each one-dimensional PPG segment is converted into a two-dimensional time-frequency scalogram via the Continuous Wavelet Transform (CWT), effectively capturing transient cardiovascular dynamics. We developed a hybrid deep learning model, termed CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision Transformer (CVT) and ConvMixer branches with temporal features from a Long Short-Term Memory network (LSTM). The experimental results on 46 subjects demonstrate an authentication accuracy of 98%, validating the robustness of the model to noise and variability between subjects. Due to its efficiency, scalability, and inherent liveness detection capability, the proposed system is well-suited for real-world mobile and embedded biometric security applications.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.SP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Nemotron Nano V2 VL</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03929v2/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03929v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03929v2">http://arxiv.org/abs/2511.03929v2</a><br>We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03943v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03943v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03943v1">http://arxiv.org/abs/2511.03943v1</a><br>Temporal action localization requires precise boundary detection; however, current methods apply uniform computation despite significant variations in difficulty across boundaries. We present two complementary contributions. First, Boundary Distance Regression (BDR) provides information-theoretically optimal localization through signed-distance regression rather than classification, achieving 43% sharper boundary peaks. BDR retrofits to existing methods with approximately 50 lines of code, yielding consistent 1.8 to 3.1% <a href="mailto:&#x6d;&#x41;&#80;&#64;&#48;&#46;&#55;">mAP@0.7</a> improvements across diverse architectures. Second, Adaptive Temporal Refinement (ATR) allocates computation via continuous depth selection $\tau \in [0,1]$, enabling end-to-end differentiable optimization without reinforcement learning. On THUMOS14, ATR achieves 56.5% <a href="mailto:&#109;&#65;&#x50;&#x40;&#x30;&#46;&#55;">mAP@0.7</a> at 162G FLOPs, compared to 53.6% at 198G for uniform processing, providing a 2.9% improvement with 18% less compute. Gains scale with boundary heterogeneity, showing 4.2% improvement on short actions. Training cost is mitigated via knowledge distillation, with lightweight students retaining 99% performance at baseline cost. Results are validated across four benchmarks with rigorous statistical testing.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03950v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03950v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03950v1">http://arxiv.org/abs/2511.03950v1</a><br>Reconstructing real-world objects from multi-view images is essential for applications in 3D editing, AR&#x2F;VR, and digital content creation. Existing methods typically prioritize either geometric accuracy (Multi-View Stereo) or photorealistic rendering (Novel View Synthesis), often decoupling geometry and appearance optimization, which hinders downstream editing tasks. This paper advocates an unified treatment on geometry and appearance optimization for seamless Gaussian-mesh joint optimization. More specifically, we propose a novel framework that simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, leveraging photometric consistency from input images and geometric regularization from normal and depth maps. The obtained high-quality 3D reconstruction can be further exploit in down-stream editing tasks, such as relighting and shape deformation. The code will be publicly available upon acceptance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Linear Fractional Transformation Model and Calibration Method for Light Field Camera</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03962v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03962v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03962v1">http://arxiv.org/abs/2511.03962v1</a><br>Accurate calibration of internal parameters is a crucial yet challenging prerequisite for 3D reconstruction using light field cameras. In this paper, we propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled the main lens and micro lens array (MLA). The proposed method includes an analytical solution based on least squares, followed by nonlinear refinement. The method for detecting features from the raw images is also introduced. Experimental results on both physical and simulated data have verified the performance of proposed method. Based on proposed model, the simulation of raw light field images becomes faster, which is crucial for data-driven deep learning methods. The corresponding code can be obtained from the author’s website.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Simple 3D Pose Features Support Human and Machine Social Scene Understanding</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03988v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03988v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03988v1">http://arxiv.org/abs/2511.03988v1</a><br>Humans can quickly and effortlessly extract a variety of information about others’ social interactions from visual input, ranging from visuospatial cues like whether two people are facing each other to higher-level information. Yet, the computations supporting these abilities remain poorly understood, and social interaction recognition continues to challenge even the most advanced AI vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose information to make social interaction judgments, which is absent in most AI vision models. To test this, we combined state-of-the-art pose and depth estimation algorithms to extract 3D joint positions of people in short video clips depicting everyday human actions and compared their ability to predict human social interaction judgments with current AI vision models. Strikingly, 3D joint positions outperformed most current AI vision models, revealing that key social information is available in explicit body position but not in the learned features of most vision models, including even the layer-wise embeddings of the pose models used to extract joint positions. To uncover the critical pose features humans use to make social judgments, we derived a compact set of 3D social pose features describing only the 3D position and direction of faces in the videos. We found that these minimal descriptors matched the predictive strength of the full set of 3D joints and significantly improved the performance of off-the-shelf AI vision models when combined with their embeddings. Moreover, the degree to which 3D social pose features were represented in each off-the-shelf AI vision model predicted the model’s ability to match human social judgments. Together, our findings provide strong evidence that human social scene understanding relies on explicit representations of 3D pose and can be supported by simple, structured visuospatial primitives.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>q-bio.NC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03970v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03970v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03970v1">http://arxiv.org/abs/2511.03970v1</a><br>Modern scene reconstruction methods are able to accurately recover 3D surfaces that are visible in one or more images. However, this leads to incomplete reconstructions, missing all occluded surfaces. While much progress has been made on reconstructing entire objects given partial observations using generative models, the structural elements of a scene, like the walls, floors and ceilings, have received less attention. We argue that these scene elements should be relatively easy to predict, since they are typically planar, repetitive and simple, and so less costly approaches may be suitable. In this work, we present a synthetic dataset – Room Envelopes – that facilitates progress on this task by providing a set of RGB images and two associated pointmaps for each image: one capturing the visible surface and one capturing the first surface once fittings and fixtures are removed, that is, the structural layout. As we show, this enables direct supervision for feed-forward monocular geometry estimators that predict both the first visible surface and the first layout surface. This confers an understanding of the scene’s extent, as well as the shape and location of its objects.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03992v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03992v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03992v1">http://arxiv.org/abs/2511.03992v1</a><br>Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR&#x2F;VR interaction, and autonomous perception.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_03997v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_03997v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03997v1">http://arxiv.org/abs/2511.03997v1</a><br>Recent advances in text-to-video generation have achieved impressive perceptual quality, yet generated content often violates fundamental principles of physical plausibility - manifesting as implausible object dynamics, incoherent interactions, and unrealistic motion patterns. Such failures hinder the deployment of video generation models in embodied AI, robotics, and simulation-intensive domains. To bridge this gap, we propose PhysCorr, a unified framework for modeling, evaluating, and optimizing physical consistency in video generation. Specifically, we introduce PhysicsRM, the first dual-dimensional reward model that quantifies both intra-object stability and inter-object interactions. On this foundation, we develop PhyDPO, a novel direct preference optimization pipeline that leverages contrastive feedback and physics-aware reweighting to guide generation toward physically coherent outputs. Our approach is model-agnostic and scalable, enabling seamless integration into a wide range of video diffusion and transformer-based backbones. Extensive experiments across multiple benchmarks demonstrate that PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment. This work takes a critical step toward physically grounded and trustworthy video generation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04008v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04008v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04008v1">http://arxiv.org/abs/2511.04008v1</a><br>Domain generalization (DG) seeks robust Vision Transformer (ViT) performance on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging; standard fine-tuning is costly and can impair generalization. We propose GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT, SAGE) operates on inter-patch graphs to dynamically assign patches to specialized experts. This context-aware GNN routing leverages inter-patch relationships for better adaptation to domain shifts. GNN-MoE achieves state-of-the-art or competitive DG benchmark performance with high parameter efficiency, highlighting the utility of graph-based contextual routing for robust, lightweight DG.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04016v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04016v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04016v1">http://arxiv.org/abs/2511.04016v1</a><br>The performance of vision models in medical imaging is often hindered by the prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain natural images. To address this fundamental domain gap, we propose MedDChest, a new foundational Vision Transformer (ViT) model optimized specifically for thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated, multimodal dataset of over 1.2 million images, encompassing different modalities including Chest X-ray and Computed Tomography (CT) compiled from 10 public sources. A core technical contribution of our work is Guided Random Resized Crops, a novel content-aware data augmentation strategy that biases sampling towards anatomically relevant regions, overcoming the inefficiency of standard cropping techniques on medical scans. We validate our model’s effectiveness by fine-tuning it on a diverse set of downstream diagnostic tasks. Comprehensive experiments empirically demonstrate that MedDChest significantly outperforms strong, publicly available ImageNet-pretrained models. By establishing the superiority of large-scale, in-domain pre-training combined with domain-specific data augmentation, MedDChest provides a powerful and robust feature extractor that serves as a significantly better starting point for a wide array of thoracic diagnostic tasks. The model weights will be made publicly available to foster future research and applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Faithful Contouring: Near-Lossless 3D Voxel Representation Free from Iso-surface</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04029v2/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04029v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04029v2">http://arxiv.org/abs/2511.04029v2</a><br>Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93% reduction in Chamfer Distance and a 35% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04037v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04037v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04037v1">http://arxiv.org/abs/2511.04037v1</a><br>Photoplethysmography (PPG) signals, which measure changes in blood volume in the skin using light, have recently gained attention in biometric authentication because of their non-invasive acquisition, inherent liveness detection, and suitability for low-cost wearable devices. However, PPG signal quality is challenged by motion artifacts, illumination changes, and inter-subject physiological variability, making robust feature extraction and classification crucial. This study proposes a lightweight and cost-effective biometric authentication framework based on PPG signals extracted from low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The raw PPG signals undergo a standard preprocessing pipeline involving baseline drift removal, motion artifact suppression using Principal Component Analysis (PCA), bandpass filtering, Fourier-based resampling, and amplitude normalization. To generate robust representations, each one-dimensional PPG segment is converted into a two-dimensional time-frequency scalogram via the Continuous Wavelet Transform (CWT), effectively capturing transient cardiovascular dynamics. We developed a hybrid deep learning model, termed CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision Transformer (CVT) and ConvMixer branches with temporal features from a Long Short-Term Memory network (LSTM). The experimental results on 46 subjects demonstrate an authentication accuracy of 98%, validating the robustness of the model to noise and variability between subjects. Due to its efficiency, scalability, and inherent liveness detection capability, the proposed system is well-suited for real-world mobile and embedded biometric security applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.SP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04083v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04083v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04083v1">http://arxiv.org/abs/2511.04083v1</a><br>We study CT image denoising in the unpaired and self-supervised regimes by evaluating two strong, training-data-efficient paradigms: a CycleGAN-based residual translator and a Noise2Score (N2S) score-matching denoiser. Under a common evaluation protocol, a configuration sweep identifies a simple standard U-Net backbone within CycleGAN (lambda_cycle &#x3D; 30, lambda_iden &#x3D; 2, ngf &#x3D; ndf &#x3D; 64) as the most reliable setting; we then train it to convergence with a longer schedule. The selected CycleGAN improves the noisy input from 34.66 dB &#x2F; 0.9234 SSIM to 38.913 dB &#x2F; 0.971 SSIM and attains an estimated score of 1.9441 and an unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly behind in absolute PSNR &#x2F; SSIM, achieves large gains over very noisy inputs, highlighting its utility when clean pairs are unavailable. Overall, CycleGAN offers the strongest final image quality, whereas Noise2Score provides a robust pair-free alternative with competitive performance. Source code is available at <a href="https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score">https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04084v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04084v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04084v1">http://arxiv.org/abs/2511.04084v1</a><br>Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: <a href="https://github.com/nsapkota417/UKAST">https://github.com/nsapkota417/UKAST</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SpatialLock: Precise Spatial Control in Text-to-Image Synthesis</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04112v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04112v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04112v1">http://arxiv.org/abs/2511.04112v1</a><br>Text-to-Image (T2I) synthesis has made significant advancements in recent years, driving applications such as generating datasets automatically. However, precise control over object localization in generated images remains a challenge. Existing methods fail to fully utilize positional information, leading to an inadequate understanding of object spatial layouts. To address this issue, we propose SpatialLock, a novel framework that leverages perception signals and grounding information to jointly control the generation of spatial locations. SpatialLock incorporates two components: Position-Engaged Injection (PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial information through an attention layer, encouraging the model to learn the grounding information effectively. PoG employs perception-based supervision to further refine object localization. Together, these components enable the model to generate objects with precise spatial arrangements and improve the visual quality of the generated images. Experiments show that SpatialLock sets a new state-of-the-art for precise object positioning, achieving IOU scores above 0.9 across multiple datasets.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04117v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04117v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04117v1">http://arxiv.org/abs/2511.04117v1</a><br>In this paper, we propose Tortoise and Hare Guidance (THG), a training-free strategy that accelerates diffusion sampling while maintaining high-fidelity generation. We demonstrate that the noise estimate and the additional guidance term exhibit markedly different sensitivity to numerical error by reformulating the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our error-bound analysis shows that the additional guidance branch is more robust to approximation, revealing substantial redundancy that conventional solvers fail to exploit. Building on this insight, THG significantly reduces the computation of the additional guidance: the noise estimate is integrated with the tortoise equation on the original, fine-grained timestep grid, while the additional guidance is integrated with the hare equation only on a coarse grid. We also introduce (i) an error-bound-aware timestep sampler that adaptively selects step sizes and (ii) a guidance-scale scheduler that stabilizes large extrapolation spans. THG reduces the number of function evaluations (NFE) by up to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward $\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free accelerators under identical computation budgets. Our findings highlight the potential of multirate formulations for diffusion solvers, paving the way for real-time high-quality image synthesis without any model retraining. The source code is available at <a href="https://github.com/yhlee-add/THG">https://github.com/yhlee-add/THG</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text to Sketch Generation with Multi-Styles</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04123v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04123v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04123v1">http://arxiv.org/abs/2511.04123v1</a><br>Recent advances in vision-language models have facilitated progress in sketch generation. However, existing specialized methods primarily focus on generic synthesis and lack mechanisms for precise control over sketch styles. In this work, we propose a training-free framework based on diffusion models that enables explicit style guidance via textual prompts and referenced style sketches. Unlike previous style transfer methods that overwrite key and value matrices in self-attention, we incorporate the reference features as auxiliary information with linear smoothing and leverage a style-content guidance mechanism. This design effectively reduces content leakage from reference sketches and enhances synthesis quality, especially in cases with low structural similarity between reference and target sketches. Furthermore, we extend our framework to support controllable multi-style generation by integrating features from multiple reference sketches, coordinated via a joint AdaIN module. Extensive experiments demonstrate that our approach achieves high-quality sketch generation with accurate style alignment and improved flexibility in style control. The official implementation of M3S is available at <a href="https://github.com/CMACH508/M3S">https://github.com/CMACH508/M3S</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04126v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04126v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04126v1">http://arxiv.org/abs/2511.04126v1</a><br>This study presents a complete pipeline for automated tennis match analysis. Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference. Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times. The experimental results demonstrate robust performance in varying court conditions and match scenarios. The model outputs an annotated video along with detailed performance metrics, enabling coaches, broadcasters, and players to gain actionable insights into the dynamics of the game.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04128v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04128v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04128v1">http://arxiv.org/abs/2511.04128v1</a><br>Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: <a href="https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-">https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning from Online Videos at Inference Time for Computer-Use Agents</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04137v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04137v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04137v1">http://arxiv.org/abs/2511.04137v1</a><br>Computer-use agents can operate computers and automate laborious tasks, but despite recent rapid progress, they still lag behind human users, especially when tasks require domain-specific procedural knowledge about particular applications, platforms, and multi-step workflows. Humans can bridge this gap by watching video tutorials: we search, skim, and selectively imitate short segments that match our current subgoal. In this paper, we study how to enable computer-use agents to learn from online videos at inference time effectively. We propose a framework that retrieves and filters tutorial videos, converts them into structured demonstration trajectories, and dynamically selects trajectories as in-context guidance during execution. Particularly, using a VLM, we infer UI actions, segment videos into short subsequences of actions, and assign each subsequence a textual objective. At inference time, a two-stage selection mechanism dynamically chooses a single trajectory to add in context at each step, focusing the agent on the most helpful local guidance for its next decision. Experiments on two widely used benchmarks show that our framework consistently outperforms strong base agents and variants that use only textual tutorials or transcripts. Analyses highlight the importance of trajectory segmentation and selection, action filtering, and visual information, suggesting that abundant online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time. Our code is available at <a href="https://github.com/UCSB-NLP-Chang/video_demo">https://github.com/UCSB-NLP-Chang/video_demo</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing Straight: Document Orientation Detection for Efficient OCR</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04161v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04161v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04161v1">http://arxiv.org/abs/2511.04161v1</a><br>Despite significant advances in document understanding, determining the correct orientation of scanned or photographed documents remains a critical pre-processing step in the real world settings. Accurate rotation correction is essential for enhancing the performance of downstream tasks such as Optical Character Recognition (OCR) where misalignment commonly arises due to user errors, particularly incorrect base orientations of the camera during capture. In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from rotation-transformed structured and free-form English OCR datasets, and (ii) ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource languages. We also present a fast, robust and lightweight rotation classification pipeline built on the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned specifically for 4-class rotation task in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy on identifying the rotations respectively on both the datasets. Beyond classification, we demonstrate the critical role of our module in boosting OCR performance: closed-source (up to 14%) and open-weights models (up to 4x) in the simulated real-world setting.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04171v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04171v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04171v1">http://arxiv.org/abs/2511.04171v1</a><br>Image registration refers to the process of spatially aligning two or more images by mapping them into a common coordinate system, so that corresponding anatomical or tissue structures are matched across images. In digital pathology, registration enables direct comparison and integration of information from different stains or imaging modalities, sup-porting applications such as biomarker analysis and tissue reconstruction. Accurate registration of images from different modalities is an essential step in digital pathology. In this study, we investigated how various color transformation techniques affect image registration between hematoxylin and eosin (H&amp;E) stained images and non-linear multimodal images. We used a dataset of 20 tissue sample pairs, with each pair undergoing several preprocessing steps, including different color transformation (CycleGAN, Macenko, Reinhard, Vahadane), inversion, contrast adjustment, intensity normalization, and denoising. All images were registered using the VALIS registration method, which first applies rigid registration and then performs non-rigid registration in two steps on both low and high-resolution images. Registration performance was evaluated using the relative Target Registration Error (rTRE). We reported the median of median rTRE values (MMrTRE) and the average of median rTRE values (AMrTRE) for each method. In addition, we performed a custom point-based evaluation using ten manually selected key points. Registration was done separately for two scenarios, using either the original or inverted multimodal images. In both scenarios, CycleGAN color transformation achieved the lowest registration errors, while the other methods showed higher errors. These findings show that applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04190v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04190v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04190v1">http://arxiv.org/abs/2511.04190v1</a><br>Covariance descriptors capture second-order statistics of image features. They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging. We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices. We propose constructing covariance descriptors from features extracted by pre-trained general vision encoders (GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and MedSAM - are evaluated across eleven binary and multi-class datasets from the MedMNSIT benchmark. Our results show that covariance descriptors derived from GVE features consistently outperform those derived from handcrafted features. Moreover, SPDNet yields superior performance to state-of-the-art methods when combined with DINOv2 features. Our findings highlight the potential of combining covariance descriptors with powerful pretrained vision encoders for medical image analysis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AStF: Motion Style Transfer via Adaptive Statistics Fusor</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04192v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04192v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04192v1">http://arxiv.org/abs/2511.04192v1</a><br>Human motion style transfer allows characters to appear less rigidity and more realism with specific style. Traditional arbitrary image style transfer typically process mean and variance which is proved effective. Meanwhile, similar methods have been adapted for motion style transfer. However, due to the fundamental differences between images and motion, relying on mean and variance is insufficient to fully capture the complex dynamic patterns and spatiotemporal coherence properties of motion data. Building upon this, our key insight is to bring two more coefficient, skewness and kurtosis, into the analysis of motion style. Specifically, we propose a novel Adaptive Statistics Fusor (AStF) which consists of Style Disentanglement Module (SDM) and High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in conjunction with a Motion Consistency Regularization (MCR) discriminator. Experimental results show that, by providing a more comprehensive model of the spatiotemporal statistical patterns inherent in dynamic styles, our proposed AStF shows proficiency superiority in motion style transfers over state-of-the-arts. Our code and model are available at <a href="https://github.com/CHMimilanlan/AStF">https://github.com/CHMimilanlan/AStF</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04255v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04255v1">http://arxiv.org/abs/2511.04255v1</a><br>This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at <a href="https://github.com/xmed-lab/MedSapiens">https://github.com/xmed-lab/MedSapiens</a> .</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04260v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04260v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04260v1">http://arxiv.org/abs/2511.04260v1</a><br>The growing sophistication of synthetic image and deepfake generation models has turned source attribution and authenticity verification into a critical challenge for modern computer vision systems. Recent studies suggest that diffusion pipelines unintentionally imprint persistent statistical traces, known as signal leaks, within their outputs, particularly in latent representations. Building on this observation, we propose Proto-LeakNet, a signal-leak-aware and interpretable attribution framework that integrates closed-set classification with a density-based open-set evaluation on the learned embeddings, enabling analysis of unseen generators without retraining. Operating in the latent domain of diffusion models, our method re-simulates partial forward diffusion to expose residual generator-specific cues. A temporal attention encoder aggregates multi-step latent features, while a feature-weighted prototype head structures the embedding space and enables transparent attribution. Trained solely on closed data and achieving a Macro AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under post-processing, surpassing state-of-the-art methods, and achieves strong separability between known and unseen generators. These results demonstrate that modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics. The code for the whole work will be available upon submission.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04281v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04281v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04281v1">http://arxiv.org/abs/2511.04281v1</a><br>Video-based Visible-Infrared person re-identification (VVI-ReID) aims to retrieve the same pedestrian across visible and infrared modalities from video sequences. Existing methods tend to exploit modality-invariant visual features but largely overlook gait features, which are not only modality-invariant but also rich in temporal dynamics, thus limiting their ability to model the spatiotemporal consistency essential for cross-modal video matching. To address these challenges, we propose a DINOv2-Driven Gait Representation Learning (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn gait features complementary to appearance cues, facilitating robust sequence-level representations for cross-modal retrieval. Specifically, we introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which generates and enhances silhouette representations with general-purpose semantic priors from DINOv2 and jointly optimizes them with the ReID objective to achieve semantically enriched and task-adaptive gait feature learning. Furthermore, we develop a Progressive Bidirectional Multi-Granularity Enhancement (PBMGE) module, which progressively refines feature representations by enabling bidirectional interactions between gait and appearance streams across multiple spatial granularities, fully leveraging their complementarity to enhance global representations with rich local details and produce highly discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate the superiority of our approach, significantly outperforming existing state-of-the-art methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FastGS: Training 3D Gaussian Splatting in 100 Seconds</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04283v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04283v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04283v1">http://arxiv.org/abs/2511.04283v1</a><br>The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &amp; Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at <a href="https://fastgs.github.io/">https://fastgs.github.io/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T40(Primary)68T45, 68U99 (Secondary)</tag>
      
      <tag>I.4.8; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04288v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04288v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04288v1">http://arxiv.org/abs/2511.04288v1</a><br>Herbicide field trials require accurate identification of plant species and assessment of herbicide-induced damage across diverse environments. While general-purpose vision foundation models have shown promising results in complex visual domains, their performance can be limited in agriculture, where fine-grained distinctions between species and damage types are critical.   In this work, we adapt a general-purpose vision foundation model to herbicide trial characterization. Trained using a self-supervised learning approach on a large, curated agricultural dataset, the model learns rich and transferable representations optimized for herbicide trials images.   Our domain-specific model significantly outperforms the best general-purpose foundation model in both species identification (F1 score improvement from 0.91 to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions (new locations and other time), it achieves even greater gains (species identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In domain-shift scenarios, such as drone imagery, it maintains strong performance (species classification from 0.49 to 0.60).   Additionally, we show that domain-specific pretraining enhances segmentation accuracy, particularly in low-annotation regimes. An annotation-efficiency analysis reveals that, under unseen conditions, the domain-specific model achieves 5.4% higher F1 score than the general-purpose model, while using 80% fewer labeled samples.   These results demonstrate the generalization capabilities of domain-specific foundation models and their potential to significantly reduce manual annotation efforts, offering a scalable and automated solution for herbicide trial analysis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04304v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04304v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04304v1">http://arxiv.org/abs/2511.04304v1</a><br>The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04317v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04317v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04317v1">http://arxiv.org/abs/2511.04317v1</a><br>Most text-to-video(T2V) diffusion models depend on pre-trained text encoders for semantic alignment, yet they often fail to maintain video quality when provided with concise prompts rather than well-designed ones. The primary issue lies in their limited textual semantics understanding. Moreover, these text encoders cannot rephrase prompts online to better align with user intentions, which limits both the scalability and usability of the models, To address these challenges, we introduce RISE-T2V, which uniquely integrates the processes of prompt rephrasing and semantic feature extraction into a single and seamless step instead of two separate steps. RISE-T2V is universal and can be applied to various pre-trained LLMs and video diffusion models(VDMs), significantly enhancing their capabilities for T2V tasks. We propose an innovative module called the Rephrasing Adapter, enabling diffusion models to utilize text hidden states during the next token prediction of the LLM as a condition for video generation. By employing a Rephrasing Adapter, the video generation model can implicitly rephrase basic prompts into more comprehensive representations that better match the user’s intent. Furthermore, we leverage the powerful capabilities of LLMs to enable video generation models to accomplish a broader range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a versatile framework applicable to different video diffusion model architectures, significantly enhancing the ability of T2V models to generate high-quality videos that align with user intent. Visual results are available on the webpage at <a href="https://rise-t2v.github.io/">https://rise-t2v.github.io</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04334v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04334v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04334v1">http://arxiv.org/abs/2511.04334v1</a><br>The accurate delineation of tumours in radiological images like Computed Tomography is a very specialised and time-consuming task, and currently a bottleneck preventing quantitative analyses to be performed routinely in the clinical setting. For this reason, developing methods for the automated segmentation of tumours in medical imaging is of the utmost importance and has driven significant efforts in recent years. However, challenges regarding the impracticality of 3D scans, given the large amount of voxels to be analysed, usually requires the downsampling of such images or using patches thereof when applying traditional convolutional neural networks. To overcome this problem, in this paper we propose a new methodology that uses, divided into two stages, voxel sparsification and submanifold sparse convolutional networks. This method allows segmentations to be performed with high-resolution inputs and a native 3D model architecture, obtaining state-of-the-art accuracies while significantly reducing the computational resources needed in terms of GPU memory and time. We studied the deployment of this methodology in the context of Computed Tomography images of renal cancer patients from the KiTS23 challenge, and our method achieved results competitive with the challenge winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7% for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also offers significant computational improvements, achieving up to a 60% reduction in inference time and up to a 75% reduction in VRAM usage compared to an equivalent dense architecture, across both CPU and various GPU cards tested.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04344v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04344v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04344v1">http://arxiv.org/abs/2511.04344v1</a><br>This paper presents a comprehensive evaluation of nine convolutional neural network architectures for binary classification of horses and motorcycles in the VOC 2008 dataset. We address the significant class imbalance problem by implementing minority-class augmentation techniques. Our experiments compare modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and Vision Transformer across multiple performance metrics. Results demonstrate substantial performance variations, with ConvNeXt-Tiny achieving the highest Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle detection. We observe that data augmentation significantly improves minority class detection, particularly benefiting deeper architectures. This study provides insights into architecture selection for imbalanced binary classification tasks and quantifies the impact of data augmentation strategies in mitigating class imbalance issues in object detection.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04347v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04347v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04347v1">http://arxiv.org/abs/2511.04347v1</a><br>Accurate 3D object detection is essential for automated vehicles to navigate safely in complex real-world environments. Bird’s Eye View (BEV) representations, which project multi-sensor data into a top-down spatial format, have emerged as a powerful approach for robust perception. Although BEV-based fusion architectures have demonstrated strong performance through multimodal integration, the effects of sensor occlusions, caused by environmental conditions such as fog, haze, or physical obstructions, on 3D detection accuracy remain underexplored. In this work, we investigate the impact of occlusions on both camera and Light Detection and Ranging (LiDAR) outputs using the BEVFusion architecture, evaluated on the nuScenes dataset. Detection performance is measured using mean Average Precision (mAP) and the nuScenes Detection Score (NDS). Our results show that moderate camera occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is based only on the camera. On the other hand, LiDAR sharply drops in performance only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%), with a severe impact on long-range detection. In fused settings, the effect depends on which sensor is occluded: occluding the camera leads to a minor 4.1% drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8% drop (to 50.1%), revealing the model’s stronger reliance on LiDAR for the task of 3D object detection. Our results highlight the need for future research into occlusion-aware evaluation methods and improved sensor fusion techniques that can maintain detection accuracy in the presence of partial sensor failure or degradation due to adverse environmental conditions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04078v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04078v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04078v1">http://arxiv.org/abs/2511.04078v1</a><br>Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI remains a fundamental challenge due to subject variability and the entangled nature of visual features. Existing approaches primarily align neural activity directly with visual embeddings, but visual-only representations often fail to capture latent semantic dimensions, limiting interpretability and deep robustness. To address these limitations, we propose Bratrix, the first end-to-end framework to achieve multimodal Language-Anchored Vision-Brain alignment. Bratrix decouples visual stimuli into hierarchical visual and linguistic semantic components, and projects both visual and brain representations into a shared latent space, enabling the formation of aligned visual-language and brain-language embeddings. To emulate human-like perceptual reliability and handle noisy neural signals, Bratrix incorporates a novel uncertainty perception module that applies uncertainty-aware weighting during alignment. By leveraging learnable language-anchored semantic matrices to enhance cross-modal correlations and employing a two-stage training strategy of single-modality pretraining followed by multimodal fine-tuning, Bratrix-M improves alignment precision. Extensive experiments on EEG, MEG, and fMRI benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and captioning performance compared to state-of-the-art methods, specifically surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04349v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04349v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04349v1">http://arxiv.org/abs/2511.04349v1</a><br>Background In analytical chemistry, spatial information about materials is commonly captured through imaging techniques, such as traditional color cameras or with advanced hyperspectral cameras and microscopes. However, efficiently extracting and analyzing this spatial information for exploratory and predictive purposes remains a challenge, especially when using traditional chemometric methods. Recent advances in deep learning and artificial intelligence have significantly enhanced image processing capabilities, enabling the extraction of multiscale deep features that are otherwise challenging to capture with conventional image processing techniques. Despite the wide availability of open-source deep learning models, adoption in analytical chemistry remains limited because of the absence of structured, step-by-step guidance for implementing these models.   Results This tutorial aims to bridge this gap by providing a step-by-step guide for applying deep learning approaches to extract spatial information from imaging data and integrating it with other data sources, such as spectral information. Importantly, the focus of this work is not on training deep learning models for image processing but on using existing open source models to extract deep features from imaging data.   Significance The tutorial provides MATLAB code tutorial demonstrations, showcasing the processing of imaging data from various imaging modalities commonly encountered in analytical chemistry. Readers must run the tutorial steps on their own datasets using the codes presented in this tutorial.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04384v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04384v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04384v1">http://arxiv.org/abs/2511.04384v1</a><br>We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding. The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks. This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable. Extensive evaluation demonstrates that our approach substantially improves over single-task baselines in both answer accuracy and visual localization, highlighting the effectiveness of grounded multi-task learning for medical VQA applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04388v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04388v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04388v1">http://arxiv.org/abs/2511.04388v1</a><br>Depth estimation is one of the key technologies for realizing 3D perception in unmanned systems. Monocular depth estimation has been widely researched because of its low-cost advantage, but the existing methods face the challenges of poor depth estimation performance and blurred object boundaries on embedded systems. In this paper, we propose a novel monocular depth estimation model, BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate depth maps on embedded systems and significantly improves boundary quality. Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which adaptively fuses depth features to enhance boundary detail representation. Secondly, we integrate semantic knowledge into the encoder to improve the object recognition and boundary perception capabilities. Finally, BoRe-Depth is deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We demonstrate that the proposed model significantly outperforms previous lightweight models on multiple challenging datasets, and we provide detailed ablation studies for the proposed methods. The code is available at <a href="https://github.com/liangxiansheng093/BoRe-Depth">https://github.com/liangxiansheng093/BoRe-Depth</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04394v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04394v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04394v1">http://arxiv.org/abs/2511.04394v1</a><br>DORAEMON is an open-source PyTorch library that unifies visual object modeling and representation learning across diverse scales. A single YAML-driven workflow covers classification, retrieval and metric learning; more than 1000 pretrained backbones are exposed through a timm-compatible interface, together with modular losses, augmentations and distributed-training utilities. Reproducible recipes match or exceed reference results on ImageNet-1K, MS-Celeb-1M and Stanford online products, while one-command export to ONNX or HuggingFace bridges research and deployment. By consolidating datasets, models, and training techniques into one platform, DORAEMON offers a scalable foundation for rapid experimentation in visual recognition and representation learning, enabling efficient transfer of research advances to real-world applications. The repository is available at <a href="https://github.com/wuji3/DORAEMON">https://github.com/wuji3/DORAEMON</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On the Equivalence of Regression and Classification</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04422v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04422v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04422v1">http://arxiv.org/abs/2511.04422v1</a><br>A formal link between regression and classification has been tenuous. Even though the margin maximization term $|w|$ is used in support vector regression, it has at best been justified as a regularizer. We show that a regression problem with $M$ samples lying on a hyperplane has a one-to-one equivalence with a linearly separable classification task with $2M$ samples. We show that margin maximization on the equivalent classification task leads to a different regression formulation than traditionally used. Using the equivalence, we demonstrate a &#96;&#96;regressability’’ measure, that can be used to estimate the difficulty of regressing a dataset, without needing to first learn a model for it. We use the equivalence to train neural networks to learn a linearizing map, that transforms input variables into a space where a linear regressor is adequate.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T05, 68T10, 68Q32</tag>
      
      <tag>I.2.6; I.5.1; I.5.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04426v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04426v1">http://arxiv.org/abs/2511.04426v1</a><br>Analyzing octopuses in their natural habitats is challenging due to their camouflage capability, rapid changes in skin texture and color, non-rigid body deformations, and frequent occlusions, all of which are compounded by variable underwater lighting and turbidity. Addressing the lack of large-scale annotated datasets, this paper introduces HideAndSeg, a novel, minimally supervised AI-based tool for segmenting videos of octopuses. It establishes a quantitative baseline for this task. HideAndSeg integrates SAM2 with a custom-trained YOLOv11 object detector. First, the user provides point coordinates to generate the initial segmentation masks with SAM2. These masks serve as training data for the YOLO model. After that, our approach fully automates the pipeline by providing a bounding box prompt to SAM2, eliminating the need for further manual intervention. We introduce two unsupervised metrics - temporal consistency $DICE_t$ and new component count $NC_t$ - to quantitatively evaluate segmentation quality and guide mask refinement in the absence of ground-truth data, i.e., real-world information that serves to train, validate, and test AI models. Results show that HideAndSeg achieves satisfactory performance, reducing segmentation noise compared to the manually prompted approach. Our method can re-identify and segment the octopus even after periods of complete occlusion in natural environments, a scenario in which the manually prompted model fails. By reducing the need for manual analysis in real-world scenarios, this work provides a practical tool that paves the way for more efficient behavioral studies of wild cephalopods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Solving Convex Partition Visual Jigsaw Puzzles</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04450v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04450v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04450v1">http://arxiv.org/abs/2511.04450v1</a><br>Jigsaw puzzle solving requires the rearrangement of unordered pieces into their original pose in order to reconstruct a coherent whole, often an image, and is known to be an intractable problem. While the possible impact of automatic puzzle solvers can be disruptive in various application domains, most of the literature has focused on developing solvers for square jigsaw puzzles, severely limiting their practical use. In this work, we significantly expand the types of puzzles handled computationally, focusing on what is known as Convex Partitions, a major subset of polygonal puzzles whose pieces are convex. We utilize both geometrical and pictorial compatibilities, introduce a greedy solver, and report several performance measures next to the first benchmark dataset of such puzzles.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>V-Thinker: Interactive Thinking with Images</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04460v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04460v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04460v1">http://arxiv.org/abs/2511.04460v1</a><br>Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising “Thinking with Images” paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04474v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04474v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04474v1">http://arxiv.org/abs/2511.04474v1</a><br>Landslides cause severe damage to lives, infrastructure, and the environment, making accurate and timely mapping essential for disaster preparedness and response. However, conventional deep learning models often struggle when applied across different sensors, regions, or under conditions of limited training data. To address these challenges, we present a three-axis analytical framework of sensor, label, and domain for adapting geospatial foundation models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a series of experiments, we show that it consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE). The model, built on global pretraining, self-supervision, and adaptable fine-tuning, proved resilient to spectral variation, maintained accuracy under label scarcity, and generalized more reliably across diverse datasets and geographic settings. Alongside these strengths, we also highlight remaining challenges such as computational cost and the limited availability of reusable AI-ready training data for landslide research. Overall, our study positions GeoFMs as a step toward more robust and scalable approaches for landslide risk reduction and environmental monitoring.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04494v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04494v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04494v1">http://arxiv.org/abs/2511.04494v1</a><br>Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer’s output distribution, which can be expressed as $\lVert (W - \widetilde{W}) \Sigma^{1&#x2F;2}\rVert_F$ where $\Sigma^{1&#x2F;2}$ is the square root of the covariance matrix of the layer’s input and $W$, $\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18&#x2F;50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography via Implicit Neural Representation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04510v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04510v1">http://arxiv.org/abs/2511.04510v1</a><br>Fluorescence Molecular Tomography (FMT) is a promising technique for non-invasive 3D visualization of fluorescent probes, but its reconstruction remains challenging due to the inherent ill-posedness and reliance on inaccurate or often-unknown tissue optical properties. While deep learning methods have shown promise, their supervised nature limits generalization beyond training data. To address these problems, we propose $\mu$NeuFMT, a self-supervised FMT reconstruction framework that integrates implicit neural-based scene representation with explicit physical modeling of photon propagation. Its key innovation lies in jointly optimize both the fluorescence distribution and the optical properties ($\mu$) during reconstruction, eliminating the need for precise prior knowledge of tissue optics or pre-conditioned training data. We demonstrate that $\mu$NeuFMT robustly recovers accurate fluorophore distributions and optical coefficients even with severely erroneous initial values (0.5$\times$ to 2$\times$ of ground truth). Extensive numerical, phantom, and in vivo validations show that $\mu$NeuFMT outperforms conventional and supervised deep learning approaches across diverse heterogeneous scenarios. Our work establishes a new paradigm for robust and accurate FMT reconstruction, paving the way for more reliable molecular imaging in complex clinically related scenarios, such as fluorescence guided surgery.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>I.2.10; I.4.5</tag>
      
      <tag>physics.optics</tag>
      
      <tag>68T07, 78A46, 78A70, 92C55</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>THEval. Evaluation Framework for Talking Head Video Generation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04520v2/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04520v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04520v2">http://arxiv.org/abs/2511.04520v2</a><br>Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04525v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04525v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04525v1">http://arxiv.org/abs/2511.04525v1</a><br>Purpose: Accurate assessment of surgical complexity is essential in Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with longer operative times and increased risk of postoperative complications. The Parkland Grading Scale (PGS) provides a clinically validated framework for stratifying inflammation severity; however, its automation in surgical videos remains largely unexplored, particularly in realistic scenarios where complete videos must be analyzed without prior manual curation. Methods: In this work, we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity estimation in LC via the PGS, designed to operate under weak temporal supervision. Unlike prior methods limited to static images or manually trimmed clips, STC-Net operates directly on full videos. It jointly performs temporal localization and grading through a localization, window proposal, and grading module. We introduce a novel loss formulation combining hard and soft localization objectives and background-aware grading supervision. Results: Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by over 10% in both metrics and highlighting the effectiveness of weak supervision for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, making it promising for post-operative analysis and surgical training.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04555v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04555v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04555v1">http://arxiv.org/abs/2511.04555v1</a><br>Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04570v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04570v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04570v1">http://arxiv.org/abs/2511.04570v1</a><br>“Thinking with Text” and “Thinking with Images” paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce “Thinking with Video”, a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2’s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions “thinking with video” as a unified multimodal reasoning paradigm.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04583v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04583v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04583v1">http://arxiv.org/abs/2511.04583v1</a><br>Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04595v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04595v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04595v1">http://arxiv.org/abs/2511.04595v1</a><br>Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04357v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04357v1">http://arxiv.org/abs/2511.04357v1</a><br>Deploying autonomous robots that can learn new skills from demonstrations is an important challenge of modern robotics. Existing solutions often apply end-to-end imitation learning with Vision-Language Action (VLA) models or symbolic approaches with Action Model Learning (AML). On the one hand, current VLA models are limited by the lack of high-level symbolic planning, which hinders their abilities in long-horizon tasks. On the other hand, symbolic approaches in AML lack generalization and scalability perspectives. In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of human demonstrations. This representation is used to generate new planning domains during inference and serves as an orchestrator for low-level VLA policies, scaling up the number of actions that can be reproduced in a row. Our results show that GraSP-VLA is effective for modeling symbolic representations on the task of automatic planning domain generation from observations. In addition, results on real-world experiments show the potential of our Continuous Scene Graph representation to orchestrate low-level VLA policies in long-horizon tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04601v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04601v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04601v1">http://arxiv.org/abs/2511.04601v1</a><br>While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the model’s fine-grained vision-language alignment. However, the inherent token length limitation of CLIP’s text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIP’s original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04628v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04628v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04628v1">http://arxiv.org/abs/2511.04628v1</a><br>Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Polarization-resolved imaging improves eye tracking</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04652v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04652v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04652v1">http://arxiv.org/abs/2511.04652v1</a><br>Polarization-resolved near-infrared imaging adds a useful optical contrast mechanism to eye tracking by measuring the polarization state of light reflected by ocular tissues in addition to its intensity. In this paper we demonstrate how this contrast can be used to enable eye tracking. Specifically, we demonstrate that a polarization-enabled eye tracking (PET) system composed of a polarization–filter–array camera paired with a linearly polarized near-infrared illuminator can reveal trackable features across the sclera and gaze-informative patterns on the cornea, largely absent in intensity-only images. Across a cohort of 346 participants, convolutional neural network based machine learning models trained on data from PET reduced the median 95th-percentile absolute gaze error by 10–16% relative to capacity-matched intensity baselines under nominal conditions and in the presence of eyelid occlusions, eye-relief changes, and pupil-size variation. These results link light–tissue polarization effects to practical gains in human–computer interaction and position PET as a simple, robust sensing modality for future wearable devices.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>physics.optics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmark Designers Should &quot;Train on the Test Set&quot; to Expose Exploitable Non-Visual Shortcuts</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04655v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04655v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04655v1">http://arxiv.org/abs/2511.04655v1</a><br>Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to <code>game&#39;&#39; their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly </code>training on the test set’’ – probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a <code>Test-set Stress-Test&#39;&#39; (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via $k$-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score $s(x)$. We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an </code>Iterative Bias Pruning’’ (IBP) procedure. Applying this framework to four benchmarks – VSI-Bench, CV-Bench, MMMU, and VideoMME – we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04665v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04665v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04665v1">http://arxiv.org/abs/2511.04665v1</a><br>Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: <a href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04668v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04668v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04668v1">http://arxiv.org/abs/2511.04668v1</a><br>Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V – a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cambrian-S: Towards Spatial Supersensing in Video</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04670v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04670v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04670v1">http://arxiv.org/abs/2511.04670v1</a><br>We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04671v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04671v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04671v1">http://arxiv.org/abs/2511.04671v1</a><br>Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at <a href="https://portal-cornell.github.io/X-Diffusion/">https://portal-cornell.github.io/X-Diffusion/</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04675v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04675v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04675v1">http://arxiv.org/abs/2511.04675v1</a><br>We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04615v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04615v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04615v1">http://arxiv.org/abs/2511.04615v1</a><br>Deep learning models can generate virtual immunohistochemistry (IHC) stains from hematoxylin and eosin (H&amp;E) images, offering a scalable and low-cost alternative to laboratory IHC. However, reliable evaluation of image quality remains a challenge as current texture- and distribution-based metrics quantify image fidelity rather than the accuracy of IHC staining. Here, we introduce an automated and accuracy grounded framework to determine image quality across sixteen paired or unpaired image translation models. Using color deconvolution, we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by each virtual IHC model. We use the segmented masks of real and virtual IHC to compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly quantify correct pixel - level labeling without needing expert manual annotations. Our results demonstrate that conventional image fidelity metrics, including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM), correlate poorly with stain accuracy and pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based models are less reliable in providing accurate IHC positive pixel labels. Moreover, whole-slide images (WSI) reveal performance declines that are invisible in patch-based evaluations, emphasizing the need for WSI-level benchmarks. Together, this framework defines a reproducible approach for assessing the quality of virtual IHC models, a critical step to accelerate translation towards routine use by pathologists.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tracking and Understanding Object Transformations</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04678v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04678v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04678v1">http://arxiv.org/abs/2511.04678v1</a><br>Real-world objects frequently undergo state transformations. From an apple being cut into pieces to a butterfly emerging from its cocoon, tracking through these changes is important for understanding real-world objects and dynamics. However, existing methods often lose track of the target object after transformation, due to significant changes in object appearance. To address this limitation, we introduce the task of Track Any State: tracking objects through transformations while detecting and describing state changes, accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we present TubeletGraph, a zero-shot system that recovers missing objects after transformation and maps out how object states are evolving over time. TubeletGraph first identifies potentially overlooked tracks, and determines whether they should be integrated based on semantic and proximity priors. Then, it reasons about the added tracks and generates a state graph describing each observed transformation. TubeletGraph achieves state-of-the-art tracking performance under transformations, while demonstrating deeper understanding of object transformations and promising capabilities in temporal grounding and semantic reasoning for complex object transformations. Code, additional results, and the benchmark dataset are available at <a href="https://tubelet-graph.github.io/">https://tubelet-graph.github.io</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04679v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04679v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04679v1">http://arxiv.org/abs/2511.04679v1</a><br>Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04680v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04680v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04680v1">http://arxiv.org/abs/2511.04680v1</a><br>Automatic image cropping is a method for maximizing the human-perceived quality of cropped regions in photographs. Although several works have proposed techniques for producing singular crops, little work has addressed the problem of producing multiple, distinct crops with aesthetic appeal. In this paper, we motivate the problem with a discussion on modern social media applications, introduce a dataset of 277 relevant images and human labels, and evaluate the efficacy of several single-crop models with an image partitioning algorithm as a pre-processing step. The dataset is available at <a href="https://github.com/RafeLoya/carousel">https://github.com/RafeLoya/carousel</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04718v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04718v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04718v1">http://arxiv.org/abs/2511.04718v1</a><br>Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks   by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations,   treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within   specific frequency bands, limiting diagnostic sensitivity and specificity.   While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be   optimal for capturing individual variability or disease-specific alterations.   To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each   brain region and Frequency-Coupled Connectivity Learning to capture   both intra- and nuanced cross-band interactions in a unified functional   network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations   for diagnostic prediction. Experimental results on the ADNI and ABIDE   datasets demonstrate superior performance over existing methods. The   code is available at <a href="https://github.com/XXYY20221234/Ada-FCN">https://github.com/XXYY20221234/Ada-FCN</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04727v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04727v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04727v1">http://arxiv.org/abs/2511.04727v1</a><br>Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Knowledge-based anomaly detection for identifying network-induced shape artifacts</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04729v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04729v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04729v1">http://arxiv.org/abs/2511.04729v1</a><br>Synthetic data provides a promising approach to address data scarcity for training machine learning models; however, adoption without proper quality assessments may introduce artifacts, distortions, and unrealistic features that compromise model performance and clinical utility. This work introduces a novel knowledge-based anomaly detection method for detecting network-induced shape artifacts in synthetic images. The introduced method utilizes a two-stage framework comprising (i) a novel feature extractor that constructs a specialized feature space by analyzing the per-image distribution of angle gradients along anatomical boundaries, and (ii) an isolation forest-based anomaly detector. We demonstrate the effectiveness of the method for identifying network-induced shape artifacts in two synthetic mammography datasets from models trained on CSAW-M and VinDr-Mammo patient datasets respectively. Quantitative evaluation shows that the method successfully concentrates artifacts in the most anomalous partition (1st percentile), with AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study involving three imaging scientists confirmed that images identified by the method as containing network-induced shape artifacts were also flagged by human readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the most anomalous partition, approximately 1.5-2 times higher than the least anomalous partition. Kendall-Tau correlations between algorithmic and human rankings were 0.45 and 0.43 for the two datasets, indicating reasonable agreement despite the challenging nature of subtle artifact detection. This method is a step forward in the responsible use of synthetic data, as it allows developers to evaluate synthetic images for known anatomic constraints and pinpoint and address specific issues to improve the overall quality of a synthetic dataset.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CPO: Condition Preference Optimization for Controllable Image Generation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04753v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04753v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04753v1">http://arxiv.org/abs/2511.04753v1</a><br>To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t &lt; 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win–lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10%$ error rate reduction in segmentation, $70$–$80%$ in human pose, and consistent $2$–$5%$ reductions in edge and depth maps.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04766v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04766v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04766v1">http://arxiv.org/abs/2511.04766v1</a><br>Foundation models (FMs) offer powerful representations for geospatial analysis, but adapting them effectively remains challenging. Standard adaptation methods, whether full fine-tuning or efficient frozen-backbone approaches, typically employ decoders with fixed regularization strategies, failing to account for the significant heterogeneity in satellite imagery. We introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture designed to address this limitation. DARN integrates three key innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and (3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide theoretical justifications linking DARN’s optimization to stationary point convergence and its mechanism to adaptive information bottlenecks. Empirically, DARN demonstrates exceptional performance across both major adaptation paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering substantial advantages crucial for real-world deployment: superior out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms), enhanced robustness (17% relative reduction in corruption error), and improved performance on minority classes. DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Global 3D Reconstruction of Clouds &amp; Tropical Cyclones</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04773v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04773v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04773v1">http://arxiv.org/abs/2511.04773v1</a><br>Accurate forecasting of tropical cyclones (TCs) remains challenging due to limited satellite observations probing TC structure and difficulties in resolving cloud properties involved in TC intensification. Recent research has demonstrated the capabilities of machine learning methods for 3D cloud reconstruction from satellite observations. However, existing approaches have been restricted to regions where TCs are uncommon, and are poorly validated for intense storms. We introduce a new framework, based on a pre-training–fine-tuning pipeline, that learns from multiple satellites with global coverage to translate 2D satellite imagery into 3D cloud maps of relevant cloud properties. We apply our model to a custom-built TC dataset to evaluate performance in the most challenging and relevant conditions. We show that we can - for the first time - create global instantaneous 3D cloud maps and accurately reconstruct the 3D structure of intense storms. Our model not only extends available satellite observations but also provides estimates when observations are missing entirely. This is crucial for advancing our understanding of TC intensification and improving forecasts.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>physics.ao-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04779v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04779v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04779v1">http://arxiv.org/abs/2511.04779v1</a><br>Event-based cameras are becoming a popular solution for efficient, low-power eye tracking. Due to the sparse and asynchronous nature of event data, they require less processing power and offer latencies in the microsecond range. However, many existing solutions are limited to validation on powerful GPUs, with no deployment on real embedded devices. In this paper, we present EETnet, a convolutional neural network designed for eye tracking using purely event-based data, capable of running on microcontrollers with limited resources. Additionally, we outline a methodology to train, evaluate, and quantize the network using a public dataset. Finally, we propose two versions of the architecture: a classification model that detects the pupil on a grid superimposed on the original image, and a regression model that operates at the pixel level.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3D Gaussian Point Encoders</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04797v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04797v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04797v1">http://arxiv.org/abs/2511.04797v1</a><br>In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04803v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04803v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04803v1">http://arxiv.org/abs/2511.04803v1</a><br>Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at <a href="https://github.com/MMV-Lab/biomedseg-efficiency">https://github.com/MMV-Lab/biomedseg-efficiency</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.10; I.4.6</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04811v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04811v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04811v1">http://arxiv.org/abs/2511.04811v1</a><br>Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net’s self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at <a href="https://github.com/MMV-Lab/AL_BioMed_img_seg">https://github.com/MMV-Lab/AL_BioMed_img_seg</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07, 68U10</tag>
      
      <tag>I.2.10; I.4.6; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Geometry Denoising with Preferred Normal Vectors</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04848v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04848v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04848v1">http://arxiv.org/abs/2511.04848v1</a><br>We introduce a new paradigm for geometry denoising using prior knowledge about the surface normal vector. This prior knowledge comes in the form of a set of preferred normal vectors, which we refer to as label vectors. A segmentation problem is naturally embedded in the denoising process. The segmentation is based on the similarity of the normal vector to the elements of the set of label vectors. Regularization is achieved by a total variation term. We formulate a split Bregman (ADMM) approach to solve the resulting optimization problem. The vertex update step is based on second-order shape calculus.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>math.OC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04864v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04864v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04864v1">http://arxiv.org/abs/2511.04864v1</a><br>Recovering high-quality surfaces from irregular point cloud is ill-posed unless strong geometric priors are available. We introduce an implicit self-prior approach that distills a shape-specific prior directly from the input point cloud itself and embeds it within an implicit neural representation. This is achieved by jointly training a small dictionary of learnable embeddings with an implicit distance field; at every query location, the field attends to the dictionary via cross-attention, enabling the network to capture and reuse repeating structures and long-range correlations inherent to the shape. Optimized solely with self-supervised point cloud reconstruction losses, our approach requires no external training data. To effectively integrate this learned prior while preserving input fidelity, the trained field is then sampled to extract densely distributed points and analytic normals via automatic differentiation. We integrate the resulting dense point cloud and corresponding normals into a robust implicit moving least squares (RIMLS) formulation. We show this hybrid strategy preserves fine geometric details in the input data, while leveraging the learned prior to regularize sparse regions. Experiments show that our method outperforms both classical and learning-based approaches in generating high-fidelity surfaces with superior detail preservation and robustness to common data degradations.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04871v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04871v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04871v1">http://arxiv.org/abs/2511.04871v1</a><br>Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps are effective for assessing neurodegenerative diseases and microstructural properties of white matter in large number of brain conditions. However, DW-MRI inherently limits the combination of data from multiple acquisition sites without harmonization to mitigate scanner-specific biases. While the widely used ComBAT method reduces site effects in research, its reliance on linear covariate relationships, homogeneous populations, fixed site numbers, and well populated sites constrains its clinical use. To overcome these limitations, we propose Clinical-ComBAT, a method designed for real-world clinical scenarios. Clinical-ComBAT harmonizes each site independently, enabling flexibility as new data and clinics are introduced. It incorporates a non-linear polynomial data model, site-specific harmonization referenced to a normative site, and variance priors adaptable to small cohorts. It further includes hyperparameter tuning and a goodness-of-fit metric for harmonization assessment. We demonstrate its effectiveness on simulated and real data, showing improved alignment of diffusion metrics and enhanced applicability for normative modeling.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04872v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04872v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04872v1">http://arxiv.org/abs/2511.04872v1</a><br>This study evaluates the efficacy of vision transformer models, specifically Swin transformers, in enhancing the diagnostic accuracy of ear diseases compared to traditional convolutional neural networks. With a reported 27% misdiagnosis rate among specialist otolaryngologists, improving diagnostic accuracy is crucial. The research utilised a real-world dataset from the Department of Otolaryngology at the Clinical Hospital of the Universidad de Chile, comprising otoscopic videos of ear examinations depicting various middle and external ear conditions. Frames were selected based on the Laplacian and Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively, marginally outperforming the ResNet model (99.5%). These results surpassed metrics reported in related studies. However, the evaluation uncovered a critical data leakage issue in the preprocessing step, affecting both this study and related research using the same raw dataset. After mitigating the data leakage, model performance decreased significantly. Corrected accuracies were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This finding highlights the importance of rigorous data handling in machine learning studies, especially in medical applications. The findings indicate that while vision transformers show promise, it is essential to find an optimal balance between the benefits of advanced model architectures and those derived from effective data preprocessing. This balance is key to developing a reliable machine learning model for diagnosing ear diseases.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title>
    <link href="/2025/11/06/highlights/2025-11-06-2511_04834v1/"/>
    <url>/2025/11/06/highlights/2025-11-06-2511_04834v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04834v1">http://arxiv.org/abs/2511.04834v1</a><br>Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices</title>
    <link href="/2025/11/05/cs.AR/2025-11-05-2511_03765v2/"/>
    <url>/2025/11/05/cs.AR/2025-11-05-2511_03765v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03765v2">http://arxiv.org/abs/2511.03765v2</a><br>On-device fine-tuning of CNNs is essential to withstand domain shift in edge applications such as Human Activity Recognition (HAR), yet full fine-tuning is infeasible under strict memory, compute, and energy budgets. We present LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional layers, (ii) selectively updates only the output-side core with zero-initialization to keep the auxiliary path inactive at the start, and (iii) fuses the update back into dense kernels, leaving inference cost unchanged. This design preserves convolutional structure and reduces the number of trainable parameters by up to two orders of magnitude compared to full fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, consistently outperforming prior parameter-efficient baselines under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and selective-core training yield 1.4-3.8x faster convergence to target F1. LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN adaptation practical for edge platforms.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03120v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03120v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03120v1">http://arxiv.org/abs/2511.03120v1</a><br>Integrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03132v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03132v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03132v1">http://arxiv.org/abs/2511.03132v1</a><br>This paper presents the first AI&#x2F;ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI&#x2F;ML for damage assessment during a disaster and lessons learned to the benefit of the AI&#x2F;ML research and user communities.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03206v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03206v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03206v1">http://arxiv.org/abs/2511.03206v1</a><br>Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative deep learning for foundational video translation in ultrasound</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03255v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03255v1">http://arxiv.org/abs/2511.03255v1</a><br>Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+&#x2F;-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+&#x2F;-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+&#x2F;-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03367v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03367v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03367v1">http://arxiv.org/abs/2511.03367v1</a><br>Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: <a href="https://github.com/Gahyeonkim09/AAPL">https://github.com/Gahyeonkim09/AAPL</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03328v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03328v1">http://arxiv.org/abs/2511.03328v1</a><br>A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of “reasoning MLLMs” that offer explicit control over their internal thinking processes (normally referred as the “thinking mode”) alongside the standard “non-thinking mode”. This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these “dual-state” MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active “thinking mode” capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03855v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03855v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03855v1">http://arxiv.org/abs/2511.03855v1</a><br>Deep learned (DL) models for image recognition have been shown to fail to generalize to data from different devices, populations, etc. COVID-19 detection from Chest X-rays (CXRs), in particular, has been shown to fail to generalize to out-of-distribution (OOD) data from new clinical sources not covered in the training set. This occurs because models learn to exploit shortcuts - source-specific artifacts that do not translate to new distributions - rather than reasonable biomarkers to maximize performance on in-distribution (ID) data. Rendering the models more robust to distribution shifts, our study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during training. Our empirical results demonstrate that this technique can significantly reduce the performance gap between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results averaged over ten random seeds across key metrics such as AUC, F1, accuracy, recall and specificity. Our source code is publicly available at <a href="https://github.com/Duongmai127/Noisy-ood">https://github.com/Duongmai127/Noisy-ood</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03882v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03882v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03882v1">http://arxiv.org/abs/2511.03882v1</a><br>Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03891v2/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03891v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03891v2">http://arxiv.org/abs/2511.03891v2</a><br>Small, imbalanced datasets and poor input image quality can lead to high false predictions rates with deep learning models. This paper introduces Class-Based Image Composition, an approach that allows us to reformulate training inputs through a fusion of multiple images of the same class into combined visual composites, named Composite Input Images (CoImg). That enhances the intra-class variance and improves the valuable information density per training sample and increases the ability of the model to distinguish between subtle disease patterns. Our method was evaluated on the Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et al., 2024), which contains 2,064 high-resolution optical coherence tomography (OCT) scans of the human retina, representing seven distinct diseases with a significant class imbalance. We constructed a perfectly class-balanced version of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout composite image. To assess the effectiveness of this new representation, we conducted a comparative analysis between the original dataset and its variant using a VGG16 model. A fair comparison was ensured by utilizing the identical model architecture and hyperparameters for all experiments. The proposed approach markedly improved diagnostic results.The enhanced Dataset achieved near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared to a baseline model trained on raw dataset. The false prediction rate was also significantly lower, this demonstrates that the method can producehigh-quality predictions even for weak datasets affected by class imbalance or small sample size.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.DB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>I Detect What I Don&#39;t Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging</title>
    <link href="/2025/11/05/cs.AI/2025-11-05-2511_03912v1/"/>
    <url>/2025/11/05/cs.AI/2025-11-05-2511_03912v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03912v1">http://arxiv.org/abs/2511.03912v1</a><br>Unknown anomaly detection in medical imaging remains a fundamental challenge due to the scarcity of labeled anomalies and the high cost of expert supervision. We introduce an unsupervised, oracle-free framework that incrementally expands a trusted set of normal samples without any anomaly labels. Starting from a small, verified seed of normal images, our method alternates between lightweight adapter updates and uncertainty-gated sample admission. A frozen pretrained vision backbone is augmented with tiny convolutional adapters, ensuring rapid domain adaptation with negligible computational overhead. Extracted embeddings are stored in a compact coreset enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during incremental expansion is enforced by dual probabilistic gates, a sample is admitted into the normal memory only if its distance to the existing coreset lies within a calibrated z-score threshold, and its SWAG-based epistemic uncertainty remains below a seed-calibrated bound. This mechanism prevents drift and false inclusions without relying on generative reconstruction or replay buffers. Empirically, our system steadily refines the notion of normality as unlabeled data arrive, producing substantial gains over baselines. On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5, ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These results highlight the effectiveness and efficiency of the proposed framework for real-world, label-scarce medical imaging applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
    <link href="/2025/11/05/cs.CL/2025-11-05-2511_03328v1/"/>
    <url>/2025/11/05/cs.CL/2025-11-05-2511_03328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03328v1">http://arxiv.org/abs/2511.03328v1</a><br>A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of “reasoning MLLMs” that offer explicit control over their internal thinking processes (normally referred as the “thinking mode”) alongside the standard “non-thinking mode”. This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these “dual-state” MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active “thinking mode” capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
    <link href="/2025/11/05/cs.CY/2025-11-05-2511_03132v1/"/>
    <url>/2025/11/05/cs.CY/2025-11-05-2511_03132v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03132v1">http://arxiv.org/abs/2511.03132v1</a><br>This paper presents the first AI&#x2F;ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI&#x2F;ML for damage assessment during a disaster and lessons learned to the benefit of the AI&#x2F;ML research and user communities.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition</title>
    <link href="/2025/11/05/cs.DB/2025-11-05-2511_03891v2/"/>
    <url>/2025/11/05/cs.DB/2025-11-05-2511_03891v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03891v2">http://arxiv.org/abs/2511.03891v2</a><br>Small, imbalanced datasets and poor input image quality can lead to high false predictions rates with deep learning models. This paper introduces Class-Based Image Composition, an approach that allows us to reformulate training inputs through a fusion of multiple images of the same class into combined visual composites, named Composite Input Images (CoImg). That enhances the intra-class variance and improves the valuable information density per training sample and increases the ability of the model to distinguish between subtle disease patterns. Our method was evaluated on the Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et al., 2024), which contains 2,064 high-resolution optical coherence tomography (OCT) scans of the human retina, representing seven distinct diseases with a significant class imbalance. We constructed a perfectly class-balanced version of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout composite image. To assess the effectiveness of this new representation, we conducted a comparative analysis between the original dataset and its variant using a VGG16 model. A fair comparison was ensured by utilizing the identical model architecture and hyperparameters for all experiments. The proposed approach markedly improved diagnostic results.The enhanced Dataset achieved near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared to a baseline model trained on raw dataset. The false prediction rate was also significantly lower, this demonstrates that the method can producehigh-quality predictions even for weak datasets affected by class imbalance or small sample size.</p>]]></content>
    
    
    <categories>
      
      <category>cs.DB</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.DB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</title>
    <link href="/2025/11/05/cs.GR/2025-11-05-2511_03147v1/"/>
    <url>/2025/11/05/cs.GR/2025-11-05-2511_03147v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03147v1">http://arxiv.org/abs/2511.03147v1</a><br>Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Accelerating Physical Property Reasoning for Augmented Visual Cognition</title>
    <link href="/2025/11/05/cs.HC/2025-11-05-2511_03126v1/"/>
    <url>/2025/11/05/cs.HC/2025-11-05-2511_03126v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03126v1">http://arxiv.org/abs/2511.03126v1</a><br>This paper introduces \sysname, a system that accelerates vision-guided physical property reasoning to enable augmented visual cognition. \sysname minimizes the run-time latency of this reasoning pipeline through a combination of both algorithmic and systematic optimizations, including rapid geometric 3D reconstruction, efficient semantic feature fusion, and parallel view encoding. Through these simple yet effective optimizations, \sysname reduces the end-to-end latency of this reasoning pipeline from 10–20 minutes to less than 6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname achieves this 62.9$\times$–287.2$\times$ speedup while not only reaching on-par (and sometimes slightly better) object-level physical property estimation accuracy(e.g. mass), but also demonstrating superior performance in material segmentation and voxel-level inference than two SOTA baselines. We further combine gaze-tracking with \sysname to localize the object of interest in cluttered, real-world environments, streamlining the physical property reasoning on smart glasses. The case study with Meta Aria Glasses conducted at an IKEA furniture store demonstrates that \sysname achives consistently high performance compared to controlled captures, providing robust property estimations even with fewer views in real-world scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupled Entropy Minimization</title>
    <link href="/2025/11/05/cs.IT/2025-11-05-2511_03256v1/"/>
    <url>/2025/11/05/cs.IT/2025-11-05-2511_03256v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03256v1">http://arxiv.org/abs/2511.03256v1</a><br>Entropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.IT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>stat.ML</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>math.ST</tag>
      
      <tag>stat.TH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03147v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03147v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03147v1">http://arxiv.org/abs/2511.03147v1</a><br>Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test Time Adaptation Using Adaptive Quantile Recalibration</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03148v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03148v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03148v1">http://arxiv.org/abs/2511.03148v1</a><br>Domain adaptation is a key strategy for enhancing the generalizability of deep learning models in real-world scenarios, where test distributions often diverge significantly from the training domain. However, conventional approaches typically rely on prior knowledge of the target domain or require model retraining, limiting their practicality in dynamic or resource-constrained environments. Recent test-time adaptation methods based on batch normalization statistic updates allow for unsupervised adaptation, but they often fail to capture complex activation distributions and are constrained to specific normalization layers. We propose Adaptive Quantile Recalibration (AQR), a test-time adaptation technique that modifies pre-activation distributions by aligning quantiles on a channel-wise basis. AQR captures the full shape of activation distributions and generalizes across architectures employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of estimating distribution tails under varying batch sizes, AQR incorporates a robust tail calibration strategy that improves stability and precision. Our method leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining models. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR achieves robust adaptation across diverse settings, outperforming existing test-time adaptation baselines. These results highlight AQR’s potential for deployment in real-world scenarios with dynamic and unpredictable data distributions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Probabilistic U-Net Approach to Downscaling Climate Simulations</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03197v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03197v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03197v1">http://arxiv.org/abs/2511.03197v1</a><br>Climate models are limited by heavy computational costs, often producing outputs at coarse spatial resolutions, while many climate change impact studies require finer scales. Statistical downscaling bridges this gap, and we adapt the probabilistic U-Net for this task, combining a deterministic U-Net backbone with a variational latent space to capture aleatoric uncertainty. We evaluate four training objectives, afCRPS and WMSE-MS-SSIM with three settings for downscaling precipitation and temperature from $16\times$ coarser resolution. Our main finding is that WMSE-MS-SSIM performs well for extremes under certain settings, whereas afCRPS better captures spatial variability across scales.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>physics.ao-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03206v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03206v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03206v1">http://arxiv.org/abs/2511.03206v1</a><br>Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03239v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03239v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03239v1">http://arxiv.org/abs/2511.03239v1</a><br>Modern AI systems are increasingly constrained not by model capacity but by the quality and diversity of their data. Despite growing emphasis on data-centric AI, most datasets are still gathered in an open-loop manner which accumulates redundant samples without feedback from the current coverage. This results in inefficient storage, costly labeling, and limited generalization. To address this, this paper introduces \ac{FCDC}, a paradigm that formulates data collection as a closed-loop control problem. \ac{FCDC} continuously approximates the state of the collected data distribution using an online probabilistic model and adaptively regulates sample retention using based on feedback signals such as likelihood and Mahalanobis distance. Through this feedback mechanism, the system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy from accumulating over time. Besides showcasing the controllability of \ac{FCDC} on a synthetic dataset, experiments on a real data stream show that \ac{FCDC} produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data storage by $\SI{39.8}{\percent}$. These results demonstrate that data collection itself can be actively controlled, transforming collection from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupled Entropy Minimization</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03256v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03256v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03256v1">http://arxiv.org/abs/2511.03256v1</a><br>Entropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>stat.ML</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>math.ST</tag>
      
      <tag>stat.TH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03367v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03367v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03367v1">http://arxiv.org/abs/2511.03367v1</a><br>Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: <a href="https://github.com/Gahyeonkim09/AAPL">https://github.com/Gahyeonkim09/AAPL</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03328v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03328v1">http://arxiv.org/abs/2511.03328v1</a><br>A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of “reasoning MLLMs” that offer explicit control over their internal thinking processes (normally referred as the “thinking mode”) alongside the standard “non-thinking mode”. This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these “dual-state” MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active “thinking mode” capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>What&#39;s in Common? Multimodal Models Hallucinate When Reasoning Across Scenes</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03768v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03768v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03768v1">http://arxiv.org/abs/2511.03768v1</a><br>Multimodal language models possess a remarkable ability to handle an open-vocabulary’s worth of objects. Yet the best models still suffer from hallucinations when reasoning about scenes in the real world, revealing a gap between their seemingly strong performance on existing perception benchmarks that are saturating and their reasoning in the real world. To address this gap, we build a novel benchmark of in-the-wild scenes that we call Common-O. With more than 10.5k examples using exclusively new images not found in web training data to avoid contamination, Common-O goes beyond just perception, inspired by cognitive tests for humans, to probe reasoning across scenes by asking “what’s in common?”. We evaluate leading multimodal language models, including models specifically trained to perform chain-of-thought reasoning. We find that perceiving objects in single images is tractable for most models, yet reasoning across scenes is very challenging even for the best models, including reasoning models. Despite saturating many leaderboards focusing on perception, the best performing model only achieves 35% on Common-O – and on Common-O Complex, consisting of more complex scenes, the best model achieves only 1%. Curiously, we find models are more prone to hallucinate when similar objects are present in the scene, suggesting models may be relying on object co-occurrence seen during training. Among the models we evaluated, we found scale can provide modest improvements while models explicitly trained with multi-image inputs show bigger improvements, suggesting scaled multi-image training may offer promise. We make our benchmark publicly available to spur research into the challenge of hallucination when reasoning across scenes.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03876v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03876v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03876v1">http://arxiv.org/abs/2511.03876v1</a><br>Background: Non-invasive imaging-based assessment of blood flow plays a critical role in evaluating heart function and structure. Computed Tomography (CT) is a widely-used imaging modality that can robustly evaluate cardiovascular anatomy and function, but direct methods to estimate blood flow velocity from movies of contrast evolution have not been developed.   Purpose: This study evaluates the impact of CT imaging on Physics-Informed Neural Networks (PINN)-based flow estimation and proposes an improved framework, SinoFlow, which uses sinogram data directly to estimate blood flow.   Methods: We generated pulsatile flow fields in an idealized 2D vessel bifurcation using computational fluid dynamics and simulated CT scans with varying gantry rotation speeds, tube currents, and pulse mode imaging settings. We compared the performance of PINN-based flow estimation using reconstructed images (ImageFlow) to SinoFlow.   Results: SinoFlow significantly improved flow estimation performance by avoiding propagating errors introduced by filtered backprojection. SinoFlow was robust across all tested gantry rotation speeds and consistently produced lower mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow was compatible with pulsed-mode imaging and maintained higher accuracy with shorter pulse widths.   Conclusions: This study demonstrates the potential of SinoFlow for CT-based flow estimation, providing a more promising approach for non-invasive blood flow assessment. The findings aim to inform future applications of PINNs to CT images and provide a solution for image-based estimation, with reasonable acquisition parameters yielding accurate flow estimates.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
      <tag>physics.med-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03882v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03882v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03882v1">http://arxiv.org/abs/2511.03882v1</a><br>Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03890v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03890v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03890v1">http://arxiv.org/abs/2511.03890v1</a><br>Accurate geometric modeling of the aortic valve from 3D CT images is essential for biomechanical analysis and patient-specific simulations to assess valve health or make a preoperative plan. However, it remains challenging to generate aortic valve meshes with both high-quality and consistency across different patients. Traditional approaches often produce triangular meshes with irregular topologies, which can result in poorly shaped elements and inconsistent correspondence due to inter-patient anatomical variation. In this work, we address these challenges by introducing a template-fitting pipeline with deep neural networks to generate structured quad (i.e., quadrilateral) meshes from 3D CT images to represent aortic valve geometries. By remeshing aortic valves of all patients with a common quad mesh template, we ensure a uniform mesh topology with consistent node-to-node and element-to-element correspondence across patients. This consistency enables us to simplify the learning objective of the deep neural networks, by employing a loss function with only two terms (i.e., a geometry reconstruction term and a smoothness regularization term), which is sufficient to preserve mesh smoothness and element quality. Our experiments demonstrate that the proposed approach produces high-quality aortic valve surface meshes with improved smoothness and shape quality, while requiring fewer explicit regularization terms compared to the traditional methods. These results highlight that using structured quad meshes for the template and neural network training not only ensures mesh correspondence and quality but also simplifies the training process, thus enhancing the effectiveness and efficiency of aortic valve modeling.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model</title>
    <link href="/2025/11/05/cs.LG/2025-11-05-2511_03888v1/"/>
    <url>/2025/11/05/cs.LG/2025-11-05-2511_03888v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03888v1">http://arxiv.org/abs/2511.03888v1</a><br>The global waste crisis is escalating, with solid waste generation expected to increase by 70% by 2050. Traditional waste collection methods, particularly in remote or harsh environments like deserts, are labor-intensive, inefficient, and often hazardous. Recent advances in computer vision and deep learning have opened the door to automated waste detection systems, yet most research focuses on urban environments and recyclable materials, overlooking organic and hazardous waste and underexplored terrains such as deserts. In this work, we propose an enhanced real-time object detection framework based on a pruned, lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies. Using the DroneTrashNet dataset, we demonstrate significant improvements in precision, recall, and mean average precision (mAP), while achieving low latency and compact model size suitable for deployment on resource-constrained aerial drones. Benchmarking our model against state-of-the-art lightweight YOLO variants further highlights its optimal balance of accuracy and efficiency. Our results validate the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing What You Say: Expressive Image Generation from Speech</title>
    <link href="/2025/11/05/cs.MM/2025-11-05-2511_03423v1/"/>
    <url>/2025/11/05/cs.MM/2025-11-05-2511_03423v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03423v1">http://arxiv.org/abs/2511.03423v1</a><br>This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction</title>
    <link href="/2025/11/05/cs.NA/2025-11-05-2511_03093v1/"/>
    <url>/2025/11/05/cs.NA/2025-11-05-2511_03093v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03093v1">http://arxiv.org/abs/2511.03093v1</a><br>Cardiac contraction is a rapid, coordinated process that unfolds across three-dimensional tissue on millisecond timescales. Traditional optical imaging is often inadequate for capturing dynamic cellular structure in the beating heart because of a fundamental trade-off between spatial and temporal resolution. To overcome these limitations, we propose a high-performance computational imaging framework that integrates Compressive Sensing (CS) with Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The system performs compressed acquisition of fluorescence signals via random binary mask coding using a Digital Micromirror Device (DMD). We propose a Plug-and-Play (PnP) framework, solved using the alternating direction method of multipliers (ADMM), which flexibly incorporates advanced denoisers, including Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in dynamic imaging, we further introduce temporal regularization enforcing smoothness between adjacent z-slices. Experimental results on zebrafish heart imaging under high compression ratios demonstrate that the proposed method successfully reconstructs cellular structures with excellent denoising performance and image clarity, validating the effectiveness and robustness of our algorithm in real-world high-speed, low-light biological imaging scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.NA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</title>
    <link href="/2025/11/05/cs.RO/2025-11-05-2511_03571v1/"/>
    <url>/2025/11/05/cs.RO/2025-11-05-2511_03571v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03571v1">http://arxiv.org/abs/2511.03571v1</a><br>Robust 3D semantic occupancy is crucial for legged&#x2F;humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free&#x2F;occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range&#x2F;occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic occupancy; standardized within-&#x2F;cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged&#x2F;humanoid robots. Datasets and code will be publicly available at <a href="https://github.com/MasterHow/OneOcc">https://github.com/MasterHow/OneOcc</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural</title>
    <link href="/2025/11/05/cs.RO/2025-11-05-2511_03651v1/"/>
    <url>/2025/11/05/cs.RO/2025-11-05-2511_03651v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03651v1">http://arxiv.org/abs/2511.03651v1</a><br>This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world’s largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone’s propellers, which also protects the drone’s critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system’s robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.9; J.5</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</title>
    <link href="/2025/11/05/cs.RO/2025-11-05-2511_03882v1/"/>
    <url>/2025/11/05/cs.RO/2025-11-05-2511_03882v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03882v1">http://arxiv.org/abs/2511.03882v1</a><br>Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural</title>
    <link href="/2025/11/05/cs.SY/2025-11-05-2511_03651v1/"/>
    <url>/2025/11/05/cs.SY/2025-11-05-2511_03651v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03651v1">http://arxiv.org/abs/2511.03651v1</a><br>This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world’s largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone’s propellers, which also protects the drone’s critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system’s robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.</p>]]></content>
    
    
    <categories>
      
      <category>cs.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.9; J.5</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing What You Say: Expressive Image Generation from Speech</title>
    <link href="/2025/11/05/eess.AS/2025-11-05-2511_03423v1/"/>
    <url>/2025/11/05/eess.AS/2025-11-05-2511_03423v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03423v1">http://arxiv.org/abs/2511.03423v1</a><br>This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.</p>]]></content>
    
    
    <categories>
      
      <category>eess.AS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03093v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03093v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03093v1">http://arxiv.org/abs/2511.03093v1</a><br>Cardiac contraction is a rapid, coordinated process that unfolds across three-dimensional tissue on millisecond timescales. Traditional optical imaging is often inadequate for capturing dynamic cellular structure in the beating heart because of a fundamental trade-off between spatial and temporal resolution. To overcome these limitations, we propose a high-performance computational imaging framework that integrates Compressive Sensing (CS) with Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The system performs compressed acquisition of fluorescence signals via random binary mask coding using a Digital Micromirror Device (DMD). We propose a Plug-and-Play (PnP) framework, solved using the alternating direction method of multipliers (ADMM), which flexibly incorporates advanced denoisers, including Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in dynamic imaging, we further introduce temporal regularization enforcing smoothness between adjacent z-slices. Experimental results on zebrafish heart imaging under high compression ratios demonstrate that the proposed method successfully reconstructs cellular structures with excellent denoising performance and image clarity, validating the effectiveness and robustness of our algorithm in real-world high-speed, low-light biological imaging scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03098v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03098v1">http://arxiv.org/abs/2511.03098v1</a><br>The Intermeshed Steel Connection (ISC) system, when paired with robotic manipulators, can accelerate steel-frame assembly and improve worker safety by eliminating manual assembly. Dependable perception is one of the initial stages for ISC-aware robots. However, this is hampered by the absence of a dedicated image corpus, as collecting photographs on active construction sites is logistically difficult and raises safety and privacy concerns. In response, we introduce ISC-Perception, the first hybrid dataset expressly designed for ISC component detection. It blends procedurally rendered CAD images, game-engine photorealistic scenes, and a limited, curated set of real photographs, enabling fully automatic labelling of the synthetic portion. We explicitly account for all human effort to produce the dataset, including simulation engine and scene setup, asset preparation, post-processing scripts and quality checks; our total human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for manual labelling at 60,s per image (-81.7%). A manual pilot on a representative image with five instances of ISC members took 60,s (maximum 80,s), anchoring the manual baseline. Detectors trained on ISC-Perception achieved a mean Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we report <a href="mailto:&#109;&#x41;&#80;&#x40;&#48;&#x2e;&#x35;&#x30;">mAP@0.50</a>&#x2F;mAP@[0.50:0.95] of 0.943&#x2F;0.823. By bridging the data gap for construction-robotics perception, ISC-Perception facilitates rapid development of custom object detectors and is freely available for research and industrial use upon request.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03099v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03099v1">http://arxiv.org/abs/2511.03099v1</a><br>In orthodontic treatment, particularly within telemedicine contexts, observing patients’ dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03120v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03120v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03120v1">http://arxiv.org/abs/2511.03120v1</a><br>Integrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03132v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03132v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03132v1">http://arxiv.org/abs/2511.03132v1</a><br>This paper presents the first AI&#x2F;ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI&#x2F;ML for damage assessment during a disaster and lessons learned to the benefit of the AI&#x2F;ML research and user communities.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03147v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03147v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03147v1">http://arxiv.org/abs/2511.03147v1</a><br>Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test Time Adaptation Using Adaptive Quantile Recalibration</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03148v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03148v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03148v1">http://arxiv.org/abs/2511.03148v1</a><br>Domain adaptation is a key strategy for enhancing the generalizability of deep learning models in real-world scenarios, where test distributions often diverge significantly from the training domain. However, conventional approaches typically rely on prior knowledge of the target domain or require model retraining, limiting their practicality in dynamic or resource-constrained environments. Recent test-time adaptation methods based on batch normalization statistic updates allow for unsupervised adaptation, but they often fail to capture complex activation distributions and are constrained to specific normalization layers. We propose Adaptive Quantile Recalibration (AQR), a test-time adaptation technique that modifies pre-activation distributions by aligning quantiles on a channel-wise basis. AQR captures the full shape of activation distributions and generalizes across architectures employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of estimating distribution tails under varying batch sizes, AQR incorporates a robust tail calibration strategy that improves stability and precision. Our method leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining models. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR achieves robust adaptation across diverse settings, outperforming existing test-time adaptation baselines. These results highlight AQR’s potential for deployment in real-world scenarios with dynamic and unpredictable data distributions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Finetuning-Free Personalization of Text to Image Generation via Hypernetworks</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03156v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03156v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03156v1">http://arxiv.org/abs/2511.03156v1</a><br>Personalizing text-to-image diffusion models has traditionally relied on subject-specific fine-tuning approaches such as DreamBooth~\cite{ruiz2023dreambooth}, which are computationally expensive and slow at inference. Recent adapter- and encoder-based methods attempt to reduce this overhead but still depend on additional fine-tuning or large backbone models for satisfactory results. In this work, we revisit an orthogonal direction: fine-tuning-free personalization via Hypernetworks that predict LoRA-adapted weights directly from subject images. Prior hypernetwork-based approaches, however, suffer from costly data generation or unstable attempts to mimic base model optimization trajectories. We address these limitations with an end-to-end training objective, stabilized by a simple output regularization, yielding reliable and effective hypernetworks. Our method removes the need for per-subject optimization at test time while preserving both subject fidelity and prompt alignment. To further enhance compositional generalization at inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG), which combines the compositional strengths of the base diffusion model with the subject fidelity of personalized models during sampling. Extensive experiments on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves strong personalization performance and highlights the promise of hypernetworks as a scalable and effective direction for open-category personalization.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03163v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03163v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03163v1">http://arxiv.org/abs/2511.03163v1</a><br>Accurate detection and delineation of anatomical structures in medical imaging are critical for computer-assisted interventions, particularly in laparoscopic liver surgery where 2D video streams limit depth perception and complicate landmark localization. While recent works have leveraged monocular depth cues for enhanced landmark detection, challenges remain in fusing RGB and depth features and in efficiently adapting large-scale vision models to surgical domains. We propose a depth-guided liver landmark segmentation framework integrating semantic and geometric cues via vision foundation encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB features and Depth Anything V2 (DA2) encoder to extract depth-aware features. To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient projection method that replaces the computationally expensive SVD with a Subsampled Randomized Fourier Transform (SRFT). This enables efficient fine-tuning of high-dimensional attention layers without sacrificing representational power. A cross-attention fusion module further integrates RGB and depth cues. To assess cross-dataset generalization, we also construct a new Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark. On the public L3D dataset, our method achieves a 4.85% improvement in Dice Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface Distance compared to the D2GPLand. To further assess generalization capability, we evaluate our model on LLSD dataset. Our model maintains competitive performance and significantly outperforms SAM-based baselines, demonstrating strong cross-dataset robustness and adaptability to unseen surgical environments. These results demonstrate that our SRFT-GaLore-enhanced dual-encoder framework enables scalable and precise segmentation under real-time, depth-constrained surgical settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Accelerating Physical Property Reasoning for Augmented Visual Cognition</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03126v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03126v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03126v1">http://arxiv.org/abs/2511.03126v1</a><br>This paper introduces \sysname, a system that accelerates vision-guided physical property reasoning to enable augmented visual cognition. \sysname minimizes the run-time latency of this reasoning pipeline through a combination of both algorithmic and systematic optimizations, including rapid geometric 3D reconstruction, efficient semantic feature fusion, and parallel view encoding. Through these simple yet effective optimizations, \sysname reduces the end-to-end latency of this reasoning pipeline from 10–20 minutes to less than 6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname achieves this 62.9$\times$–287.2$\times$ speedup while not only reaching on-par (and sometimes slightly better) object-level physical property estimation accuracy(e.g. mass), but also demonstrating superior performance in material segmentation and voxel-level inference than two SOTA baselines. We further combine gaze-tracking with \sysname to localize the object of interest in cluttered, real-world environments, streamlining the physical property reasoning on smart glasses. The case study with Meta Aria Glasses conducted at an IKEA furniture store demonstrates that \sysname achives consistently high performance compared to controlled captures, providing robust property estimations even with fewer views in real-world scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03178v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03178v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03178v1">http://arxiv.org/abs/2511.03178v1</a><br>Anticipating forthcoming surgical events is vital for real-time assistance in endonasal transsphenoidal pituitary surgery, where visibility is limited and workflow changes rapidly. Most visual question answering (VQA) systems reason on isolated frames with static vision language alignment, providing little support for forecasting next steps or instrument needs. Existing surgical VQA datasets likewise center on the current scene rather than the near future. We introduce PitVQA-Anticipation, the first VQA dataset designed for forward looking surgical reasoning. It comprises 33.5 hours of operative video and 734,769 question answer pairs built from temporally grouped clips and expert annotations across four tasks: predicting the future phase, next step, upcoming instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video language model that adapts a large language model using a GRU Gated Temporal Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics, while an adaptive gate injects visual context into the language stream at the token level. Parameter efficient fine tuning customizes the language backbone to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and EndoVis datasets, surpassing strong image and video based baselines. Ablations show that temporal recurrence and gated fusion drive most of the gains. A frame budget study indicates a trade-off: 8 frames maximize fluency, whereas 32 frames slightly reduce BLEU but improve numeric time estimation. By pairing a temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA advances surgical VQA from retrospective description to proactive anticipation. PitVQA-Anticipation offers a comprehensive benchmark for this setting and highlights the importance of targeted temporal modeling for reliable, future aware surgical assistance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03194v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03194v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03194v1">http://arxiv.org/abs/2511.03194v1</a><br>Publicly available, large-scale medical imaging datasets are crucial for developing and validating artificial intelligence models and conducting retrospective clinical research. However, datasets that combine functional and anatomical imaging with detailed clinical reports across multiple cancer types remain scarce. Here, we present PETWB-REP, a curated dataset comprising whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography&#x2F;Computed Tomography (PET&#x2F;CT) scans and corresponding radiology reports from 490 patients diagnosed with various malignancies. The dataset primarily includes common cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and ovarian cancer. This dataset includes paired PET and CT images, de-identified textual reports, and structured clinical metadata. It is designed to support research in medical imaging, radiomics, artificial intelligence, and multi-modal learning.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Probabilistic U-Net Approach to Downscaling Climate Simulations</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03197v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03197v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03197v1">http://arxiv.org/abs/2511.03197v1</a><br>Climate models are limited by heavy computational costs, often producing outputs at coarse spatial resolutions, while many climate change impact studies require finer scales. Statistical downscaling bridges this gap, and we adapt the probabilistic U-Net for this task, combining a deterministic U-Net backbone with a variational latent space to capture aleatoric uncertainty. We evaluate four training objectives, afCRPS and WMSE-MS-SSIM with three settings for downscaling precipitation and temperature from $16\times$ coarser resolution. Our main finding is that WMSE-MS-SSIM performs well for extremes under certain settings, whereas afCRPS better captures spatial variability across scales.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>physics.ao-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03206v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03206v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03206v1">http://arxiv.org/abs/2511.03206v1</a><br>Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03212v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03212v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03212v1">http://arxiv.org/abs/2511.03212v1</a><br>Accurately assessing the risk of cesarean section (CS) delivery is critical, especially in settings with limited medical resources, where access to healthcare is often restricted. Early and reliable risk prediction allows better-informed prenatal care decisions and can improve maternal and neonatal outcomes. However, most existing predictive models are tailored for in-hospital use during labor and rely on parameters that are often unavailable in resource-limited or home-based settings. In this study, we conduct a pilot investigation to examine the feasibility of using 3D body shape for CS risk assessment for future applications with more affordable general devices. We propose a novel multi-view-based Transformer network, MvBody, which predicts CS risk using only self-reported medical data and 3D optical body scans obtained between the 31st and 38th weeks of gestation. To enhance training efficiency and model generalizability in data-scarce environments, we incorporate a metric learning loss into the network. Compared to widely used machine learning models and the latest advanced 3D analysis methods, our method demonstrates superior performance, achieving an accuracy of 84.62% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set. To improve transparency and trust in the model’s predictions, we apply the Integrated Gradients algorithm to provide theoretically grounded explanations of the model’s decision-making process. Our results indicate that pre-pregnancy weight, maternal age, obstetric history, previous CS history, and body shape, particularly around the head and shoulders, are key contributors to CS risk prediction.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T10, 68T45</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03219v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03219v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03219v1">http://arxiv.org/abs/2511.03219v1</a><br>Augmentation for dense prediction typically relies on either sample mixing or generative synthesis. Mixing improves robustness but misaligned masks yield soft label ambiguity. Diffusion synthesis increases apparent diversity but, when trained as common samples, overlooks the structural benefit of mask conditioning and introduces synthetic-real domain shift. We propose a paired, diffusion-guided paradigm that fuses the strengths of both. For each real image, a synthetic counterpart is generated under the same mask and the pair is used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which mixes only image appearance while supervision always uses the original hard mask. This produces a continuous family of intermediate samples that smoothly bridges synthetic and real appearances under shared geometry, enlarging diversity without compromising pixel-level semantics. To keep learning aligned with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the mixing strength and the loss weight of mixed samples over training, gradually re-anchoring optimization to real data and mitigating distributional bias. Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC 2017, the approach achieves state-of-the-art segmentation performance and consistent gains over baselines. The results show that combining label-preserving mixing with diffusion-driven diversity, together with adaptive re-anchoring, yields robust and generalizable endoscopic segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03232v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03232v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03232v1">http://arxiv.org/abs/2511.03232v1</a><br>Recently, Mamba-based super-resolution (SR) methods have demonstrated the ability to capture global receptive fields with linear complexity, addressing the quadratic computational cost of Transformer-based SR approaches. However, existing Mamba-based methods lack fine-grained transitions across different modeling scales, which limits the efficiency of feature representation. In this paper, we propose T-PMambaSR, a lightweight SR framework that integrates window-based self-attention with Progressive Mamba. By enabling interactions among receptive fields of different scales, our method establishes a fine-grained modeling paradigm that progressively enhances feature representation with linear complexity. Furthermore, we introduce an Adaptive High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost during Transformer and Mamba processing. Extensive experiments demonstrate that T-PMambaSR progressively enhances the model’s receptive field and expressiveness, yielding better performance than recent Transformer- or Mamba-based methods while incurring lower computational cost. Our codes will be released after acceptance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03239v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03239v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03239v1">http://arxiv.org/abs/2511.03239v1</a><br>Modern AI systems are increasingly constrained not by model capacity but by the quality and diversity of their data. Despite growing emphasis on data-centric AI, most datasets are still gathered in an open-loop manner which accumulates redundant samples without feedback from the current coverage. This results in inefficient storage, costly labeling, and limited generalization. To address this, this paper introduces \ac{FCDC}, a paradigm that formulates data collection as a closed-loop control problem. \ac{FCDC} continuously approximates the state of the collected data distribution using an online probabilistic model and adaptively regulates sample retention using based on feedback signals such as likelihood and Mahalanobis distance. Through this feedback mechanism, the system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy from accumulating over time. Besides showcasing the controllability of \ac{FCDC} on a synthetic dataset, experiments on a real data stream show that \ac{FCDC} produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data storage by $\SI{39.8}{\percent}$. These results demonstrate that data collection itself can be actively controlled, transforming collection from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03245v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03245v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03245v1">http://arxiv.org/abs/2511.03245v1</a><br>Recently, remarkable progress has been made in large-scale pre-trained model tuning, and inference efficiency is becoming more crucial for practical deployment. Early exiting in conjunction with multi-stage predictors, when cooperated with a parameter-efficient fine-tuning strategy, offers a straightforward way to achieve an inference-efficient model. However, a key challenge remains unresolved: How can early stages provide low-level fundamental features to deep stages while simultaneously supplying high-level discriminative features to early-stage predictors? To address this problem, we propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively decouple the low-level representative ability and high-level discriminative ability in early stages. First, in terms of architecture, we introduce a lightweight bypass module into multi-stage predictors for functional decomposition of shallow features from early stages, while a high-order statistics-based predictor is developed for early stages to effectively enhance their discriminative ability. To reasonably train our multi-predictor architecture, a decoupled optimization is proposed to allocate two-phase loss weights for multi-stage predictors during model tuning, where the initial training phase enables the model to prioritize the acquisition of discriminative ability of deep stages via emphasizing representative ability of early stages, and the latter training phase drives discriminative ability towards earlier stages as much as possible. As such, our DMPO can effectively decouple representative and discriminative abilities in early stages in terms of architecture design and model optimization. Experiments across various datasets and pre-trained backbones demonstrate that DMPO clearly outperforms its counterparts when reducing computational cost.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative deep learning for foundational video translation in ultrasound</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03255v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03255v1">http://arxiv.org/abs/2511.03255v1</a><br>Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+&#x2F;-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+&#x2F;-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+&#x2F;-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupled Entropy Minimization</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03256v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03256v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03256v1">http://arxiv.org/abs/2511.03256v1</a><br>Entropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>stat.ML</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>math.ST</tag>
      
      <tag>stat.TH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03267v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03267v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03267v1">http://arxiv.org/abs/2511.03267v1</a><br>3D anomaly detection (3D-AD) plays a critical role in industrial manufacturing, particularly in ensuring the reliability and safety of core equipment components. Although existing 3D datasets like Real3D-AD and MVTec 3D-AD offer broad application support, they fall short in capturing the complexities and subtle defects found in real industrial environments. This limitation hampers precise anomaly detection research, especially for industrial equipment components (IEC) such as bearings, rings, and bolts. To address this challenge, we have developed a point cloud anomaly detection dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is directly collected from actual production lines, ensuring high fidelity and relevance. Compared to existing datasets, IEC3D-AD features significantly improved point cloud resolution and defect annotation granularity, facilitating more demanding anomaly detection tasks. Furthermore, inspired by generative 2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This paradigm generates synthetic point cloud samples based on geometric morphological analysis, then reduces the margin and increases the overlap between normal and abnormal point-level features through spatial discrepancy optimization. Extensive experiments demonstrate the effectiveness of our method on both IEC3D-AD and other datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Medical Image Segmentation via Heat Conduction Equation</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03260v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03260v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03260v1">http://arxiv.org/abs/2511.03260v1</a><br>Medical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03272v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03272v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03272v1">http://arxiv.org/abs/2511.03272v1</a><br>Generating long videos remains a fundamental challenge, and achieving high controllability in video inpainting and outpainting is particularly demanding. To address both of these challenges simultaneously and achieve controllable video inpainting and outpainting for long video clips, we introduce a novel and unified approach for long video inpainting and outpainting that extends text-to-video diffusion models to generate arbitrarily long, spatially edited videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a large pre-trained video diffusion model like Alibaba’s Wan 2.1 for masked region video synthesis, and employs an overlap-and-blend temporal co-denoising strategy with high-order solvers to maintain consistency across long sequences. In contrast to prior work that struggles with fixed-length clips or exhibits stitching artifacts, our system enables arbitrarily long video generation and editing without noticeable seams or drift. We validate our approach on challenging inpainting&#x2F;outpainting tasks including editing or adding objects over hundreds of frames and demonstrate superior performance to baseline methods like Wan 2.1 model and VACE in terms of quality (PSNR&#x2F;SSIM), and perceptual realism (LPIPS). Our method enables practical long-range video editing with minimal overhead, achieved a balance between parameter efficient and superior performance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03317v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03317v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03317v1">http://arxiv.org/abs/2511.03317v1</a><br>Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at <a href="https://github.com/AIDC-AI/Diffusion-SDPO">https://github.com/AIDC-AI/Diffusion-SDPO</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03328v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03328v1">http://arxiv.org/abs/2511.03328v1</a><br>A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of “reasoning MLLMs” that offer explicit control over their internal thinking processes (normally referred as the “thinking mode”) alongside the standard “non-thinking mode”. This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these “dual-state” MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active “thinking mode” capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03332v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03332v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03332v1">http://arxiv.org/abs/2511.03332v1</a><br>In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03334v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03334v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03334v1">http://arxiv.org/abs/2511.03334v1</a><br>Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen’s robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03325v2/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03325v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03325v2">http://arxiv.org/abs/2511.03325v2</a><br>Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video–Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool–tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11% on REAL-Colon-VQA and +9% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at <a href="https://github.com/madratak/SurgViVQA">https://github.com/madratak/SurgViVQA</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene Mutation Prediction from Histopathology</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03365v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03365v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03365v1">http://arxiv.org/abs/2511.03365v1</a><br>Ovarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&amp;E) histopathological images. Using $\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2%$ (Macro AUC of $0.87 \pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} &#x3D; 0.82 \pm 0.02$, $AUC_{BRCA1} &#x3D; 0.76 \pm 0.04$, and $AUC_{ARID1A} &#x3D; 0.73 \pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03367v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03367v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03367v1">http://arxiv.org/abs/2511.03367v1</a><br>Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: <a href="https://github.com/Gahyeonkim09/AAPL">https://github.com/Gahyeonkim09/AAPL</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03416v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03416v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03416v1">http://arxiv.org/abs/2511.03416v1</a><br>Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans. In this work, we propose an automated method for standardizing this alignment. Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo’s principal axes, from which four candidate orientations are derived. The candidate in standard orientation is selected using one of three strategies: a heuristic based on Pearson’s correlation assessing shape, image matching to an atlas through normalized cross-correlation, and a Random Forest classifier. We tested our method on 2166 images longitudinally acquired 3D ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images, PCA correctly extracted the principal axes of the embryo. The correct candidate was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%, 95.8%, and 98.4% of images, respectively. A Majority Vote of these selection methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline enables consistent embryonic alignment in the first trimester, enabling scalable analysis in both clinical and research settings. The code is publicly available at: <a href="https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment">https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing What You Say: Expressive Image Generation from Speech</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03423v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03423v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03423v1">http://arxiv.org/abs/2511.03423v1</a><br>This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generalizing Shape-from-Template to Topological Changes</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03459v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03459v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03459v1">http://arxiv.org/abs/2511.03459v1</a><br>Reconstructing the surfaces of deformable objects from correspondences between a 3D template and a 2D image is well studied under Shape-from-Template (SfT) methods; however, existing approaches break down when topological changes accompany the deformation. We propose a principled extension of SfT that enables reconstruction in the presence of such changes. Our approach is initialized with a classical SfT solution and iteratively adapts the template by partitioning its spatial domain so as to minimize an energy functional that jointly encodes physical plausibility and reprojection consistency. We demonstrate that the method robustly captures a wide range of practically relevant topological events including tears and cuts on bounded 2D surfaces, thereby establishing the first general framework for topological-change-aware SfT. Experiments on both synthetic and real data confirm that our approach consistently outperforms baseline methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03571v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03571v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03571v1">http://arxiv.org/abs/2511.03571v1</a><br>Robust 3D semantic occupancy is crucial for legged&#x2F;humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free&#x2F;occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range&#x2F;occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic occupancy; standardized within-&#x2F;cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged&#x2F;humanoid robots. Datasets and code will be publicly available at <a href="https://github.com/MasterHow/OneOcc">https://github.com/MasterHow/OneOcc</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Human Mesh Modeling for Anny Body</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03589v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03589v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03589v1">http://arxiv.org/abs/2511.03589v1</a><br>Parametric body models are central to many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms – across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling – supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic humans generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models, while remaining interpretable and broadly representative. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Signal Intensity-weighted coordinate channels improve learning stability and generalisation in 1D and 2D CNNs in localisation tasks on biomedical signals</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03645v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03645v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03645v1">http://arxiv.org/abs/2511.03645v1</a><br>Localisation tasks in biomedical data often require models to learn meaningful spatial or temporal relationships from signals with complex intensity distributions. A common strategy, exemplified by CoordConv layers, is to append coordinate channels to convolutional inputs, enabling networks to learn absolute positions. In this work, we propose a signal intensity-weighted coordinate representation that replaces the pure coordinate channels with channels scaled by local signal intensity. This modification embeds an intensity-position coupling directly in the input representation, introducing a simple and modality-agnostic inductive bias. We evaluate the approach on two distinct localisation problems: (i) predicting the time of morphological transition in 20-second, two-lead ECG signals, and (ii) regressing the coordinates of nuclear centres in cytological images from the SiPaKMeD dataset. In both cases, the proposed representation yields faster convergence and higher generalisation performance relative to conventional coordinate-channel approaches, demonstrating its effectiveness across both one-dimensional and two-dimensional biomedical signals.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03651v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03651v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03651v1">http://arxiv.org/abs/2511.03651v1</a><br>This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world’s largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone’s propellers, which also protects the drone’s critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system’s robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.9; J.5</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03665v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03665v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03665v1">http://arxiv.org/abs/2511.03665v1</a><br>This paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03666v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03666v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03666v1">http://arxiv.org/abs/2511.03666v1</a><br>Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03765v2/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03765v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03765v2">http://arxiv.org/abs/2511.03765v2</a><br>On-device fine-tuning of CNNs is essential to withstand domain shift in edge applications such as Human Activity Recognition (HAR), yet full fine-tuning is infeasible under strict memory, compute, and energy budgets. We present LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional layers, (ii) selectively updates only the output-side core with zero-initialization to keep the auxiliary path inactive at the start, and (iii) fuses the update back into dense kernels, leaving inference cost unchanged. This design preserves convolutional structure and reduces the number of trainable parameters by up to two orders of magnitude compared to full fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, consistently outperforming prior parameter-efficient baselines under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and selective-core training yield 1.4-3.8x faster convergence to target F1. LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN adaptation practical for edge platforms.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03725v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03725v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03725v1">http://arxiv.org/abs/2511.03725v1</a><br>Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature – intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets – KTH, Penn Action, HAA500, and UCF-101 – demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SILVI: Simple Interface for Labeling Video Interactions</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03819v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03819v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03819v1">http://arxiv.org/abs/2511.03819v1</a><br>Computer vision methods are increasingly used for the automated analysis of large volumes of video data collected through camera traps, drones, or direct observations of animals in the wild. While recent advances have focused primarily on detecting individual actions, much less work has addressed the detection and annotation of interactions – a crucial aspect for understanding social and individualized animal behavior. Existing open-source annotation tools support either behavioral labeling without localization of individuals, or localization without the capacity to capture interactions. To bridge this gap, we present SILVI, an open-source labeling software that integrates both functionalities. SILVI enables researchers to annotate behaviors and interactions directly within video data, generating structured outputs suitable for training and validating computer vision models. By linking behavioral ecology with computer vision, SILVI facilitates the development of automated approaches for fine-grained behavioral analyses. Although developed primarily in the context of animal behavior, SILVI could be useful more broadly to annotate human interactions in other videos that require extracting dynamic scene graphs. The software, along with documentation and download instructions, is available at: <a href="https://gitlab.gwdg.de/kanbertay/interaction-labelling-app">https://gitlab.gwdg.de/kanbertay/interaction-labelling-app</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03855v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03855v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03855v1">http://arxiv.org/abs/2511.03855v1</a><br>Deep learned (DL) models for image recognition have been shown to fail to generalize to data from different devices, populations, etc. COVID-19 detection from Chest X-rays (CXRs), in particular, has been shown to fail to generalize to out-of-distribution (OOD) data from new clinical sources not covered in the training set. This occurs because models learn to exploit shortcuts - source-specific artifacts that do not translate to new distributions - rather than reasonable biomarkers to maximize performance on in-distribution (ID) data. Rendering the models more robust to distribution shifts, our study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during training. Our empirical results demonstrate that this technique can significantly reduce the performance gap between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results averaged over ten random seeds across key metrics such as AUC, F1, accuracy, recall and specificity. Our source code is publicly available at <a href="https://github.com/Duongmai127/Noisy-ood">https://github.com/Duongmai127/Noisy-ood</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03876v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03876v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03876v1">http://arxiv.org/abs/2511.03876v1</a><br>Background: Non-invasive imaging-based assessment of blood flow plays a critical role in evaluating heart function and structure. Computed Tomography (CT) is a widely-used imaging modality that can robustly evaluate cardiovascular anatomy and function, but direct methods to estimate blood flow velocity from movies of contrast evolution have not been developed.   Purpose: This study evaluates the impact of CT imaging on Physics-Informed Neural Networks (PINN)-based flow estimation and proposes an improved framework, SinoFlow, which uses sinogram data directly to estimate blood flow.   Methods: We generated pulsatile flow fields in an idealized 2D vessel bifurcation using computational fluid dynamics and simulated CT scans with varying gantry rotation speeds, tube currents, and pulse mode imaging settings. We compared the performance of PINN-based flow estimation using reconstructed images (ImageFlow) to SinoFlow.   Results: SinoFlow significantly improved flow estimation performance by avoiding propagating errors introduced by filtered backprojection. SinoFlow was robust across all tested gantry rotation speeds and consistently produced lower mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow was compatible with pulsed-mode imaging and maintained higher accuracy with shorter pulse widths.   Conclusions: This study demonstrates the potential of SinoFlow for CT-based flow estimation, providing a more promising approach for non-invasive blood flow assessment. The findings aim to inform future applications of PINNs to CT images and provide a solution for image-based estimation, with reasonable acquisition parameters yielding accurate flow estimates.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
      <tag>physics.med-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03882v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03882v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03882v1">http://arxiv.org/abs/2511.03882v1</a><br>Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>What&#39;s in Common? Multimodal Models Hallucinate When Reasoning Across Scenes</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03768v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03768v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03768v1">http://arxiv.org/abs/2511.03768v1</a><br>Multimodal language models possess a remarkable ability to handle an open-vocabulary’s worth of objects. Yet the best models still suffer from hallucinations when reasoning about scenes in the real world, revealing a gap between their seemingly strong performance on existing perception benchmarks that are saturating and their reasoning in the real world. To address this gap, we build a novel benchmark of in-the-wild scenes that we call Common-O. With more than 10.5k examples using exclusively new images not found in web training data to avoid contamination, Common-O goes beyond just perception, inspired by cognitive tests for humans, to probe reasoning across scenes by asking “what’s in common?”. We evaluate leading multimodal language models, including models specifically trained to perform chain-of-thought reasoning. We find that perceiving objects in single images is tractable for most models, yet reasoning across scenes is very challenging even for the best models, including reasoning models. Despite saturating many leaderboards focusing on perception, the best performing model only achieves 35% on Common-O – and on Common-O Complex, consisting of more complex scenes, the best model achieves only 1%. Curiously, we find models are more prone to hallucinate when similar objects are present in the scene, suggesting models may be relying on object co-occurrence seen during training. Among the models we evaluated, we found scale can provide modest improvements while models explicitly trained with multi-image inputs show bigger improvements, suggesting scaled multi-image training may offer promise. We make our benchmark publicly available to spur research into the challenge of hallucination when reasoning across scenes.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03888v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03888v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03888v1">http://arxiv.org/abs/2511.03888v1</a><br>The global waste crisis is escalating, with solid waste generation expected to increase by 70% by 2050. Traditional waste collection methods, particularly in remote or harsh environments like deserts, are labor-intensive, inefficient, and often hazardous. Recent advances in computer vision and deep learning have opened the door to automated waste detection systems, yet most research focuses on urban environments and recyclable materials, overlooking organic and hazardous waste and underexplored terrains such as deserts. In this work, we propose an enhanced real-time object detection framework based on a pruned, lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies. Using the DroneTrashNet dataset, we demonstrate significant improvements in precision, recall, and mean average precision (mAP), while achieving low latency and compact model size suitable for deployment on resource-constrained aerial drones. Benchmarking our model against state-of-the-art lightweight YOLO variants further highlights its optimal balance of accuracy and efficiency. Our results validate the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03890v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03890v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03890v1">http://arxiv.org/abs/2511.03890v1</a><br>Accurate geometric modeling of the aortic valve from 3D CT images is essential for biomechanical analysis and patient-specific simulations to assess valve health or make a preoperative plan. However, it remains challenging to generate aortic valve meshes with both high-quality and consistency across different patients. Traditional approaches often produce triangular meshes with irregular topologies, which can result in poorly shaped elements and inconsistent correspondence due to inter-patient anatomical variation. In this work, we address these challenges by introducing a template-fitting pipeline with deep neural networks to generate structured quad (i.e., quadrilateral) meshes from 3D CT images to represent aortic valve geometries. By remeshing aortic valves of all patients with a common quad mesh template, we ensure a uniform mesh topology with consistent node-to-node and element-to-element correspondence across patients. This consistency enables us to simplify the learning objective of the deep neural networks, by employing a loss function with only two terms (i.e., a geometry reconstruction term and a smoothness regularization term), which is sufficient to preserve mesh smoothness and element quality. Our experiments demonstrate that the proposed approach produces high-quality aortic valve surface meshes with improved smoothness and shape quality, while requiring fewer explicit regularization terms compared to the traditional methods. These results highlight that using structured quad meshes for the template and neural network training not only ensures mesh correspondence and quality but also simplifies the training process, thus enhancing the effectiveness and efficiency of aortic valve modeling.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03891v2/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03891v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03891v2">http://arxiv.org/abs/2511.03891v2</a><br>Small, imbalanced datasets and poor input image quality can lead to high false predictions rates with deep learning models. This paper introduces Class-Based Image Composition, an approach that allows us to reformulate training inputs through a fusion of multiple images of the same class into combined visual composites, named Composite Input Images (CoImg). That enhances the intra-class variance and improves the valuable information density per training sample and increases the ability of the model to distinguish between subtle disease patterns. Our method was evaluated on the Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et al., 2024), which contains 2,064 high-resolution optical coherence tomography (OCT) scans of the human retina, representing seven distinct diseases with a significant class imbalance. We constructed a perfectly class-balanced version of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout composite image. To assess the effectiveness of this new representation, we conducted a comparative analysis between the original dataset and its variant using a VGG16 model. A fair comparison was ensured by utilizing the identical model architecture and hyperparameters for all experiments. The proposed approach markedly improved diagnostic results.The enhanced Dataset achieved near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared to a baseline model trained on raw dataset. The false prediction rate was also significantly lower, this demonstrates that the method can producehigh-quality predictions even for weak datasets affected by class imbalance or small sample size.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.DB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>I Detect What I Don&#39;t Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging</title>
    <link href="/2025/11/05/cs.CV/2025-11-05-2511_03912v1/"/>
    <url>/2025/11/05/cs.CV/2025-11-05-2511_03912v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03912v1">http://arxiv.org/abs/2511.03912v1</a><br>Unknown anomaly detection in medical imaging remains a fundamental challenge due to the scarcity of labeled anomalies and the high cost of expert supervision. We introduce an unsupervised, oracle-free framework that incrementally expands a trusted set of normal samples without any anomaly labels. Starting from a small, verified seed of normal images, our method alternates between lightweight adapter updates and uncertainty-gated sample admission. A frozen pretrained vision backbone is augmented with tiny convolutional adapters, ensuring rapid domain adaptation with negligible computational overhead. Extracted embeddings are stored in a compact coreset enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during incremental expansion is enforced by dual probabilistic gates, a sample is admitted into the normal memory only if its distance to the existing coreset lies within a calibrated z-score threshold, and its SWAG-based epistemic uncertainty remains below a seed-calibrated bound. This mechanism prevents drift and false inclusions without relying on generative reconstruction or replay buffers. Empirically, our system steadily refines the notion of normality as unlabeled data arrive, producing substantial gains over baselines. On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5, ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These results highlight the effectiveness and efficiency of the proposed framework for real-world, label-scarce medical imaging applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly</title>
    <link href="/2025/11/05/eess.IV/2025-11-05-2511_03098v1/"/>
    <url>/2025/11/05/eess.IV/2025-11-05-2511_03098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03098v1">http://arxiv.org/abs/2511.03098v1</a><br>The Intermeshed Steel Connection (ISC) system, when paired with robotic manipulators, can accelerate steel-frame assembly and improve worker safety by eliminating manual assembly. Dependable perception is one of the initial stages for ISC-aware robots. However, this is hampered by the absence of a dedicated image corpus, as collecting photographs on active construction sites is logistically difficult and raises safety and privacy concerns. In response, we introduce ISC-Perception, the first hybrid dataset expressly designed for ISC component detection. It blends procedurally rendered CAD images, game-engine photorealistic scenes, and a limited, curated set of real photographs, enabling fully automatic labelling of the synthetic portion. We explicitly account for all human effort to produce the dataset, including simulation engine and scene setup, asset preparation, post-processing scripts and quality checks; our total human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for manual labelling at 60,s per image (-81.7%). A manual pilot on a representative image with five instances of ISC members took 60,s (maximum 80,s), anchoring the manual baseline. Detectors trained on ISC-Perception achieved a mean Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we report <a href="mailto:&#x6d;&#65;&#x50;&#x40;&#48;&#46;&#53;&#48;">mAP@0.50</a>&#x2F;mAP@[0.50:0.95] of 0.943&#x2F;0.823. By bridging the data gap for construction-robotics perception, ISC-Perception facilitates rapid development of custom object detectors and is freely available for research and industrial use upon request.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene Mutation Prediction from Histopathology</title>
    <link href="/2025/11/05/eess.IV/2025-11-05-2511_03365v1/"/>
    <url>/2025/11/05/eess.IV/2025-11-05-2511_03365v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03365v1">http://arxiv.org/abs/2511.03365v1</a><br>Ovarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&amp;E) histopathological images. Using $\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2%$ (Macro AUC of $0.87 \pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} &#x3D; 0.82 \pm 0.02$, $AUC_{BRCA1} &#x3D; 0.76 \pm 0.04$, and $AUC_{ARID1A} &#x3D; 0.73 \pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</title>
    <link href="/2025/11/05/eess.IV/2025-11-05-2511_03571v1/"/>
    <url>/2025/11/05/eess.IV/2025-11-05-2511_03571v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03571v1">http://arxiv.org/abs/2511.03571v1</a><br>Robust 3D semantic occupancy is crucial for legged&#x2F;humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free&#x2F;occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range&#x2F;occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic occupancy; standardized within-&#x2F;cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged&#x2F;humanoid robots. Datasets and code will be publicly available at <a href="https://github.com/MasterHow/OneOcc">https://github.com/MasterHow/OneOcc</a>.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images</title>
    <link href="/2025/11/05/eess.IV/2025-11-05-2511_03890v1/"/>
    <url>/2025/11/05/eess.IV/2025-11-05-2511_03890v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03890v1">http://arxiv.org/abs/2511.03890v1</a><br>Accurate geometric modeling of the aortic valve from 3D CT images is essential for biomechanical analysis and patient-specific simulations to assess valve health or make a preoperative plan. However, it remains challenging to generate aortic valve meshes with both high-quality and consistency across different patients. Traditional approaches often produce triangular meshes with irregular topologies, which can result in poorly shaped elements and inconsistent correspondence due to inter-patient anatomical variation. In this work, we address these challenges by introducing a template-fitting pipeline with deep neural networks to generate structured quad (i.e., quadrilateral) meshes from 3D CT images to represent aortic valve geometries. By remeshing aortic valves of all patients with a common quad mesh template, we ensure a uniform mesh topology with consistent node-to-node and element-to-element correspondence across patients. This consistency enables us to simplify the learning objective of the deep neural networks, by employing a loss function with only two terms (i.e., a geometry reconstruction term and a smoothness regularization term), which is sufficient to preserve mesh smoothness and element quality. Our experiments demonstrate that the proposed approach produces high-quality aortic valve surface meshes with improved smoothness and shape quality, while requiring fewer explicit regularization terms compared to the traditional methods. These results highlight that using structured quad meshes for the template and neural network training not only ensures mesh correspondence and quality but also simplifies the training process, thus enhancing the effectiveness and efficiency of aortic valve modeling.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study</title>
    <link href="/2025/11/05/eess.IV/2025-11-05-2511_03876v1/"/>
    <url>/2025/11/05/eess.IV/2025-11-05-2511_03876v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03876v1">http://arxiv.org/abs/2511.03876v1</a><br>Background: Non-invasive imaging-based assessment of blood flow plays a critical role in evaluating heart function and structure. Computed Tomography (CT) is a widely-used imaging modality that can robustly evaluate cardiovascular anatomy and function, but direct methods to estimate blood flow velocity from movies of contrast evolution have not been developed.   Purpose: This study evaluates the impact of CT imaging on Physics-Informed Neural Networks (PINN)-based flow estimation and proposes an improved framework, SinoFlow, which uses sinogram data directly to estimate blood flow.   Methods: We generated pulsatile flow fields in an idealized 2D vessel bifurcation using computational fluid dynamics and simulated CT scans with varying gantry rotation speeds, tube currents, and pulse mode imaging settings. We compared the performance of PINN-based flow estimation using reconstructed images (ImageFlow) to SinoFlow.   Results: SinoFlow significantly improved flow estimation performance by avoiding propagating errors introduced by filtered backprojection. SinoFlow was robust across all tested gantry rotation speeds and consistently produced lower mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow was compatible with pulsed-mode imaging and maintained higher accuracy with shorter pulse widths.   Conclusions: This study demonstrates the potential of SinoFlow for CT-based flow estimation, providing a more promising approach for non-invasive blood flow assessment. The findings aim to inform future applications of PINNs to CT images and provide a solution for image-based estimation, with reasonable acquisition parameters yielding accurate flow estimates.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
      <tag>physics.med-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural</title>
    <link href="/2025/11/05/eess.SY/2025-11-05-2511_03651v1/"/>
    <url>/2025/11/05/eess.SY/2025-11-05-2511_03651v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03651v1">http://arxiv.org/abs/2511.03651v1</a><br>This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world’s largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone’s propellers, which also protects the drone’s critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system’s robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.9; J.5</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03093v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03093v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03093v1">http://arxiv.org/abs/2511.03093v1</a><br>Cardiac contraction is a rapid, coordinated process that unfolds across three-dimensional tissue on millisecond timescales. Traditional optical imaging is often inadequate for capturing dynamic cellular structure in the beating heart because of a fundamental trade-off between spatial and temporal resolution. To overcome these limitations, we propose a high-performance computational imaging framework that integrates Compressive Sensing (CS) with Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The system performs compressed acquisition of fluorescence signals via random binary mask coding using a Digital Micromirror Device (DMD). We propose a Plug-and-Play (PnP) framework, solved using the alternating direction method of multipliers (ADMM), which flexibly incorporates advanced denoisers, including Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in dynamic imaging, we further introduce temporal regularization enforcing smoothness between adjacent z-slices. Experimental results on zebrafish heart imaging under high compression ratios demonstrate that the proposed method successfully reconstructs cellular structures with excellent denoising performance and image clarity, validating the effectiveness and robustness of our algorithm in real-world high-speed, low-light biological imaging scenarios.</p>]]></content>
    
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03098v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03098v1">http://arxiv.org/abs/2511.03098v1</a><br>The Intermeshed Steel Connection (ISC) system, when paired with robotic manipulators, can accelerate steel-frame assembly and improve worker safety by eliminating manual assembly. Dependable perception is one of the initial stages for ISC-aware robots. However, this is hampered by the absence of a dedicated image corpus, as collecting photographs on active construction sites is logistically difficult and raises safety and privacy concerns. In response, we introduce ISC-Perception, the first hybrid dataset expressly designed for ISC component detection. It blends procedurally rendered CAD images, game-engine photorealistic scenes, and a limited, curated set of real photographs, enabling fully automatic labelling of the synthetic portion. We explicitly account for all human effort to produce the dataset, including simulation engine and scene setup, asset preparation, post-processing scripts and quality checks; our total human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for manual labelling at 60,s per image (-81.7%). A manual pilot on a representative image with five instances of ISC members took 60,s (maximum 80,s), anchoring the manual baseline. Detectors trained on ISC-Perception achieved a mean Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we report <a href="mailto:&#x6d;&#x41;&#80;&#64;&#48;&#46;&#53;&#x30;">mAP@0.50</a>&#x2F;mAP@[0.50:0.95] of 0.943&#x2F;0.823. By bridging the data gap for construction-robotics perception, ISC-Perception facilitates rapid development of custom object detectors and is freely available for research and industrial use upon request.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03099v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03099v1">http://arxiv.org/abs/2511.03099v1</a><br>In orthodontic treatment, particularly within telemedicine contexts, observing patients’ dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Accelerating Physical Property Reasoning for Augmented Visual Cognition</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03126v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03126v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03126v1">http://arxiv.org/abs/2511.03126v1</a><br>This paper introduces \sysname, a system that accelerates vision-guided physical property reasoning to enable augmented visual cognition. \sysname minimizes the run-time latency of this reasoning pipeline through a combination of both algorithmic and systematic optimizations, including rapid geometric 3D reconstruction, efficient semantic feature fusion, and parallel view encoding. Through these simple yet effective optimizations, \sysname reduces the end-to-end latency of this reasoning pipeline from 10–20 minutes to less than 6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname achieves this 62.9$\times$–287.2$\times$ speedup while not only reaching on-par (and sometimes slightly better) object-level physical property estimation accuracy(e.g. mass), but also demonstrating superior performance in material segmentation and voxel-level inference than two SOTA baselines. We further combine gaze-tracking with \sysname to localize the object of interest in cluttered, real-world environments, streamlining the physical property reasoning on smart glasses. The case study with Meta Aria Glasses conducted at an IKEA furniture store demonstrates that \sysname achives consistently high performance compared to controlled captures, providing robust property estimations even with fewer views in real-world scenarios.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03120v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03120v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03120v1">http://arxiv.org/abs/2511.03120v1</a><br>Integrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03132v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03132v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03132v1">http://arxiv.org/abs/2511.03132v1</a><br>This paper presents the first AI&#x2F;ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI&#x2F;ML for damage assessment during a disaster and lessons learned to the benefit of the AI&#x2F;ML research and user communities.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03147v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03147v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03147v1">http://arxiv.org/abs/2511.03147v1</a><br>Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test Time Adaptation Using Adaptive Quantile Recalibration</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03148v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03148v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03148v1">http://arxiv.org/abs/2511.03148v1</a><br>Domain adaptation is a key strategy for enhancing the generalizability of deep learning models in real-world scenarios, where test distributions often diverge significantly from the training domain. However, conventional approaches typically rely on prior knowledge of the target domain or require model retraining, limiting their practicality in dynamic or resource-constrained environments. Recent test-time adaptation methods based on batch normalization statistic updates allow for unsupervised adaptation, but they often fail to capture complex activation distributions and are constrained to specific normalization layers. We propose Adaptive Quantile Recalibration (AQR), a test-time adaptation technique that modifies pre-activation distributions by aligning quantiles on a channel-wise basis. AQR captures the full shape of activation distributions and generalizes across architectures employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of estimating distribution tails under varying batch sizes, AQR incorporates a robust tail calibration strategy that improves stability and precision. Our method leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining models. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR achieves robust adaptation across diverse settings, outperforming existing test-time adaptation baselines. These results highlight AQR’s potential for deployment in real-world scenarios with dynamic and unpredictable data distributions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Finetuning-Free Personalization of Text to Image Generation via Hypernetworks</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03156v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03156v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03156v1">http://arxiv.org/abs/2511.03156v1</a><br>Personalizing text-to-image diffusion models has traditionally relied on subject-specific fine-tuning approaches such as DreamBooth~\cite{ruiz2023dreambooth}, which are computationally expensive and slow at inference. Recent adapter- and encoder-based methods attempt to reduce this overhead but still depend on additional fine-tuning or large backbone models for satisfactory results. In this work, we revisit an orthogonal direction: fine-tuning-free personalization via Hypernetworks that predict LoRA-adapted weights directly from subject images. Prior hypernetwork-based approaches, however, suffer from costly data generation or unstable attempts to mimic base model optimization trajectories. We address these limitations with an end-to-end training objective, stabilized by a simple output regularization, yielding reliable and effective hypernetworks. Our method removes the need for per-subject optimization at test time while preserving both subject fidelity and prompt alignment. To further enhance compositional generalization at inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG), which combines the compositional strengths of the base diffusion model with the subject fidelity of personalized models during sampling. Extensive experiments on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves strong personalization performance and highlights the promise of hypernetworks as a scalable and effective direction for open-category personalization.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03163v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03163v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03163v1">http://arxiv.org/abs/2511.03163v1</a><br>Accurate detection and delineation of anatomical structures in medical imaging are critical for computer-assisted interventions, particularly in laparoscopic liver surgery where 2D video streams limit depth perception and complicate landmark localization. While recent works have leveraged monocular depth cues for enhanced landmark detection, challenges remain in fusing RGB and depth features and in efficiently adapting large-scale vision models to surgical domains. We propose a depth-guided liver landmark segmentation framework integrating semantic and geometric cues via vision foundation encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB features and Depth Anything V2 (DA2) encoder to extract depth-aware features. To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient projection method that replaces the computationally expensive SVD with a Subsampled Randomized Fourier Transform (SRFT). This enables efficient fine-tuning of high-dimensional attention layers without sacrificing representational power. A cross-attention fusion module further integrates RGB and depth cues. To assess cross-dataset generalization, we also construct a new Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark. On the public L3D dataset, our method achieves a 4.85% improvement in Dice Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface Distance compared to the D2GPLand. To further assess generalization capability, we evaluate our model on LLSD dataset. Our model maintains competitive performance and significantly outperforms SAM-based baselines, demonstrating strong cross-dataset robustness and adaptability to unseen surgical environments. These results demonstrate that our SRFT-GaLore-enhanced dual-encoder framework enables scalable and precise segmentation under real-time, depth-constrained surgical settings.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03178v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03178v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03178v1">http://arxiv.org/abs/2511.03178v1</a><br>Anticipating forthcoming surgical events is vital for real-time assistance in endonasal transsphenoidal pituitary surgery, where visibility is limited and workflow changes rapidly. Most visual question answering (VQA) systems reason on isolated frames with static vision language alignment, providing little support for forecasting next steps or instrument needs. Existing surgical VQA datasets likewise center on the current scene rather than the near future. We introduce PitVQA-Anticipation, the first VQA dataset designed for forward looking surgical reasoning. It comprises 33.5 hours of operative video and 734,769 question answer pairs built from temporally grouped clips and expert annotations across four tasks: predicting the future phase, next step, upcoming instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video language model that adapts a large language model using a GRU Gated Temporal Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics, while an adaptive gate injects visual context into the language stream at the token level. Parameter efficient fine tuning customizes the language backbone to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and EndoVis datasets, surpassing strong image and video based baselines. Ablations show that temporal recurrence and gated fusion drive most of the gains. A frame budget study indicates a trade-off: 8 frames maximize fluency, whereas 32 frames slightly reduce BLEU but improve numeric time estimation. By pairing a temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA advances surgical VQA from retrospective description to proactive anticipation. PitVQA-Anticipation offers a comprehensive benchmark for this setting and highlights the importance of targeted temporal modeling for reliable, future aware surgical assistance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Probabilistic U-Net Approach to Downscaling Climate Simulations</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03197v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03197v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03197v1">http://arxiv.org/abs/2511.03197v1</a><br>Climate models are limited by heavy computational costs, often producing outputs at coarse spatial resolutions, while many climate change impact studies require finer scales. Statistical downscaling bridges this gap, and we adapt the probabilistic U-Net for this task, combining a deterministic U-Net backbone with a variational latent space to capture aleatoric uncertainty. We evaluate four training objectives, afCRPS and WMSE-MS-SSIM with three settings for downscaling precipitation and temperature from $16\times$ coarser resolution. Our main finding is that WMSE-MS-SSIM performs well for extremes under certain settings, whereas afCRPS better captures spatial variability across scales.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>physics.ao-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03206v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03206v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03206v1">http://arxiv.org/abs/2511.03206v1</a><br>Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03194v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03194v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03194v1">http://arxiv.org/abs/2511.03194v1</a><br>Publicly available, large-scale medical imaging datasets are crucial for developing and validating artificial intelligence models and conducting retrospective clinical research. However, datasets that combine functional and anatomical imaging with detailed clinical reports across multiple cancer types remain scarce. Here, we present PETWB-REP, a curated dataset comprising whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography&#x2F;Computed Tomography (PET&#x2F;CT) scans and corresponding radiology reports from 490 patients diagnosed with various malignancies. The dataset primarily includes common cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and ovarian cancer. This dataset includes paired PET and CT images, de-identified textual reports, and structured clinical metadata. It is designed to support research in medical imaging, radiomics, artificial intelligence, and multi-modal learning.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03212v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03212v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03212v1">http://arxiv.org/abs/2511.03212v1</a><br>Accurately assessing the risk of cesarean section (CS) delivery is critical, especially in settings with limited medical resources, where access to healthcare is often restricted. Early and reliable risk prediction allows better-informed prenatal care decisions and can improve maternal and neonatal outcomes. However, most existing predictive models are tailored for in-hospital use during labor and rely on parameters that are often unavailable in resource-limited or home-based settings. In this study, we conduct a pilot investigation to examine the feasibility of using 3D body shape for CS risk assessment for future applications with more affordable general devices. We propose a novel multi-view-based Transformer network, MvBody, which predicts CS risk using only self-reported medical data and 3D optical body scans obtained between the 31st and 38th weeks of gestation. To enhance training efficiency and model generalizability in data-scarce environments, we incorporate a metric learning loss into the network. Compared to widely used machine learning models and the latest advanced 3D analysis methods, our method demonstrates superior performance, achieving an accuracy of 84.62% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set. To improve transparency and trust in the model’s predictions, we apply the Integrated Gradients algorithm to provide theoretically grounded explanations of the model’s decision-making process. Our results indicate that pre-pregnancy weight, maternal age, obstetric history, previous CS history, and body shape, particularly around the head and shoulders, are key contributors to CS risk prediction.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T10, 68T45</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03219v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03219v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03219v1">http://arxiv.org/abs/2511.03219v1</a><br>Augmentation for dense prediction typically relies on either sample mixing or generative synthesis. Mixing improves robustness but misaligned masks yield soft label ambiguity. Diffusion synthesis increases apparent diversity but, when trained as common samples, overlooks the structural benefit of mask conditioning and introduces synthetic-real domain shift. We propose a paired, diffusion-guided paradigm that fuses the strengths of both. For each real image, a synthetic counterpart is generated under the same mask and the pair is used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which mixes only image appearance while supervision always uses the original hard mask. This produces a continuous family of intermediate samples that smoothly bridges synthetic and real appearances under shared geometry, enlarging diversity without compromising pixel-level semantics. To keep learning aligned with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the mixing strength and the loss weight of mixed samples over training, gradually re-anchoring optimization to real data and mitigating distributional bias. Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC 2017, the approach achieves state-of-the-art segmentation performance and consistent gains over baselines. The results show that combining label-preserving mixing with diffusion-driven diversity, together with adaptive re-anchoring, yields robust and generalizable endoscopic segmentation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03232v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03232v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03232v1">http://arxiv.org/abs/2511.03232v1</a><br>Recently, Mamba-based super-resolution (SR) methods have demonstrated the ability to capture global receptive fields with linear complexity, addressing the quadratic computational cost of Transformer-based SR approaches. However, existing Mamba-based methods lack fine-grained transitions across different modeling scales, which limits the efficiency of feature representation. In this paper, we propose T-PMambaSR, a lightweight SR framework that integrates window-based self-attention with Progressive Mamba. By enabling interactions among receptive fields of different scales, our method establishes a fine-grained modeling paradigm that progressively enhances feature representation with linear complexity. Furthermore, we introduce an Adaptive High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost during Transformer and Mamba processing. Extensive experiments demonstrate that T-PMambaSR progressively enhances the model’s receptive field and expressiveness, yielding better performance than recent Transformer- or Mamba-based methods while incurring lower computational cost. Our codes will be released after acceptance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03239v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03239v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03239v1">http://arxiv.org/abs/2511.03239v1</a><br>Modern AI systems are increasingly constrained not by model capacity but by the quality and diversity of their data. Despite growing emphasis on data-centric AI, most datasets are still gathered in an open-loop manner which accumulates redundant samples without feedback from the current coverage. This results in inefficient storage, costly labeling, and limited generalization. To address this, this paper introduces \ac{FCDC}, a paradigm that formulates data collection as a closed-loop control problem. \ac{FCDC} continuously approximates the state of the collected data distribution using an online probabilistic model and adaptively regulates sample retention using based on feedback signals such as likelihood and Mahalanobis distance. Through this feedback mechanism, the system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy from accumulating over time. Besides showcasing the controllability of \ac{FCDC} on a synthetic dataset, experiments on a real data stream show that \ac{FCDC} produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data storage by $\SI{39.8}{\percent}$. These results demonstrate that data collection itself can be actively controlled, transforming collection from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03245v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03245v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03245v1">http://arxiv.org/abs/2511.03245v1</a><br>Recently, remarkable progress has been made in large-scale pre-trained model tuning, and inference efficiency is becoming more crucial for practical deployment. Early exiting in conjunction with multi-stage predictors, when cooperated with a parameter-efficient fine-tuning strategy, offers a straightforward way to achieve an inference-efficient model. However, a key challenge remains unresolved: How can early stages provide low-level fundamental features to deep stages while simultaneously supplying high-level discriminative features to early-stage predictors? To address this problem, we propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively decouple the low-level representative ability and high-level discriminative ability in early stages. First, in terms of architecture, we introduce a lightweight bypass module into multi-stage predictors for functional decomposition of shallow features from early stages, while a high-order statistics-based predictor is developed for early stages to effectively enhance their discriminative ability. To reasonably train our multi-predictor architecture, a decoupled optimization is proposed to allocate two-phase loss weights for multi-stage predictors during model tuning, where the initial training phase enables the model to prioritize the acquisition of discriminative ability of deep stages via emphasizing representative ability of early stages, and the latter training phase drives discriminative ability towards earlier stages as much as possible. As such, our DMPO can effectively decouple representative and discriminative abilities in early stages in terms of architecture design and model optimization. Experiments across various datasets and pre-trained backbones demonstrate that DMPO clearly outperforms its counterparts when reducing computational cost.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative deep learning for foundational video translation in ultrasound</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03255v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03255v1">http://arxiv.org/abs/2511.03255v1</a><br>Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+&#x2F;-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+&#x2F;-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+&#x2F;-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupled Entropy Minimization</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03256v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03256v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03256v1">http://arxiv.org/abs/2511.03256v1</a><br>Entropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>stat.ML</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>math.ST</tag>
      
      <tag>stat.TH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Medical Image Segmentation via Heat Conduction Equation</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03260v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03260v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03260v1">http://arxiv.org/abs/2511.03260v1</a><br>Medical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03267v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03267v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03267v1">http://arxiv.org/abs/2511.03267v1</a><br>3D anomaly detection (3D-AD) plays a critical role in industrial manufacturing, particularly in ensuring the reliability and safety of core equipment components. Although existing 3D datasets like Real3D-AD and MVTec 3D-AD offer broad application support, they fall short in capturing the complexities and subtle defects found in real industrial environments. This limitation hampers precise anomaly detection research, especially for industrial equipment components (IEC) such as bearings, rings, and bolts. To address this challenge, we have developed a point cloud anomaly detection dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is directly collected from actual production lines, ensuring high fidelity and relevance. Compared to existing datasets, IEC3D-AD features significantly improved point cloud resolution and defect annotation granularity, facilitating more demanding anomaly detection tasks. Furthermore, inspired by generative 2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This paradigm generates synthetic point cloud samples based on geometric morphological analysis, then reduces the margin and increases the overlap between normal and abnormal point-level features through spatial discrepancy optimization. Extensive experiments demonstrate the effectiveness of our method on both IEC3D-AD and other datasets.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03272v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03272v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03272v1">http://arxiv.org/abs/2511.03272v1</a><br>Generating long videos remains a fundamental challenge, and achieving high controllability in video inpainting and outpainting is particularly demanding. To address both of these challenges simultaneously and achieve controllable video inpainting and outpainting for long video clips, we introduce a novel and unified approach for long video inpainting and outpainting that extends text-to-video diffusion models to generate arbitrarily long, spatially edited videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a large pre-trained video diffusion model like Alibaba’s Wan 2.1 for masked region video synthesis, and employs an overlap-and-blend temporal co-denoising strategy with high-order solvers to maintain consistency across long sequences. In contrast to prior work that struggles with fixed-length clips or exhibits stitching artifacts, our system enables arbitrarily long video generation and editing without noticeable seams or drift. We validate our approach on challenging inpainting&#x2F;outpainting tasks including editing or adding objects over hundreds of frames and demonstrate superior performance to baseline methods like Wan 2.1 model and VACE in terms of quality (PSNR&#x2F;SSIM), and perceptual realism (LPIPS). Our method enables practical long-range video editing with minimal overhead, achieved a balance between parameter efficient and superior performance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03317v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03317v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03317v1">http://arxiv.org/abs/2511.03317v1</a><br>Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at <a href="https://github.com/AIDC-AI/Diffusion-SDPO">https://github.com/AIDC-AI/Diffusion-SDPO</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03328v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03328v1">http://arxiv.org/abs/2511.03328v1</a><br>A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of “reasoning MLLMs” that offer explicit control over their internal thinking processes (normally referred as the “thinking mode”) alongside the standard “non-thinking mode”. This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these “dual-state” MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active “thinking mode” capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03325v2/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03325v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03325v2">http://arxiv.org/abs/2511.03325v2</a><br>Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video–Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool–tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11% on REAL-Colon-VQA and +9% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at <a href="https://github.com/madratak/SurgViVQA">https://github.com/madratak/SurgViVQA</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03332v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03332v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03332v1">http://arxiv.org/abs/2511.03332v1</a><br>In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03334v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03334v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03334v1">http://arxiv.org/abs/2511.03334v1</a><br>Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen’s robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03367v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03367v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03367v1">http://arxiv.org/abs/2511.03367v1</a><br>Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: <a href="https://github.com/Gahyeonkim09/AAPL">https://github.com/Gahyeonkim09/AAPL</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene Mutation Prediction from Histopathology</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03365v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03365v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03365v1">http://arxiv.org/abs/2511.03365v1</a><br>Ovarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&amp;E) histopathological images. Using $\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2%$ (Macro AUC of $0.87 \pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} &#x3D; 0.82 \pm 0.02$, $AUC_{BRCA1} &#x3D; 0.76 \pm 0.04$, and $AUC_{ARID1A} &#x3D; 0.73 \pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing What You Say: Expressive Image Generation from Speech</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03423v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03423v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03423v1">http://arxiv.org/abs/2511.03423v1</a><br>This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03416v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03416v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03416v1">http://arxiv.org/abs/2511.03416v1</a><br>Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans. In this work, we propose an automated method for standardizing this alignment. Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo’s principal axes, from which four candidate orientations are derived. The candidate in standard orientation is selected using one of three strategies: a heuristic based on Pearson’s correlation assessing shape, image matching to an atlas through normalized cross-correlation, and a Random Forest classifier. We tested our method on 2166 images longitudinally acquired 3D ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images, PCA correctly extracted the principal axes of the embryo. The correct candidate was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%, 95.8%, and 98.4% of images, respectively. A Majority Vote of these selection methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline enables consistent embryonic alignment in the first trimester, enabling scalable analysis in both clinical and research settings. The code is publicly available at: <a href="https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment">https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generalizing Shape-from-Template to Topological Changes</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03459v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03459v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03459v1">http://arxiv.org/abs/2511.03459v1</a><br>Reconstructing the surfaces of deformable objects from correspondences between a 3D template and a 2D image is well studied under Shape-from-Template (SfT) methods; however, existing approaches break down when topological changes accompany the deformation. We propose a principled extension of SfT that enables reconstruction in the presence of such changes. Our approach is initialized with a classical SfT solution and iteratively adapts the template by partitioning its spatial domain so as to minimize an energy functional that jointly encodes physical plausibility and reprojection consistency. We demonstrate that the method robustly captures a wide range of practically relevant topological events including tears and cuts on bounded 2D surfaces, thereby establishing the first general framework for topological-change-aware SfT. Experiments on both synthetic and real data confirm that our approach consistently outperforms baseline methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03571v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03571v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03571v1">http://arxiv.org/abs/2511.03571v1</a><br>Robust 3D semantic occupancy is crucial for legged&#x2F;humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free&#x2F;occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range&#x2F;occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic occupancy; standardized within-&#x2F;cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged&#x2F;humanoid robots. Datasets and code will be publicly available at <a href="https://github.com/MasterHow/OneOcc">https://github.com/MasterHow/OneOcc</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Human Mesh Modeling for Anny Body</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03589v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03589v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03589v1">http://arxiv.org/abs/2511.03589v1</a><br>Parametric body models are central to many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms – across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling – supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic humans generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models, while remaining interpretable and broadly representative. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Signal Intensity-weighted coordinate channels improve learning stability and generalisation in 1D and 2D CNNs in localisation tasks on biomedical signals</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03645v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03645v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03645v1">http://arxiv.org/abs/2511.03645v1</a><br>Localisation tasks in biomedical data often require models to learn meaningful spatial or temporal relationships from signals with complex intensity distributions. A common strategy, exemplified by CoordConv layers, is to append coordinate channels to convolutional inputs, enabling networks to learn absolute positions. In this work, we propose a signal intensity-weighted coordinate representation that replaces the pure coordinate channels with channels scaled by local signal intensity. This modification embeds an intensity-position coupling directly in the input representation, introducing a simple and modality-agnostic inductive bias. We evaluate the approach on two distinct localisation problems: (i) predicting the time of morphological transition in 20-second, two-lead ECG signals, and (ii) regressing the coordinates of nuclear centres in cytological images from the SiPaKMeD dataset. In both cases, the proposed representation yields faster convergence and higher generalisation performance relative to conventional coordinate-channel approaches, demonstrating its effectiveness across both one-dimensional and two-dimensional biomedical signals.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03651v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03651v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03651v1">http://arxiv.org/abs/2511.03651v1</a><br>This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world’s largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone’s propellers, which also protects the drone’s critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system’s robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.9; J.5</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03665v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03665v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03665v1">http://arxiv.org/abs/2511.03665v1</a><br>This paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03666v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03666v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03666v1">http://arxiv.org/abs/2511.03666v1</a><br>Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03725v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03725v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03725v1">http://arxiv.org/abs/2511.03725v1</a><br>Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature – intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets – KTH, Penn Action, HAA500, and UCF-101 – demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03765v2/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03765v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03765v2">http://arxiv.org/abs/2511.03765v2</a><br>On-device fine-tuning of CNNs is essential to withstand domain shift in edge applications such as Human Activity Recognition (HAR), yet full fine-tuning is infeasible under strict memory, compute, and energy budgets. We present LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional layers, (ii) selectively updates only the output-side core with zero-initialization to keep the auxiliary path inactive at the start, and (iii) fuses the update back into dense kernels, leaving inference cost unchanged. This design preserves convolutional structure and reduces the number of trainable parameters by up to two orders of magnitude compared to full fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, consistently outperforming prior parameter-efficient baselines under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and selective-core training yield 1.4-3.8x faster convergence to target F1. LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN adaptation practical for edge platforms.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>What&#39;s in Common? Multimodal Models Hallucinate When Reasoning Across Scenes</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03768v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03768v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03768v1">http://arxiv.org/abs/2511.03768v1</a><br>Multimodal language models possess a remarkable ability to handle an open-vocabulary’s worth of objects. Yet the best models still suffer from hallucinations when reasoning about scenes in the real world, revealing a gap between their seemingly strong performance on existing perception benchmarks that are saturating and their reasoning in the real world. To address this gap, we build a novel benchmark of in-the-wild scenes that we call Common-O. With more than 10.5k examples using exclusively new images not found in web training data to avoid contamination, Common-O goes beyond just perception, inspired by cognitive tests for humans, to probe reasoning across scenes by asking “what’s in common?”. We evaluate leading multimodal language models, including models specifically trained to perform chain-of-thought reasoning. We find that perceiving objects in single images is tractable for most models, yet reasoning across scenes is very challenging even for the best models, including reasoning models. Despite saturating many leaderboards focusing on perception, the best performing model only achieves 35% on Common-O – and on Common-O Complex, consisting of more complex scenes, the best model achieves only 1%. Curiously, we find models are more prone to hallucinate when similar objects are present in the scene, suggesting models may be relying on object co-occurrence seen during training. Among the models we evaluated, we found scale can provide modest improvements while models explicitly trained with multi-image inputs show bigger improvements, suggesting scaled multi-image training may offer promise. We make our benchmark publicly available to spur research into the challenge of hallucination when reasoning across scenes.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SILVI: Simple Interface for Labeling Video Interactions</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03819v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03819v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03819v1">http://arxiv.org/abs/2511.03819v1</a><br>Computer vision methods are increasingly used for the automated analysis of large volumes of video data collected through camera traps, drones, or direct observations of animals in the wild. While recent advances have focused primarily on detecting individual actions, much less work has addressed the detection and annotation of interactions – a crucial aspect for understanding social and individualized animal behavior. Existing open-source annotation tools support either behavioral labeling without localization of individuals, or localization without the capacity to capture interactions. To bridge this gap, we present SILVI, an open-source labeling software that integrates both functionalities. SILVI enables researchers to annotate behaviors and interactions directly within video data, generating structured outputs suitable for training and validating computer vision models. By linking behavioral ecology with computer vision, SILVI facilitates the development of automated approaches for fine-grained behavioral analyses. Although developed primarily in the context of animal behavior, SILVI could be useful more broadly to annotate human interactions in other videos that require extracting dynamic scene graphs. The software, along with documentation and download instructions, is available at: <a href="https://gitlab.gwdg.de/kanbertay/interaction-labelling-app">https://gitlab.gwdg.de/kanbertay/interaction-labelling-app</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03855v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03855v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03855v1">http://arxiv.org/abs/2511.03855v1</a><br>Deep learned (DL) models for image recognition have been shown to fail to generalize to data from different devices, populations, etc. COVID-19 detection from Chest X-rays (CXRs), in particular, has been shown to fail to generalize to out-of-distribution (OOD) data from new clinical sources not covered in the training set. This occurs because models learn to exploit shortcuts - source-specific artifacts that do not translate to new distributions - rather than reasonable biomarkers to maximize performance on in-distribution (ID) data. Rendering the models more robust to distribution shifts, our study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during training. Our empirical results demonstrate that this technique can significantly reduce the performance gap between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results averaged over ten random seeds across key metrics such as AUC, F1, accuracy, recall and specificity. Our source code is publicly available at <a href="https://github.com/Duongmai127/Noisy-ood">https://github.com/Duongmai127/Noisy-ood</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03876v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03876v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03876v1">http://arxiv.org/abs/2511.03876v1</a><br>Background: Non-invasive imaging-based assessment of blood flow plays a critical role in evaluating heart function and structure. Computed Tomography (CT) is a widely-used imaging modality that can robustly evaluate cardiovascular anatomy and function, but direct methods to estimate blood flow velocity from movies of contrast evolution have not been developed.   Purpose: This study evaluates the impact of CT imaging on Physics-Informed Neural Networks (PINN)-based flow estimation and proposes an improved framework, SinoFlow, which uses sinogram data directly to estimate blood flow.   Methods: We generated pulsatile flow fields in an idealized 2D vessel bifurcation using computational fluid dynamics and simulated CT scans with varying gantry rotation speeds, tube currents, and pulse mode imaging settings. We compared the performance of PINN-based flow estimation using reconstructed images (ImageFlow) to SinoFlow.   Results: SinoFlow significantly improved flow estimation performance by avoiding propagating errors introduced by filtered backprojection. SinoFlow was robust across all tested gantry rotation speeds and consistently produced lower mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow was compatible with pulsed-mode imaging and maintained higher accuracy with shorter pulse widths.   Conclusions: This study demonstrates the potential of SinoFlow for CT-based flow estimation, providing a more promising approach for non-invasive blood flow assessment. The findings aim to inform future applications of PINNs to CT images and provide a solution for image-based estimation, with reasonable acquisition parameters yielding accurate flow estimates.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
      <tag>physics.med-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03888v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03888v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03888v1">http://arxiv.org/abs/2511.03888v1</a><br>The global waste crisis is escalating, with solid waste generation expected to increase by 70% by 2050. Traditional waste collection methods, particularly in remote or harsh environments like deserts, are labor-intensive, inefficient, and often hazardous. Recent advances in computer vision and deep learning have opened the door to automated waste detection systems, yet most research focuses on urban environments and recyclable materials, overlooking organic and hazardous waste and underexplored terrains such as deserts. In this work, we propose an enhanced real-time object detection framework based on a pruned, lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies. Using the DroneTrashNet dataset, we demonstrate significant improvements in precision, recall, and mean average precision (mAP), while achieving low latency and compact model size suitable for deployment on resource-constrained aerial drones. Benchmarking our model against state-of-the-art lightweight YOLO variants further highlights its optimal balance of accuracy and efficiency. Our results validate the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03882v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03882v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03882v1">http://arxiv.org/abs/2511.03882v1</a><br>Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03890v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03890v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03890v1">http://arxiv.org/abs/2511.03890v1</a><br>Accurate geometric modeling of the aortic valve from 3D CT images is essential for biomechanical analysis and patient-specific simulations to assess valve health or make a preoperative plan. However, it remains challenging to generate aortic valve meshes with both high-quality and consistency across different patients. Traditional approaches often produce triangular meshes with irregular topologies, which can result in poorly shaped elements and inconsistent correspondence due to inter-patient anatomical variation. In this work, we address these challenges by introducing a template-fitting pipeline with deep neural networks to generate structured quad (i.e., quadrilateral) meshes from 3D CT images to represent aortic valve geometries. By remeshing aortic valves of all patients with a common quad mesh template, we ensure a uniform mesh topology with consistent node-to-node and element-to-element correspondence across patients. This consistency enables us to simplify the learning objective of the deep neural networks, by employing a loss function with only two terms (i.e., a geometry reconstruction term and a smoothness regularization term), which is sufficient to preserve mesh smoothness and element quality. Our experiments demonstrate that the proposed approach produces high-quality aortic valve surface meshes with improved smoothness and shape quality, while requiring fewer explicit regularization terms compared to the traditional methods. These results highlight that using structured quad meshes for the template and neural network training not only ensures mesh correspondence and quality but also simplifies the training process, thus enhancing the effectiveness and efficiency of aortic valve modeling.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03891v2/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03891v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03891v2">http://arxiv.org/abs/2511.03891v2</a><br>Small, imbalanced datasets and poor input image quality can lead to high false predictions rates with deep learning models. This paper introduces Class-Based Image Composition, an approach that allows us to reformulate training inputs through a fusion of multiple images of the same class into combined visual composites, named Composite Input Images (CoImg). That enhances the intra-class variance and improves the valuable information density per training sample and increases the ability of the model to distinguish between subtle disease patterns. Our method was evaluated on the Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et al., 2024), which contains 2,064 high-resolution optical coherence tomography (OCT) scans of the human retina, representing seven distinct diseases with a significant class imbalance. We constructed a perfectly class-balanced version of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout composite image. To assess the effectiveness of this new representation, we conducted a comparative analysis between the original dataset and its variant using a VGG16 model. A fair comparison was ensured by utilizing the identical model architecture and hyperparameters for all experiments. The proposed approach markedly improved diagnostic results.The enhanced Dataset achieved near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared to a baseline model trained on raw dataset. The false prediction rate was also significantly lower, this demonstrates that the method can producehigh-quality predictions even for weak datasets affected by class imbalance or small sample size.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.DB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>I Detect What I Don&#39;t Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging</title>
    <link href="/2025/11/05/highlights/2025-11-05-2511_03912v1/"/>
    <url>/2025/11/05/highlights/2025-11-05-2511_03912v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03912v1">http://arxiv.org/abs/2511.03912v1</a><br>Unknown anomaly detection in medical imaging remains a fundamental challenge due to the scarcity of labeled anomalies and the high cost of expert supervision. We introduce an unsupervised, oracle-free framework that incrementally expands a trusted set of normal samples without any anomaly labels. Starting from a small, verified seed of normal images, our method alternates between lightweight adapter updates and uncertainty-gated sample admission. A frozen pretrained vision backbone is augmented with tiny convolutional adapters, ensuring rapid domain adaptation with negligible computational overhead. Extracted embeddings are stored in a compact coreset enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during incremental expansion is enforced by dual probabilistic gates, a sample is admitted into the normal memory only if its distance to the existing coreset lies within a calibrated z-score threshold, and its SWAG-based epistemic uncertainty remains below a seed-calibrated bound. This mechanism prevents drift and false inclusions without relying on generative reconstruction or replay buffers. Empirically, our system steadily refines the notion of normality as unlabeled data arrive, producing substantial gains over baselines. On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5, ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These results highlight the effectiveness and efficiency of the proposed framework for real-world, label-scarce medical imaging applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02193v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02193v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02193v1">http://arxiv.org/abs/2511.02193v1</a><br>Accurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 $%$ on DRIVE and 1.25 $%$ on STARE, demonstrating its effectiveness and advancement. The project code is public via <a href="https://github.com/liujiawen-jpg/MM-UNet">https://github.com/liujiawen-jpg/MM-UNet</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02207v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02207v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02207v1">http://arxiv.org/abs/2511.02207v1</a><br>Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02210v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02210v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02210v1">http://arxiv.org/abs/2511.02210v1</a><br>Segmental longitudinal strain (SLS) of the left ventricle (LV) is an important prognostic indicator for evaluating regional LV dysfunction, in particular for diagnosing and managing myocardial ischemia. Current techniques for strain estimation require significant manual intervention and expertise, limiting their efficiency and making them too resource-intensive for monitoring purposes. This study introduces the first automated pipeline, autoStrain, for SLS estimation in transesophageal echocardiography (TEE) using deep learning (DL) methods for motion estimation. We present a comparative analysis of two DL approaches: TeeFlow, based on the RAFT optical flow model for dense frame-to-frame predictions, and TeeTracker, based on the CoTracker point trajectory model for sparse long-sequence predictions.   As ground truth motion data from real echocardiographic sequences are hardly accessible, we took advantage of a unique simulation pipeline (SIMUS) to generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with ground truth myocardial motion to train and evaluate both models. Our evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.   Clinical validation on 16 patients further demonstrated that SLS estimation with our autoStrain pipeline aligned with clinical references, achieving a mean difference (95% limits of agreement) of 1.09% (-8.90% to 11.09%). Incorporation of simulated ischemia in the synTEE data improved the accuracy of the models in quantifying abnormal deformation. Our findings indicate that integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer&#39;s Disease Diagnosis</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02228v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02228v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02228v1">http://arxiv.org/abs/2511.02228v1</a><br>Alzheimer’s disease (AD) is the most prevalent form of dementia, and its early diagnosis is essential for slowing disease progression. Recent studies on multimodal neuroimaging fusion using MRI and PET have achieved promising results by integrating multi-scale complementary features. However, most existing approaches primarily emphasize cross-modal complementarity while overlooking the diagnostic importance of modality-specific features. In addition, the inherent distributional differences between modalities often lead to biased and noisy representations, degrading classification performance. To address these challenges, we propose a Collaborative Attention and Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The proposed model introduces a learnable parameter representation (LPR) block to compensate for missing modality information, followed by a shared encoder and modality-independent encoders to preserve both shared and specific representations. Furthermore, a consistency-guided mechanism is employed to explicitly align the latent distributions across modalities. Experimental results on the ADNI dataset demonstrate that our method achieves superior diagnostic performance compared with existing fusion strategies.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02400v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02400v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02400v1">http://arxiv.org/abs/2511.02400v1</a><br>The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: <a href="https://github.com/Minds-R-Lab/MammoClean">https://github.com/Minds-R-Lab/MammoClean</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02404v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02404v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02404v1">http://arxiv.org/abs/2511.02404v1</a><br>Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic cats) have vertically elongated pupils linked to ambush predation; yet, how such specializations manifest in downstream visual representations remains incompletely understood. We present a unified, frozen-encoder benchmark that quantifies feline-human cross-species representational alignment in the wild, across convolutional networks, supervised Vision Transformers, windowed transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel Alignment (linear and RBF) and Representational Similarity Analysis, with additional distributional and stability tests reported in the paper. Across models, DINO ViT-B&#x2F;16 attains the most substantial alignment (mean CKA-RBF $\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$), peaking at early blocks, indicating that token-level self-supervision induces early-stage features that bridge species-specific statistics. Supervised ViTs are competitive on CKA yet show weaker geometric correspondence than DINO (e.g., ViT-B&#x2F;16 RSA $\approx0.53$ at block8; ViT-L&#x2F;16 $\approx0.47$ at block14), revealing depth-dependent divergences between similarity and representational geometry. CNNs remain strong baselines but below plain ViTs on alignment, and windowed transformers underperform plain ViTs, implicating architectural inductive biases in cross-species alignment. Results indicate that self-supervision coupled with ViT inductive biases yields representational geometries that more closely align feline and human visual systems than widely used CNNs and windowed Transformers, providing testable neuroscientific hypotheses about where and how cross-species visual computations converge. We release our code and dataset for reference and reproducibility.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Kullback-Leibler divergence method for input-system-state identification</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02426v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02426v1">http://arxiv.org/abs/2511.02426v1</a><br>The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02560v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02560v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02560v1">http://arxiv.org/abs/2511.02560v1</a><br>We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at <a href="https://github.com/microsoft/SigmaCollab">https://github.com/microsoft/SigmaCollab</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02565v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02565v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02565v1">http://arxiv.org/abs/2511.02565v1</a><br>Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02580v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02580v1">http://arxiv.org/abs/2511.02580v1</a><br>Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02717v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02717v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02717v1">http://arxiv.org/abs/2511.02717v1</a><br>The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLEXICORP: End-user Explainability of Convolutional Neural Networks</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02720v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02720v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02720v1">http://arxiv.org/abs/2511.02720v1</a><br>Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.   To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.   We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NEF-NET+: Adapting Electrocardio panorama in the wild</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02880v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02880v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02880v1">http://arxiv.org/abs/2511.02880v1</a><br>Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and compensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Hints</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02933v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02933v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02933v1">http://arxiv.org/abs/2511.02933v1</a><br>Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_02953v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_02953v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02953v1">http://arxiv.org/abs/2511.02953v1</a><br>Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model’s ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment</title>
    <link href="/2025/11/04/cs.AI/2025-11-04-2511_03019v1/"/>
    <url>/2025/11/04/cs.AI/2025-11-04-2511_03019v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03019v1">http://arxiv.org/abs/2511.03019v1</a><br>Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning</title>
    <link href="/2025/11/04/cs.CL/2025-11-04-2511_02280v1/"/>
    <url>/2025/11/04/cs.CL/2025-11-04-2511_02280v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02280v1">http://arxiv.org/abs/2511.02280v1</a><br>We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at <a href="https://github.com/BytedanceDouyinContent/SAIL-RL">https://github.com/BytedanceDouyinContent/SAIL-RL</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning</title>
    <link href="/2025/11/04/cs.CL/2025-11-04-2511_02360v1/"/>
    <url>/2025/11/04/cs.CL/2025-11-04-2511_02360v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02360v1">http://arxiv.org/abs/2511.02360v1</a><br>In human cognition, there exist numerous thought processes that are tacit and beyond verbal expression, enabling us to understand and interact with the world in multiple ways. However, contemporary Vision-Language Models (VLMs) remain constrained to reasoning within the discrete and rigid space of linguistic tokens, thereby bottlenecking the rich, high-dimensional nature of visual perception. To bridge this gap, we propose CoCoVa (Chain of Continuous Vision-Language Thought), a novel framework for vision-language model that leverages continuous cross-modal reasoning for diverse vision-language tasks. The core of CoCoVa is an iterative reasoning cycle, where a novel Latent Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a chain of latent thought vectors through cross-modal fusion. To focus this process, a token selection mechanism dynamically identifies salient visual regions, mimicking attentional focus. To ensure these latent thoughts remain grounded, we train the model with a multi-task objective that combines contrastive learning and diffusion-based reconstruction, enforcing alignment between latent representations and both visual and textual modalities. Evaluations show CoCoVa improves accuracy and token efficiency over strong baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B models on almost all benchmarks. When scaled to 7B LLM backbones, it remains competitive with state-of-the-art models. Qualitative analysis validates that learned latent space captures interpretable and structured reasoning patterns, highlighting the potential of CoCoVa to bridge the representational gap between discrete language processing and the continuous nature of visual understanding.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</title>
    <link href="/2025/11/04/cs.CL/2025-11-04-2511_02495v1/"/>
    <url>/2025/11/04/cs.CL/2025-11-04-2511_02495v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02495v1">http://arxiv.org/abs/2511.02495v1</a><br>Recent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at <a href="https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890">https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniChange: Unifying Change Detection with Multimodal Large Language Model</title>
    <link href="/2025/11/04/cs.CL/2025-11-04-2511_02607v1/"/>
    <url>/2025/11/04/cs.CL/2025-11-04-2511_02607v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02607v1">http://arxiv.org/abs/2511.02607v1</a><br>Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at <a href="https://github.com/Erxucomeon/UniChange">https://github.com/Erxucomeon/UniChange</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions</title>
    <link href="/2025/11/04/cs.CL/2025-11-04-2511_02288v1/"/>
    <url>/2025/11/04/cs.CL/2025-11-04-2511_02288v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02288v1">http://arxiv.org/abs/2511.02288v1</a><br>We propose a Graph Neural Network (GNN)-based approach for Handwritten Mathematical Expression (HME) recognition by modeling HMEs as graphs, where nodes represent symbols and edges capture spatial dependencies. A deep BLSTM network is used for symbol segmentation, recognition, and spatial relation classification, forming an initial primitive graph. A 2D-CFG parser then generates all possible spatial relations, while the GNN-based link prediction model refines the structure by removing unnecessary connections, ultimately forming the Symbol Label Graph. Experimental results demonstrate the effectiveness of our approach, showing promising performance in HME structure recognition.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</title>
    <link href="/2025/11/04/cs.CL/2025-11-04-2511_02778v1/"/>
    <url>/2025/11/04/cs.CL/2025-11-04-2511_02778v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02778v1">http://arxiv.org/abs/2511.02778v1</a><br>Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model’s intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at <a href="https://github.com/CSU-JPG/VCode">https://github.com/CSU-JPG/VCode</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3D Point Cloud Object Detection on Edge Devices for Split Computing</title>
    <link href="/2025/11/04/cs.DC/2025-11-04-2511_02293v1/"/>
    <url>/2025/11/04/cs.DC/2025-11-04-2511_02293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02293v1">http://arxiv.org/abs/2511.02293v1</a><br>The field of autonomous driving technology is rapidly advancing, with deep learning being a key component. Particularly in the field of sensing, 3D point cloud data collected by LiDAR is utilized to run deep neural network models for 3D object detection. However, these state-of-the-art models are complex, leading to longer processing times and increased power consumption on edge devices. The objective of this study is to address these issues by leveraging Split Computing, a distributed machine learning inference method. Split Computing aims to lessen the computational burden on edge devices, thereby reducing processing time and power consumption. Furthermore, it minimizes the risk of data breaches by only transmitting intermediate data from the deep neural network model. Experimental results show that splitting after voxelization reduces the inference time by 70.8% and the edge device execution time by 90.0%. When splitting within the network, the inference time is reduced by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.</p>]]></content>
    
    
    <categories>
      
      <category>cs.DC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
      <tag>C.2.4; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Can Foundation Models Revolutionize Mobile AR Sparse Sensing?</title>
    <link href="/2025/11/04/cs.ET/2025-11-04-2511_02215v1/"/>
    <url>/2025/11/04/cs.ET/2025-11-04-2511_02215v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02215v1">http://arxiv.org/abs/2511.02215v1</a><br>Mobile sensing systems have long faced a fundamental trade-off between sensing quality and efficiency due to constraints in computation, power, and other limitations. Sparse sensing, which aims to acquire and process only a subset of sensor data, has been a key strategy for maintaining performance under such constraints. However, existing sparse sensing methods often suffer from reduced accuracy, as missing information across space and time introduces uncertainty into many sensing systems. In this work, we investigate whether foundation models can change the landscape of mobile sparse sensing. Using real-world mobile AR data, our evaluations demonstrate that foundation models offer significant improvements in geometry-aware image warping, a central technique for enabling accurate reuse of cross-frame information. Furthermore, our study demonstrates the scalability of foundation model-based sparse sensing and shows its leading performance in 3D scene reconstruction. Collectively, our study reveals critical aspects of the promises and the open challenges of integrating foundation models into mobile sparse sensing systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.ET</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.ET</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</title>
    <link href="/2025/11/04/cs.GR/2025-11-04-2511_02483v2/"/>
    <url>/2025/11/04/cs.GR/2025-11-04-2511_02483v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02483v2">http://arxiv.org/abs/2511.02483v2</a><br>We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at <a href="https://vcai.mpi-inf.mpg.de/projects/OLATverse/">https://vcai.mpi-inf.mpg.de/projects/OLATverse/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</title>
    <link href="/2025/11/04/cs.GR/2025-11-04-2511_02580v1/"/>
    <url>/2025/11/04/cs.GR/2025-11-04-2511_02580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02580v1">http://arxiv.org/abs/2511.02580v1</a><br>Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI-Generated Image Detection: An Empirical Study and Future Research Directions</title>
    <link href="/2025/11/04/cs.GT/2025-11-04-2511_02791v1/"/>
    <url>/2025/11/04/cs.GT/2025-11-04-2511_02791v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02791v1">http://arxiv.org/abs/2511.02791v1</a><br>The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HAGI++: Head-Assisted Gaze Imputation and Generation</title>
    <link href="/2025/11/04/cs.HC/2025-11-04-2511_02468v1/"/>
    <url>/2025/11/04/cs.HC/2025-11-04-2511_02468v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02468v1">http://arxiv.org/abs/2511.02468v1</a><br>Mobile eye tracking plays a vital role in capturing human visual attention across both real-world and extended reality (XR) environments, making it an essential tool for applications ranging from behavioural research to human-computer interaction. However, missing values due to blinks, pupil detection errors, or illumination changes pose significant challenges for further gaze data analysis. To address this challenge, we introduce HAGI++ - a multi-modal diffusion-based approach for gaze data imputation that, for the first time, uses the integrated head orientation sensors to exploit the inherent correlation between head and eye movements. HAGI++ employs a transformer-based diffusion model to learn cross-modal dependencies between eye and head representations and can be readily extended to incorporate additional body movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D, and HOT3D datasets demonstrate that HAGI++ consistently outperforms conventional interpolation methods and deep learning-based time-series imputation baselines in gaze imputation. Furthermore, statistical analyses confirm that HAGI++ produces gaze velocity distributions that closely match actual human gaze behaviour, ensuring more realistic gaze imputations. Moreover, by incorporating wrist motion captured from commercial wearable devices, HAGI++ surpasses prior methods that rely on full-body motion capture in the extreme case of 100% missing gaze data (pure gaze generation). Our method paves the way for more complete and accurate eye gaze recordings in real-world settings and has significant potential for enhancing gaze-based analysis and interaction across various application domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration</title>
    <link href="/2025/11/04/cs.HC/2025-11-04-2511_02560v1/"/>
    <url>/2025/11/04/cs.HC/2025-11-04-2511_02560v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02560v1">http://arxiv.org/abs/2511.02560v1</a><br>We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at <a href="https://github.com/microsoft/SigmaCollab">https://github.com/microsoft/SigmaCollab</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Kullback-Leibler divergence method for input-system-state identification</title>
    <link href="/2025/11/04/cs.IT/2025-11-04-2511_02426v1/"/>
    <url>/2025/11/04/cs.IT/2025-11-04-2511_02426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02426v1">http://arxiv.org/abs/2511.02426v1</a><br>The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.IT</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02205v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02205v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02205v1">http://arxiv.org/abs/2511.02205v1</a><br>Multimodal spatiotemporal learning on real-world experimental data is constrained by two challenges: within-modality measurements are sparse, irregular, and noisy (QA&#x2F;QC artifacts) but cross-modally correlated; the set of available modalities varies across space and time, shrinking the usable record unless models can adapt to arbitrary subsets at train and test time. We propose OmniField, a continuity-aware framework that learns a continuous neural field conditioned on available modalities and iteratively fuses cross-modal context. A multimodal crosstalk block architecture paired with iterative cross-modal refinement aligns signals prior to the decoder, enabling unified reconstruction, interpolation, forecasting, and cross-modal prediction without gridding or surrogate preprocessing. Extensive evaluations show that OmniField consistently outperforms eight strong multimodal spatiotemporal baselines. Under heavy simulated sensor noise, performance remains close to clean-input levels, highlighting robustness to corrupted measurements.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02395v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02395v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02395v1">http://arxiv.org/abs/2511.02395v1</a><br>Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning. While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context. Radar sensors overcome this problem with their ability to provide a direct measurement of a point’s Doppler velocity, which can be exploited for single-scan moving object segmentation. However, radar point clouds are often sparse and noisy, making data annotation for use in supervised learning very tedious, time-consuming, and cost-intensive. To overcome this problem, we address the task of self-supervised moving object segmentation of sparse and noisy radar point clouds. We follow a two-step approach of contrastive self-supervised representation learning with subsequent supervised fine-tuning using limited amounts of annotated data. We propose a novel clustering-based contrastive loss function with cluster refinement based on dynamic points removal to pretrain the network to produce motion-aware representations of the radar data. Our method improves label efficiency after fine-tuning, effectively boosting state-of-the-art performance by self-supervised pretraining.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02400v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02400v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02400v1">http://arxiv.org/abs/2511.02400v1</a><br>The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: <a href="https://github.com/Minds-R-Lab/MammoClean">https://github.com/Minds-R-Lab/MammoClean</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02558v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02558v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02558v1">http://arxiv.org/abs/2511.02558v1</a><br>Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer’s disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant’s entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>q-bio.NC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02580v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02580v1">http://arxiv.org/abs/2511.02580v1</a><br>Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02832v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02832v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02832v1">http://arxiv.org/abs/2511.02832v1</a><br>Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at <a href="https://yanjieze.com/TWIST2">https://yanjieze.com/TWIST2</a> . Our collected dataset is also open-sourced at <a href="https://twist-data.github.io/">https://twist-data.github.io</a> .</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02953v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02953v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02953v1">http://arxiv.org/abs/2511.02953v1</a><br>Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model’s ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_02992v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_02992v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02992v1">http://arxiv.org/abs/2511.02992v1</a><br>Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_03053v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_03053v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03053v1">http://arxiv.org/abs/2511.03053v1</a><br>Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds’ uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data-Efficient Realized Volatility Forecasting with Vision Transformers</title>
    <link href="/2025/11/04/cs.LG/2025-11-04-2511_03046v1/"/>
    <url>/2025/11/04/cs.LG/2025-11-04-2511_03046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03046v1">http://arxiv.org/abs/2511.03046v1</a><br>Recent work in financial machine learning has shown the virtue of complexity: the phenomenon by which deep learning methods capable of learning highly nonlinear relationships outperform simpler approaches in financial forecasting. While transformer architectures like Informer have shown promise for financial time series forecasting, the application of transformer models for options data remains largely unexplored. We conduct preliminary studies towards the development of a transformer model for options data by training the Vision Transformer (ViT) architecture, typically used in modern image recognition and classification systems, to predict the realized volatility of an asset over the next 30 days from its implied volatility surface (augmented with date information) for a single day. We show that the ViT can learn seasonal patterns and nonlinear features from the IV surface, suggesting a promising direction for model development.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>I.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization</title>
    <link href="/2025/11/04/cs.NA/2025-11-04-2511_02329v1/"/>
    <url>/2025/11/04/cs.NA/2025-11-04-2511_02329v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02329v1">http://arxiv.org/abs/2511.02329v1</a><br>We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) – originally developed for group synchronization – to camera location estimation. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, showing that cycle consistency alone – without access to inter-camera distances – suffices to achieve the lowest sample complexity currently known. To further enhance robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation synchronization. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.NA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ME</tag>
      
      <tag>90C26, 90C17, 68Q87, 65C20, 90-08, 60-08</tag>
      
      <tag>G.1.6; I.4.0</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization</title>
    <link href="/2025/11/04/cs.RO/2025-11-04-2511_02329v1/"/>
    <url>/2025/11/04/cs.RO/2025-11-04-2511_02329v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02329v1">http://arxiv.org/abs/2511.02329v1</a><br>We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) – originally developed for group synchronization – to camera location estimation. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, showing that cycle consistency alone – without access to inter-camera distances – suffices to achieve the lowest sample complexity currently known. To further enhance robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation synchronization. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ME</tag>
      
      <tag>90C26, 90C17, 68Q87, 65C20, 90-08, 60-08</tag>
      
      <tag>G.1.6; I.4.0</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Synthetic Crop-Weed Image Generation and its Impact on Model Generalization</title>
    <link href="/2025/11/04/cs.RO/2025-11-04-2511_02417v1/"/>
    <url>/2025/11/04/cs.RO/2025-11-04-2511_02417v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02417v1">http://arxiv.org/abs/2511.02417v1</a><br>Precise semantic segmentation of crops and weeds is necessary for agricultural weeding robots. However, training deep learning models requires large annotated datasets, which are costly to obtain in real fields. Synthetic data can reduce this burden, but the gap between simulated and real images remains a challenge. In this paper, we present a pipeline for procedural generation of synthetic crop-weed images using Blender, producing annotated datasets under diverse conditions of plant growth, weed density, lighting, and camera angle. We benchmark several state-of-the-art segmentation models on synthetic and real datasets and analyze their cross-domain generalization. Our results show that training on synthetic images leads to a sim-to-real gap of 10%, surpassing previous state-of-the-art methods. Moreover, synthetic data demonstrates good generalization properties, outperforming real datasets in cross-domain scenarios. These findings highlight the potential of synthetic agricultural datasets and support hybrid strategies for more efficient model training.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</title>
    <link href="/2025/11/04/cs.RO/2025-11-04-2511_02427v1/"/>
    <url>/2025/11/04/cs.RO/2025-11-04-2511_02427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02427v1">http://arxiv.org/abs/2511.02427v1</a><br>Video Understanding, Scene Interpretation and Commonsense Reasoning are highly challenging tasks enabling the interpretation of visual information, allowing agents to perceive, interact with and make rational decisions in its environment. Large Language Models (LLMs) and Visual Language Models (VLMs) have shown remarkable advancements in these areas in recent years, enabling domain-specific applications as well as zero-shot open vocabulary tasks, combining multiple domains. However, the required computational complexity poses challenges for their application on edge devices and in the context of Mobile Robotics, especially considering the trade-off between accuracy and inference time. In this paper, we investigate the capabilities of state-of-the-art VLMs for the task of Scene Interpretation and Action Recognition, with special regard to small VLMs capable of being deployed to edge devices in the context of Mobile Robotics. The proposed pipeline is evaluated on a diverse dataset consisting of various real-world cityscape, on-campus and indoor scenarios. The experimental evaluation discusses the potential of these small models on edge devices, with particular emphasis on challenges, weaknesses, inherent model biases and the application of the gained information. Supplementary material is provided via the following repository: <a href="https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/">https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems</title>
    <link href="/2025/11/04/cs.RO/2025-11-04-2511_02507v1/"/>
    <url>/2025/11/04/cs.RO/2025-11-04-2511_02507v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02507v1">http://arxiv.org/abs/2511.02507v1</a><br>Recent advancements in Deep Learning enable hardware-based cognitive systems, that is, mechatronic systems in general and robotics in particular with integrated Artificial Intelligence, to interact with dynamic and unstructured environments. While the results are impressive, the application of such systems to critical tasks like autonomous driving as well as service and care robotics necessitate the evaluation of large amount of heterogeneous data. Automated report generation for Mobile Robotics can play a crucial role in facilitating the evaluation and acceptance of such systems in various domains. In this paper, we propose a pipeline for generating automated reports in natural language utilizing various multi-modal sensors that solely relies on local models capable of being deployed on edge computing devices, thus preserving the privacy of all actors involved and eliminating the need for external services. In particular, we evaluate our implementation on a diverse dataset spanning multiple domains including indoor, outdoor and urban environments, providing quantitative as well as qualitative evaluation results. Various generated example reports and other supplementary materials are available via a public repository.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</title>
    <link href="/2025/11/04/cs.RO/2025-11-04-2511_02832v1/"/>
    <url>/2025/11/04/cs.RO/2025-11-04-2511_02832v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02832v1">http://arxiv.org/abs/2511.02832v1</a><br>Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at <a href="https://yanjieze.com/TWIST2">https://yanjieze.com/TWIST2</a> . Our collected dataset is also open-sourced at <a href="https://twist-data.github.io/">https://twist-data.github.io</a> .</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation</title>
    <link href="/2025/11/04/cs.RO/2025-11-04-2511_02953v1/"/>
    <url>/2025/11/04/cs.RO/2025-11-04-2511_02953v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02953v1">http://arxiv.org/abs/2511.02953v1</a><br>Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model’s ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data</title>
    <link href="/2025/11/04/cs.RO/2025-11-04-2511_02994v1/"/>
    <url>/2025/11/04/cs.RO/2025-11-04-2511_02994v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02994v1">http://arxiv.org/abs/2511.02994v1</a><br>For developing safe Autonomous Driving Systems (ADS), rigorous testing is required before they are deemed safe for road deployments. Since comprehensive conventional physical testing is impractical due to cost and safety concerns, Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing VTE-generated sensor outputs against their real-world analogues can be a strong indication that the VTE accurately represents reality. Correspondingly, this work explores a comprehensive experimental approach to finding evaluation metrics suitable for comparing real-world and simulated LiDAR scans. The metrics were tested in terms of sensitivity and accuracy with different noise, density, distortion, sensor orientation, and channel settings. From comparing the metrics, we found that Density Aware Chamfer Distance (DCD) works best across all cases. In the second step of the research, a Virtual Testing Environment was generated using real LiDAR scan data. The data was collected in a controlled environment with only static objects using an instrumented vehicle equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans were compared in terms of model perception and geometric similarity. Actual and simulated LiDAR scans have a similar semantic segmentation output with a mIoU of 21% with corrected intensity and an average density aware chamfer distance (DCD) of 0.63. This indicates a slight difference in the geometric properties of simulated and real LiDAR scans and a significant difference between model outputs. During the comparison, density-aware chamfer distance was found to be the most correlated among the metrics with perception methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Kullback-Leibler divergence method for input-system-state identification</title>
    <link href="/2025/11/04/cs.SY/2025-11-04-2511_02426v1/"/>
    <url>/2025/11/04/cs.SY/2025-11-04-2511_02426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02426v1">http://arxiv.org/abs/2511.02426v1</a><br>The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
    <link href="/2025/11/04/cs.SY/2025-11-04-2511_02717v1/"/>
    <url>/2025/11/04/cs.SY/2025-11-04-2511_02717v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02717v1">http://arxiv.org/abs/2511.02717v1</a><br>The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.</p>]]></content>
    
    
    <categories>
      
      <category>cs.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
    <link href="/2025/11/04/eess.AS/2025-11-04-2511_02717v1/"/>
    <url>/2025/11/04/eess.AS/2025-11-04-2511_02717v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02717v1">http://arxiv.org/abs/2511.02717v1</a><br>The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.</p>]]></content>
    
    
    <categories>
      
      <category>eess.AS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Instance Segmentation to 3D Growth Trajectory Reconstruction in Planktonic Foraminifera</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02142v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02142v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02142v1">http://arxiv.org/abs/2511.02142v1</a><br>Planktonic foraminifera, marine protists characterized by their intricate chambered shells, serve as valuable indicators of past and present environmental conditions. Understanding their chamber growth trajectory provides crucial insights into organismal development and ecological adaptation under changing environments. However, automated tracing of chamber growth from imaging data remains largely unexplored, with existing approaches relying heavily on manual segmentation of each chamber, which is time-consuming and subjective. In this study, we propose an end-to-end pipeline that integrates instance segmentation, a computer vision technique not extensively explored in foraminifera, with a dedicated chamber ordering algorithm to automatically reconstruct three-dimensional growth trajectories from high-resolution computed tomography scans. We quantitatively and qualitatively evaluate multiple instance segmentation methods, each optimized for distinct spatial features of the chambers, and examine their downstream influence on growth-order reconstruction accuracy. Experimental results on expert-annotated datasets demonstrate that the proposed pipeline substantially reduces manual effort while maintaining biologically meaningful accuracy. Although segmentation models exhibit under-segmentation in smaller chambers due to reduced voxel fidelity and subtle inter-chamber connectivity, the chamber-ordering algorithm remains robust, achieving consistent reconstruction of developmental trajectories even under partial segmentation. This work provides the first fully automated and reproducible pipeline for digital foraminiferal growth analysis, establishing a foundation for large-scale, data-driven ecological studies.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02144v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02144v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02144v1">http://arxiv.org/abs/2511.02144v1</a><br>Accurate quantification of pavement crack width plays a pivotal role in assessing structural integrity and guiding maintenance interventions. However, achieving precise crack width measurements presents significant challenges due to: (1) the complex, non-uniform morphology of crack boundaries, which limits the efficacy of conventional approaches, and (2) the demand for rapid measurement capabilities from arbitrary pixel locations to facilitate comprehensive pavement condition evaluation. To overcome these limitations, this study introduces a cascaded framework integrating Principal Component Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from digital images. The proposed methodology comprises three sequential stages: (1) initial crack segmentation using established detection algorithms to generate a binary representation, (2) determination of the primary orientation axis for quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations were conducted across three publicly available datasets, demonstrating that the proposed approach achieves superior performance in both computational efficiency and measurement accuracy compared to existing state-of-the-art techniques.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Autobiasing Event Cameras for Flickering Mitigation</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02180v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02180v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02180v1">http://arxiv.org/abs/2511.02180v1</a><br>Understanding and mitigating flicker effects caused by rapid variations in light intensity is critical for enhancing the performance of event cameras in diverse environments. This paper introduces an innovative autonomous mechanism for tuning the biases of event cameras, effectively addressing flicker across a wide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on additional hardware or software for flicker filtering, our approach leverages the event cameras inherent bias settings. Utilizing a simple Convolutional Neural Networks -CNNs, the system identifies instances of flicker in a spatial space and dynamically adjusts specific biases to minimize its impact. The efficacy of this autobiasing system was robustly tested using a face detector framework under both well-lit and low-light conditions, as well as across various frequencies. The results demonstrated significant improvements: enhanced YOLO confidence metrics for face detection, and an increased percentage of frames capturing detected faces. Moreover, the average gradient, which serves as an indicator of flicker presence through edge detection, decreased by 38.2 percent in well-lit conditions and by 53.6 percent in low-light conditions. These findings underscore the potential of our approach to significantly improve the functionality of event cameras in a range of adverse lighting scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02182v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02182v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02182v1">http://arxiv.org/abs/2511.02182v1</a><br>In this technical report, we introduce a framework to address Grounded Video Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The GVQA task demands robust multimodal models capable of complex reasoning over video content, grounding the resulting answers visually, and tracking the referenced objects temporally. To achieve this capability, our proposed approach decomposes the GVQA task into a three-stage pipeline: (1) Video Reasoning &amp; QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key contribution is the introduction of a trigger moment, derived from our proposed CORTEX prompt, which pinpoints the single most visible frame of a target object to serve as a robust anchor for grounding and tracking. To this end, we achieve the HOTA score of 0.4968, which marks a significant improvement over the previous year’s winning score of 0.2704 on GVQA task.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02193v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02193v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02193v1">http://arxiv.org/abs/2511.02193v1</a><br>Accurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 $%$ on DRIVE and 1.25 $%$ on STARE, demonstrating its effectiveness and advancement. The project code is public via <a href="https://github.com/liujiawen-jpg/MM-UNet">https://github.com/liujiawen-jpg/MM-UNet</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02205v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02205v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02205v1">http://arxiv.org/abs/2511.02205v1</a><br>Multimodal spatiotemporal learning on real-world experimental data is constrained by two challenges: within-modality measurements are sparse, irregular, and noisy (QA&#x2F;QC artifacts) but cross-modally correlated; the set of available modalities varies across space and time, shrinking the usable record unless models can adapt to arbitrary subsets at train and test time. We propose OmniField, a continuity-aware framework that learns a continuous neural field conditioned on available modalities and iteratively fuses cross-modal context. A multimodal crosstalk block architecture paired with iterative cross-modal refinement aligns signals prior to the decoder, enabling unified reconstruction, interpolation, forecasting, and cross-modal prediction without gridding or surrogate preprocessing. Extensive evaluations show that OmniField consistently outperforms eight strong multimodal spatiotemporal baselines. Under heavy simulated sensor noise, performance remains close to clean-input levels, highlighting robustness to corrupted measurements.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02206v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02206v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02206v1">http://arxiv.org/abs/2511.02206v1</a><br>Background: Alzheimer’s disease (AD) diagnosis heavily relies on amyloid-beta positron emission tomography (Abeta-PET), which is limited by high cost and limited accessibility. This study explores whether Abeta-PET spatial patterns can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566 participants. A language-enhanced generative model, driven by a large language model (LLM) and multimodal information fusion, was developed to synthesize PET images. Synthesized images were evaluated for image quality, diagnostic consistency, and clinical applicability within a fully automated diagnostic pipeline. Findings: The synthetic PET images closely resemble real PET scans in both structural details (SSIM &#x3D; 0.920 +&#x2F;- 0.003) and regional patterns (Pearson’s r &#x3D; 0.955 +&#x2F;- 0.007). Diagnostic outcomes using synthetic PET show high agreement with real PET-based diagnoses (accuracy &#x3D; 0.80). Using synthetic PET, we developed a fully automatic AD diagnostic pipeline integrating PET synthesis and classification. The synthetic PET-based model (AUC &#x3D; 0.78) outperforms T1-based (AUC &#x3D; 0.68) and BBM-based (AUC &#x3D; 0.73) models, while combining synthetic PET and BBMs further improved performance (AUC &#x3D; 0.79). Ablation analysis supports the advantages of LLM integration and prompt engineering. Interpretation: Our language-enhanced generative model synthesizes realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial pattern assessment and improving the diagnostic workflow for Alzheimer’s disease.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02207v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02207v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02207v1">http://arxiv.org/abs/2511.02207v1</a><br>Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02210v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02210v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02210v1">http://arxiv.org/abs/2511.02210v1</a><br>Segmental longitudinal strain (SLS) of the left ventricle (LV) is an important prognostic indicator for evaluating regional LV dysfunction, in particular for diagnosing and managing myocardial ischemia. Current techniques for strain estimation require significant manual intervention and expertise, limiting their efficiency and making them too resource-intensive for monitoring purposes. This study introduces the first automated pipeline, autoStrain, for SLS estimation in transesophageal echocardiography (TEE) using deep learning (DL) methods for motion estimation. We present a comparative analysis of two DL approaches: TeeFlow, based on the RAFT optical flow model for dense frame-to-frame predictions, and TeeTracker, based on the CoTracker point trajectory model for sparse long-sequence predictions.   As ground truth motion data from real echocardiographic sequences are hardly accessible, we took advantage of a unique simulation pipeline (SIMUS) to generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with ground truth myocardial motion to train and evaluate both models. Our evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.   Clinical validation on 16 patients further demonstrated that SLS estimation with our autoStrain pipeline aligned with clinical references, achieving a mean difference (95% limits of agreement) of 1.09% (-8.90% to 11.09%). Incorporation of simulated ischemia in the synTEE data improved the accuracy of the models in quantifying abnormal deformation. Our findings indicate that integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>High-Resolution Magnetic Particle Imaging System Matrix Recovery Using a Vision Transformer with Residual Feature Network</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02212v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02212v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02212v1">http://arxiv.org/abs/2511.02212v1</a><br>This study presents a hybrid deep learning framework, the Vision Transformer with Residual Feature Network (VRF-Net), for recovering high-resolution system matrices in Magnetic Particle Imaging (MPI). MPI resolution often suffers from downsampling and coil sensitivity variations. VRF-Net addresses these challenges by combining transformer-based global attention with residual convolutional refinement, enabling recovery of both large-scale structures and fine details. To reflect realistic MPI conditions, the system matrix is degraded using a dual-stage downsampling strategy. Training employed paired-image super-resolution on the public Open MPI dataset and a simulated dataset incorporating variable coil sensitivity profiles. For system matrix recovery on the Open MPI dataset, VRF-Net achieved nRMSE &#x3D; 0.403, pSNR &#x3D; 39.08 dB, and SSIM &#x3D; 0.835 at 2x scaling, and maintained strong performance even at challenging scale 8x (pSNR &#x3D; 31.06 dB, SSIM &#x3D; 0.717). For the simulated dataset, VRF-Net achieved nRMSE &#x3D; 4.44, pSNR &#x3D; 28.52 dB, and SSIM &#x3D; 0.771 at 2x scaling, with stable performance at higher scales. On average, it reduced nRMSE by 88.2%, increased pSNR by 44.7%, and improved SSIM by 34.3% over interpolation and CNN-based methods. In image reconstruction of Open MPI phantoms, VRF-Net further reduced reconstruction error to nRMSE &#x3D; 1.79 at 2x scaling, while preserving structural fidelity (pSNR &#x3D; 41.58 dB, SSIM &#x3D; 0.960), outperforming existing methods. These findings demonstrate that VRF-Net enables sharper, artifact-free system matrix recovery and robust image reconstruction across multiple scales, offering a promising direction for future in vivo applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>physics.med-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Can Foundation Models Revolutionize Mobile AR Sparse Sensing?</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02215v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02215v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02215v1">http://arxiv.org/abs/2511.02215v1</a><br>Mobile sensing systems have long faced a fundamental trade-off between sensing quality and efficiency due to constraints in computation, power, and other limitations. Sparse sensing, which aims to acquire and process only a subset of sensor data, has been a key strategy for maintaining performance under such constraints. However, existing sparse sensing methods often suffer from reduced accuracy, as missing information across space and time introduces uncertainty into many sensing systems. In this work, we investigate whether foundation models can change the landscape of mobile sparse sensing. Using real-world mobile AR data, our evaluations demonstrate that foundation models offer significant improvements in geometry-aware image warping, a central technique for enabling accurate reuse of cross-frame information. Furthermore, our study demonstrates the scalability of foundation model-based sparse sensing and shows its leading performance in 3D scene reconstruction. Collectively, our study reveals critical aspects of the promises and the open challenges of integrating foundation models into mobile sparse sensing systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.ET</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer&#39;s Disease Diagnosis</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02228v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02228v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02228v1">http://arxiv.org/abs/2511.02228v1</a><br>Alzheimer’s disease (AD) is the most prevalent form of dementia, and its early diagnosis is essential for slowing disease progression. Recent studies on multimodal neuroimaging fusion using MRI and PET have achieved promising results by integrating multi-scale complementary features. However, most existing approaches primarily emphasize cross-modal complementarity while overlooking the diagnostic importance of modality-specific features. In addition, the inherent distributional differences between modalities often lead to biased and noisy representations, degrading classification performance. To address these challenges, we propose a Collaborative Attention and Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The proposed model introduces a learnable parameter representation (LPR) block to compensate for missing modality information, followed by a shared encoder and modality-independent encoders to preserve both shared and specific representations. Furthermore, a consistency-guided mechanism is employed to explicitly align the latent distributions across modalities. Experimental results on the ADNI dataset demonstrate that our method achieves superior diagnostic performance compared with existing fusion strategies.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02271v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02271v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02271v1">http://arxiv.org/abs/2511.02271v1</a><br>Medical Report Generation (MRG) is a key part of modern medical diagnostics, as it automatically generates reports from radiological images to reduce radiologists’ burden. However, reliable MRG models for lesion description face three main challenges: insufficient domain knowledge understanding, poor text-visual entity embedding alignment, and spurious correlations from cross-modal biases. Previous work only addresses single challenges, while this paper tackles all three via a novel hierarchical task decomposition approach, proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into low-, mid-, and high-level tasks: 1) Low-level: align medical entity features with spatial locations to enhance domain knowledge for visual encoders; 2) Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling (images) to boost cross-modal alignment via mutual guidance; 3) High-level: a cross-modal causal intervention module (via front-door intervention) to reduce confounders and improve interpretability. Extensive experiments confirm HTSC-CIF’s effectiveness, significantly outperforming state-of-the-art (SOTA) MRG methods. Code will be made public upon paper acceptance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02277v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02277v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02277v1">http://arxiv.org/abs/2511.02277v1</a><br>Object pose estimation is a task that is of central importance in 3D Computer Vision. Given a target image and a canonical pose, a single point estimate may very often be sufficient; however, a probabilistic pose output is related to a number of benefits when pose is not unambiguous due to sensor and projection constraints or inherent object symmetries. With this paper, we explore the usefulness of using the well-known Euler angles parameterisation as a basis for a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation, 3D pose has been parameterized in a number of ways, either in or out of the context of parameter estimation. We explore the idea that Euler angles, despite their shortcomings, may lead to useful models in a number of aspects, compared to a model built on a more complex parameterisation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02288v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02288v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02288v1">http://arxiv.org/abs/2511.02288v1</a><br>We propose a Graph Neural Network (GNN)-based approach for Handwritten Mathematical Expression (HME) recognition by modeling HMEs as graphs, where nodes represent symbols and edges capture spatial dependencies. A deep BLSTM network is used for symbol segmentation, recognition, and spatial relation classification, forming an initial primitive graph. A 2D-CFG parser then generates all possible spatial relations, while the GNN-based link prediction model refines the structure by removing unnecessary connections, ultimately forming the Symbol Label Graph. Experimental results demonstrate the effectiveness of our approach, showing promising performance in HME structure recognition.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02280v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02280v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02280v1">http://arxiv.org/abs/2511.02280v1</a><br>We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at <a href="https://github.com/BytedanceDouyinContent/SAIL-RL">https://github.com/BytedanceDouyinContent/SAIL-RL</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02247v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02247v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02247v1">http://arxiv.org/abs/2511.02247v1</a><br>Monocular depth estimation (MDE) is a critical task to guide autonomous medical robots. However, obtaining absolute (metric) depth from an endoscopy camera in surgical scenes is difficult, which limits supervised learning of depth on real endoscopic images. Current image-level unsupervised domain adaptation methods translate synthetic images with known depth maps into the style of real endoscopic frames and train depth networks using these translated images with their corresponding depth maps. However a domain gap often remains between real and translated synthetic images. In this paper, we present a latent feature alignment method to improve absolute depth estimation by reducing this domain gap in the context of endoscopic videos of the central airway. Our methods are agnostic to the image translation process and focus on the depth estimation itself. Specifically, the depth network takes translated synthetic and real endoscopic frames as input and learns latent domain-invariant features via adversarial learning and directional feature consistency. The evaluation is conducted on endoscopic videos of central airway phantoms with manually aligned absolute depth maps. Compared to state-of-the-art MDE methods, our approach achieves superior performance on both absolute and relative depth metrics, and consistently improves results across various backbones and pretrained weights. Our code is available at <a href="https://github.com/MedICL-VU/MDE">https://github.com/MedICL-VU/MDE</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3D Point Cloud Object Detection on Edge Devices for Split Computing</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02293v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02293v1">http://arxiv.org/abs/2511.02293v1</a><br>The field of autonomous driving technology is rapidly advancing, with deep learning being a key component. Particularly in the field of sensing, 3D point cloud data collected by LiDAR is utilized to run deep neural network models for 3D object detection. However, these state-of-the-art models are complex, leading to longer processing times and increased power consumption on edge devices. The objective of this study is to address these issues by leveraging Split Computing, a distributed machine learning inference method. Split Computing aims to lessen the computational burden on edge devices, thereby reducing processing time and power consumption. Furthermore, it minimizes the risk of data breaches by only transmitting intermediate data from the deep neural network model. Experimental results show that splitting after voxelization reduces the inference time by 70.8% and the edge device execution time by 90.0%. When splitting within the network, the inference time is reduced by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
      <tag>C.2.4; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAFD-CC: Global-Aware Feature Decoupling with Confidence Calibration for OOD Detection</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02335v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02335v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02335v1">http://arxiv.org/abs/2511.02335v1</a><br>Out-of-distribution (OOD) detection is paramount to ensuring the reliability and robustness of learning models in real-world applications. Existing post-hoc OOD detection methods detect OOD samples by leveraging their features and logits information without retraining. However, they often overlook the inherent correlation between features and logits, which is crucial for effective OOD detection. To address this limitation, we propose Global-Aware Feature Decoupling with Confidence Calibration (GAFD-CC). GAFD-CC aims to refine decision boundaries and increase discriminative performance. Firstly, it performs global-aware feature decoupling guided by classification weights. This involves aligning features with the direction of global classification weights to decouple them. From this, GAFD-CC extracts two types of critical information: positively correlated features that promote in-distribution (ID)&#x2F;OOD boundary refinement and negatively correlated features that suppress false positives and tighten these boundaries. Secondly, it adaptively fuses these decoupled features with multi-scale logit-based confidence for comprehensive and robust OOD detection. Extensive experiments on large-scale benchmarks demonstrate GAFD-CC’s competitive performance and strong generalization ability compared to those of state-of-the-art methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02329v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02329v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02329v1">http://arxiv.org/abs/2511.02329v1</a><br>We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) – originally developed for group synchronization – to camera location estimation. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, showing that cycle consistency alone – without access to inter-camera distances – suffices to achieve the lowest sample complexity currently known. To further enhance robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation synchronization. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ME</tag>
      
      <tag>90C26, 90C17, 68Q87, 65C20, 90-08, 60-08</tag>
      
      <tag>G.1.6; I.4.0</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02349v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02349v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02349v1">http://arxiv.org/abs/2511.02349v1</a><br>Portable physiological monitoring is essential for early detection and management of cardiovascular disease, but current methods often require specialized equipment that limits accessibility or impose impractical postures that patients cannot maintain. Video-based photoplethysmography on smartphones offers a convenient noninvasive alternative, yet it still faces reliability challenges caused by motion artifacts, lighting variations, and single-view constraints. Few studies have demonstrated reliable application to cardiovascular patients, and no widely used open datasets exist for cross-device accuracy. To address these limitations, we introduce the M3PD dataset, the first publicly available dual-view mobile photoplethysmography dataset, comprising synchronized facial and fingertip videos captured simultaneously via front and rear smartphone cameras from 60 participants (including 47 cardiovascular patients). Building on this dual-view setting, we further propose F3Mamba, which fuses the facial and fingertip views through Mamba-based temporal modeling. The model reduces heart-rate error by 21.9 to 30.2 percent over existing single-view baselines while improving robustness in challenging real-world scenarios. Data and code: <a href="https://github.com/Health-HCI-Group/F3Mamba">https://github.com/Health-HCI-Group/F3Mamba</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02360v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02360v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02360v1">http://arxiv.org/abs/2511.02360v1</a><br>In human cognition, there exist numerous thought processes that are tacit and beyond verbal expression, enabling us to understand and interact with the world in multiple ways. However, contemporary Vision-Language Models (VLMs) remain constrained to reasoning within the discrete and rigid space of linguistic tokens, thereby bottlenecking the rich, high-dimensional nature of visual perception. To bridge this gap, we propose CoCoVa (Chain of Continuous Vision-Language Thought), a novel framework for vision-language model that leverages continuous cross-modal reasoning for diverse vision-language tasks. The core of CoCoVa is an iterative reasoning cycle, where a novel Latent Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a chain of latent thought vectors through cross-modal fusion. To focus this process, a token selection mechanism dynamically identifies salient visual regions, mimicking attentional focus. To ensure these latent thoughts remain grounded, we train the model with a multi-task objective that combines contrastive learning and diffusion-based reconstruction, enforcing alignment between latent representations and both visual and textual modalities. Evaluations show CoCoVa improves accuracy and token efficiency over strong baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B models on almost all benchmarks. When scaled to 7B LLM backbones, it remains competitive with state-of-the-art models. Qualitative analysis validates that learned latent space captures interpretable and structured reasoning patterns, highlighting the potential of CoCoVa to bridge the representational gap between discrete language processing and the continuous nature of visual understanding.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02384v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02384v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02384v1">http://arxiv.org/abs/2511.02384v1</a><br>Large-scale chemical reaction datasets are crucial for AI research in chemistry. However, existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models. In response to this challenge, we propose the RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP). Our framework reformulates the traditional coordinate prediction driven parsing process into an image captioning problem, which Large Vision-Language Models (LVLMs) handle naturally. We introduce a strategy termed “BBox and Index as Visual Prompt” (BIVP), which uses our state-of-the-art molecular detector, MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the input image. This turns the downstream parsing into a natural-language description problem. Extensive experiments show that the BIVP strategy significantly improves structural extraction quality while simplifying model design. We further construct the RxnCaption-11k dataset, an order of magnitude larger than prior real-world literature benchmarks, with a balanced test subset across four layout archetypes. Experiments demonstrate that RxnCaption-VL achieves state-of-the-art performance on multiple metrics. We believe our method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry. We will release data, models, and code on GitHub.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02395v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02395v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02395v1">http://arxiv.org/abs/2511.02395v1</a><br>Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning. While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context. Radar sensors overcome this problem with their ability to provide a direct measurement of a point’s Doppler velocity, which can be exploited for single-scan moving object segmentation. However, radar point clouds are often sparse and noisy, making data annotation for use in supervised learning very tedious, time-consuming, and cost-intensive. To overcome this problem, we address the task of self-supervised moving object segmentation of sparse and noisy radar point clouds. We follow a two-step approach of contrastive self-supervised representation learning with subsequent supervised fine-tuning using limited amounts of annotated data. We propose a novel clustering-based contrastive loss function with cluster refinement based on dynamic points removal to pretrain the network to produce motion-aware representations of the radar data. Our method improves label efficiency after fine-tuning, effectively boosting state-of-the-art performance by self-supervised pretraining.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02397v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02397v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02397v1">http://arxiv.org/abs/2511.02397v1</a><br>Color consistency correction for color point clouds is a fundamental yet important task in 3D rendering and compression applications. In the past, most previous color correction methods aimed at correcting color for color images. The purpose of this paper is to propose a grouping-based hybrid color correction algorithm for color point clouds. Our algorithm begins by estimating the overlapping rate between the aligned source and target point clouds, and then adaptively partitions the target points into two groups, namely the close proximity group Gcl and the moderate proximity group Gmod, or three groups, namely Gcl, Gmod, and the distant proximity group Gdist, when the estimated overlapping rate is low or high, respectively. To correct color for target points in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) method is proposed. To correct color for target points in Gmod, a joint KBI and the histogram equalization (JKHE) method is proposed. For target points in Gdist, a histogram equalization (HE) method is proposed for color correction. Finally, we discuss the grouping-effect free property and the ablation study in our algorithm. The desired color consistency correction benefit of our algorithm has been justified through 1086 testing color point cloud pairs against the state-of-the-art methods. The C++ source code of our algorithm can be accessed from the website: <a href="https://github.com/ivpml84079/Point-cloud-color-correction">https://github.com/ivpml84079/Point-cloud-color-correction</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02400v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02400v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02400v1">http://arxiv.org/abs/2511.02400v1</a><br>The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: <a href="https://github.com/Minds-R-Lab/MammoClean">https://github.com/Minds-R-Lab/MammoClean</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02404v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02404v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02404v1">http://arxiv.org/abs/2511.02404v1</a><br>Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic cats) have vertically elongated pupils linked to ambush predation; yet, how such specializations manifest in downstream visual representations remains incompletely understood. We present a unified, frozen-encoder benchmark that quantifies feline-human cross-species representational alignment in the wild, across convolutional networks, supervised Vision Transformers, windowed transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel Alignment (linear and RBF) and Representational Similarity Analysis, with additional distributional and stability tests reported in the paper. Across models, DINO ViT-B&#x2F;16 attains the most substantial alignment (mean CKA-RBF $\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$), peaking at early blocks, indicating that token-level self-supervision induces early-stage features that bridge species-specific statistics. Supervised ViTs are competitive on CKA yet show weaker geometric correspondence than DINO (e.g., ViT-B&#x2F;16 RSA $\approx0.53$ at block8; ViT-L&#x2F;16 $\approx0.47$ at block14), revealing depth-dependent divergences between similarity and representational geometry. CNNs remain strong baselines but below plain ViTs on alignment, and windowed transformers underperform plain ViTs, implicating architectural inductive biases in cross-species alignment. Results indicate that self-supervision coupled with ViT inductive biases yields representational geometries that more closely align feline and human visual systems than widely used CNNs and windowed Transformers, providing testable neuroscientific hypotheses about where and how cross-species visual computations converge. We release our code and dataset for reference and reproducibility.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IllumFlow: Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex Decomposition</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02411v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02411v1">http://arxiv.org/abs/2511.02411v1</a><br>We present IllumFlow, a novel framework that synergizes conditional Rectified Flow (CRF) with Retinex theory for low-light image enhancement (LLIE). Our model addresses low-light enhancement through separate optimization of illumination and reflectance components, effectively handling both lighting variations and noise. Specifically, we first decompose an input image into reflectance and illumination components following Retinex theory. To model the wide dynamic range of illumination variations in low-light images, we propose a conditional rectified flow framework that represents illumination changes as a continuous flow field. While complex noise primarily resides in the reflectance component, we introduce a denoising network, enhanced by flow-derived data augmentation, to remove reflectance noise and chromatic aberration while preserving color fidelity. IllumFlow enables precise illumination adaptation across lighting conditions while naturally supporting customizable brightness enhancement. Extensive experiments on low-light enhancement and exposure correction demonstrate superior quantitative and qualitative performance over existing methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02415v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02415v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02415v1">http://arxiv.org/abs/2511.02415v1</a><br>Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&amp;A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Synthetic Crop-Weed Image Generation and its Impact on Model Generalization</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02417v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02417v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02417v1">http://arxiv.org/abs/2511.02417v1</a><br>Precise semantic segmentation of crops and weeds is necessary for agricultural weeding robots. However, training deep learning models requires large annotated datasets, which are costly to obtain in real fields. Synthetic data can reduce this burden, but the gap between simulated and real images remains a challenge. In this paper, we present a pipeline for procedural generation of synthetic crop-weed images using Blender, producing annotated datasets under diverse conditions of plant growth, weed density, lighting, and camera angle. We benchmark several state-of-the-art segmentation models on synthetic and real datasets and analyze their cross-domain generalization. Our results show that training on synthetic images leads to a sim-to-real gap of 10%, surpassing previous state-of-the-art methods. Moreover, synthetic data demonstrates good generalization properties, outperforming real datasets in cross-domain scenarios. These findings highlight the potential of synthetic agricultural datasets and support hybrid strategies for more efficient model training.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Kullback-Leibler divergence method for input-system-state identification</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02426v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02426v1">http://arxiv.org/abs/2511.02426v1</a><br>The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02427v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02427v1">http://arxiv.org/abs/2511.02427v1</a><br>Video Understanding, Scene Interpretation and Commonsense Reasoning are highly challenging tasks enabling the interpretation of visual information, allowing agents to perceive, interact with and make rational decisions in its environment. Large Language Models (LLMs) and Visual Language Models (VLMs) have shown remarkable advancements in these areas in recent years, enabling domain-specific applications as well as zero-shot open vocabulary tasks, combining multiple domains. However, the required computational complexity poses challenges for their application on edge devices and in the context of Mobile Robotics, especially considering the trade-off between accuracy and inference time. In this paper, we investigate the capabilities of state-of-the-art VLMs for the task of Scene Interpretation and Action Recognition, with special regard to small VLMs capable of being deployed to edge devices in the context of Mobile Robotics. The proposed pipeline is evaluated on a diverse dataset consisting of various real-world cityscape, on-campus and indoor scenarios. The experimental evaluation discusses the potential of these small models on edge devices, with particular emphasis on challenges, weaknesses, inherent model biases and the application of the gained information. Supplementary material is provided via the following repository: <a href="https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/">https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02462v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02462v1">http://arxiv.org/abs/2511.02462v1</a><br>Satellite image inpainting is a crucial task in remote sensing, where accurately restoring missing or occluded regions is essential for robust image analysis. In this paper, we propose KAO, a novel framework that utilizes Kernel-Adaptive Optimization within diffusion models for satellite image inpainting. KAO is specifically designed to address the challenges posed by very high-resolution (VHR) satellite datasets, such as DeepGlobe and the Massachusetts Roads Dataset. Unlike existing methods that rely on preconditioned models requiring extensive retraining or postconditioned models with significant computational overhead, KAO introduces a Latent Space Conditioning approach, optimizing a compact latent space to achieve efficient and accurate inpainting. Furthermore, we incorporate Explicit Propagation into the diffusion process, facilitating forward-backward fusion, which improves the stability and precision of the method. Experimental results demonstrate that KAO sets a new benchmark for VHR satellite image restoration, providing a scalable, high-performance solution that balances the efficiency of preconditioned models with the flexibility of postconditioned models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HAGI++: Head-Assisted Gaze Imputation and Generation</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02468v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02468v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02468v1">http://arxiv.org/abs/2511.02468v1</a><br>Mobile eye tracking plays a vital role in capturing human visual attention across both real-world and extended reality (XR) environments, making it an essential tool for applications ranging from behavioural research to human-computer interaction. However, missing values due to blinks, pupil detection errors, or illumination changes pose significant challenges for further gaze data analysis. To address this challenge, we introduce HAGI++ - a multi-modal diffusion-based approach for gaze data imputation that, for the first time, uses the integrated head orientation sensors to exploit the inherent correlation between head and eye movements. HAGI++ employs a transformer-based diffusion model to learn cross-modal dependencies between eye and head representations and can be readily extended to incorporate additional body movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D, and HOT3D datasets demonstrate that HAGI++ consistently outperforms conventional interpolation methods and deep learning-based time-series imputation baselines in gaze imputation. Furthermore, statistical analyses confirm that HAGI++ produces gaze velocity distributions that closely match actual human gaze behaviour, ensuring more realistic gaze imputations. Moreover, by incorporating wrist motion captured from commercial wearable devices, HAGI++ surpasses prior methods that rely on full-body motion capture in the extreme case of 100% missing gaze data (pure gaze generation). Our method paves the way for more complete and accurate eye gaze recordings in real-world settings and has significant potential for enhancing gaze-based analysis and interaction across various application domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02473v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02473v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02473v1">http://arxiv.org/abs/2511.02473v1</a><br>Multi-view action recognition aims to recognize human actions using multiple camera views and deals with occlusion caused by obstacles or crowds. In this task, cooperation among views, which generates a joint representation by combining multiple views, is vital. Previous studies have explored promising cooperation methods for improving performance. However, since their methods focus only on the task setting of recognizing a single action from an entire video, they are not applicable to the recently popular spatio-temporal action recognition~(STAR) setting, in which each person’s action is recognized sequentially. To address this problem, this paper proposes a multi-view action recognition method for the STAR setting, called MVAFormer. In MVAFormer, we introduce a novel transformer-based cooperation module among views. In contrast to previous studies, which utilize embedding vectors with lost spatial information, our module utilizes the feature map for effective cooperation in the STAR setting, which preserves the spatial information. Furthermore, in our module, we divide the self-attention for the same and different views to model the relationship between multiple views effectively. The results of experiments using a newly collected dataset demonstrate that MVAFormer outperforms the comparison baselines by approximately $4.4$ points on the F-measure.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02483v2/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02483v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02483v2">http://arxiv.org/abs/2511.02483v2</a><br>We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at <a href="https://vcai.mpi-inf.mpg.de/projects/OLATverse/">https://vcai.mpi-inf.mpg.de/projects/OLATverse/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02489v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02489v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02489v1">http://arxiv.org/abs/2511.02489v1</a><br>With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems. However, in GNSS-denied areas, satellite-based localization methods are prone to failure. This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching. In typical pipelines, UAV visual localization is formulated as an image-retrieval problem: features are extracted to build a localization map, and the pose of a query image is estimated by matching it to a reference database with known poses. Because publicly available UAV localization datasets are limited, many approaches recast localization as a classification task and rely on scene labels in these datasets to ensure accuracy. Other methods seek to reduce cross-domain differences using polar-coordinate reprojection, perspective transformations, or generative adversarial networks; however, they can suffer from misalignment, content loss, and limited realism. In contrast, we leverage modern object detection to accurately extract salient instances from UAV and satellite images, and integrate a graph neural network to reason about inter-image and intra-image node relationships. Using a fine-grained, graph-based node-similarity metric, our method achieves strong retrieval and localization performance. Extensive experiments on public and real-world datasets show that our approach handles heterogeneous appearance differences effectively and generalizes well, making it applicable to scenarios with larger modality gaps, such as infrared-visible image matching. Our dataset will be publicly available at the following URL: <a href="https://github.com/liutao23/ODGNNLoc.git">https://github.com/liutao23/ODGNNLoc.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02495v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02495v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02495v1">http://arxiv.org/abs/2511.02495v1</a><br>Recent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at <a href="https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890">https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02505v2/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02505v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02505v2">http://arxiv.org/abs/2511.02505v2</a><br>Shot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator’s unique artistic expression in shot assembly. To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: <a href="https://sobeymil.github.io/esa.com">https://sobeymil.github.io/esa.com</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02507v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02507v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02507v1">http://arxiv.org/abs/2511.02507v1</a><br>Recent advancements in Deep Learning enable hardware-based cognitive systems, that is, mechatronic systems in general and robotics in particular with integrated Artificial Intelligence, to interact with dynamic and unstructured environments. While the results are impressive, the application of such systems to critical tasks like autonomous driving as well as service and care robotics necessitate the evaluation of large amount of heterogeneous data. Automated report generation for Mobile Robotics can play a crucial role in facilitating the evaluation and acceptance of such systems in various domains. In this paper, we propose a pipeline for generating automated reports in natural language utilizing various multi-modal sensors that solely relies on local models capable of being deployed on edge computing devices, thus preserving the privacy of all actors involved and eliminating the need for external services. In particular, we evaluate our implementation on a diverse dataset spanning multiple domains including indoor, outdoor and urban environments, providing quantitative as well as qualitative evaluation results. Various generated example reports and other supplementary materials are available via a public repository.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02510v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02510v1">http://arxiv.org/abs/2511.02510v1</a><br>Sparse-voxel rasterization is a fast, differentiable alternative for optimization-based scene reconstruction, but it tends to underfit low-frequency content, depends on brittle pruning heuristics, and can overgrow in ways that inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that makes SV rasterization both steadier and lighter. Our loss is made low-frequency aware via an inverse-Sobel reweighting with a mid-training gamma-ramp, shifting gradient budget to flat regions only after geometry stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning logic on maximum blending weight, stabilized by EMA-hysteresis guards and refines structure through ray-footprint-based, priority-driven subdivision under an explicit growth budget. Ablations and full-system results across Mip-NeRF 360 (6scenes) and Tanks &amp; Temples (3scenes) datasets show mitigation of errors in low-frequency regions and boundary instability while keeping PSNR&#x2F;SSIM, training time, and FPS comparable to a strong SVRaster pipeline. Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency detail that prior setups miss, enabling more predictable, memory-efficient training without sacrificing perceptual quality.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02541v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02541v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02541v1">http://arxiv.org/abs/2511.02541v1</a><br>Shearography is a non-destructive testing method for detecting subsurface defects, offering high sensitivity and full-field inspection capabilities. However, its industrial adoption remains limited due to the need for expert interpretation. To reduce reliance on labeled data and manual evaluation, this study explores unsupervised learning methods for automated anomaly detection in shearographic images. Three architectures are evaluated: a fully connected autoencoder, a convolutional autoencoder, and a student-teacher feature matching model. All models are trained solely on defect-free data. A controlled dataset was developed using a custom specimen with reproducible defect patterns, enabling systematic acquisition of shearographic measurements under both ideal and realistic deformation conditions. Two training subsets were defined: one containing only undistorted, defect-free samples, and one additionally including globally deformed, yet defect-free, data. The latter simulates practical inspection conditions by incorporating deformation-induced fringe patterns that may obscure localized anomalies. The models are evaluated in terms of binary classification and, for the student-teacher model, spatial defect localization. Results show that the student-teacher approach achieves superior classification robustness and enables precise localization. Compared to the autoencoder-based models, it demonstrates improved separability of feature representations, as visualized through t-SNE embeddings. Additionally, a YOLOv8 model trained on labeled defect data serves as a reference to benchmark localization quality. This study underscores the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02558v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02558v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02558v1">http://arxiv.org/abs/2511.02558v1</a><br>Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer’s disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant’s entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>q-bio.NC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02503v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02503v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02503v1">http://arxiv.org/abs/2511.02503v1</a><br>The automation of workflows in advanced microscopy is a key goal where foundation models like Language Models (LLMs) and Vision-Language Models (VLMs) show great potential. However, adapting these general-purpose models for specialized scientific tasks is critical, and the optimal domain adaptation strategy is often unclear. To address this, we introduce PtychoBench, a new multi-modal, multi-task benchmark for ptychographic analysis. Using this benchmark, we systematically compare two specialization strategies: Supervised Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies on a visual artifact detection task with VLMs and a textual parameter recommendation task with LLMs in a data-scarce regime. Our findings reveal that the optimal specialization pathway is task-dependent. For the visual task, SFT and ICL are highly complementary, with a fine-tuned model guided by context-aware examples achieving the highest mean performance (Micro-F1 of 0.728). Conversely, for the textual task, ICL on a large base model is the superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a powerful “super-expert” SFT model (0-shot Micro-F1 of 0.839). We also confirm the superiority of context-aware prompting and identify a consistent contextual interference phenomenon in fine-tuned models. These results, benchmarked against strong baselines including GPT-4o and a DINOv3-based classifier, offer key observations for AI in science: the optimal specialization path in our benchmark is dependent on the task modality, offering a clear framework for developing more effective science-based agentic systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02560v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02560v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02560v1">http://arxiv.org/abs/2511.02560v1</a><br>We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at <a href="https://github.com/microsoft/SigmaCollab">https://github.com/microsoft/SigmaCollab</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02563v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02563v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02563v1">http://arxiv.org/abs/2511.02563v1</a><br>This report describes the UVH-26 dataset, the first public release by AIM@IISc of a large-scale dataset of annotated traffic-camera images from India. The dataset comprises 26,646 high-resolution (1080p) images sampled from 2800 Bengaluru’s Safe-City CCTV cameras over a 4-week period, and subsequently annotated through a crowdsourced hackathon involving 565 college students from across India. In total, 1.8 million bounding boxes were labeled across 14 vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler (Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller, Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k consensus ground truth bounding boxes and labels were derived for distinct objects in the 26k images using Majority Voting and STAPLE algorithms. Further, we train multiple contemporary detectors, including YOLO11-S&#x2F;X, RT-DETR-S&#x2F;X, and DAMO-YOLO-T&#x2F;L using these datasets, and report accuracy based on mAP50, mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in mAP50:95 over equivalent baseline models trained on COCO dataset, with RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40 for COCO-trained weights for common classes (Car, Bus, and Truck). This demonstrates the benefits of domain-specific training data for Indian traffic scenarios. The release package provides the 26k images with consensus annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the 6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the heterogeneity of Indian urban mobility directly from operational traffic-camera streams, UVH-26 addresses a critical gap in existing global benchmarks, and offers a foundation for advancing detection, classification, and deployment of intelligent transportation systems in emerging nations with complex traffic conditions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02564v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02564v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02564v1">http://arxiv.org/abs/2511.02564v1</a><br>Video-based person re-identification (ReID) in cross-view domains (for example, aerial-ground surveillance) remains an open problem because of extreme viewpoint shifts, scale disparities, and temporal inconsistencies. To address these challenges, we propose MTF-CVReID, a parameter-efficient framework that introduces seven complementary modules over a ViT-B&#x2F;16 backbone. Specifically, we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment (IVFA) for perspective-invariant representation alignment; (6) Hierarchical Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities; and (7) Multi-View Identity Consistency Learning (MVICL) that enforces cross-view identity coherence using a contrastive learning paradigm. Despite adding only about 2 million parameters and 0.7 GFLOPs over the baseline, MTF-CVReID maintains real-time efficiency (189 FPS) and achieves state-of-the-art performance on the AG-VPReID benchmark across all altitude levels, with strong cross-dataset generalization to G2A-VReID and MARS datasets. These results show that carefully designed adapter-based modules can substantially enhance cross-view robustness and temporal consistency without compromising computational efficiency. The source code is available at <a href="https://github.com/MdRashidunnabi/MTF-CVReID">https://github.com/MdRashidunnabi/MTF-CVReID</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02565v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02565v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02565v1">http://arxiv.org/abs/2511.02565v1</a><br>Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02576v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02576v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02576v1">http://arxiv.org/abs/2511.02576v1</a><br>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over&#x2F;under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: <a href="https://gitlab.inria.fr/adelangl/SCORE">https://gitlab.inria.fr/adelangl/SCORE</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02580v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02580v1">http://arxiv.org/abs/2511.02580v1</a><br>Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zero-Shot Multi-Animal Tracking in the Wild</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02591v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02591v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02591v1">http://arxiv.org/abs/2511.02591v1</a><br>Multi-animal tracking is crucial for understanding animal ecology and behavior. However, it remains a challenging task due to variations in habitat, motion patterns, and species appearance. Traditional approaches typically require extensive model fine-tuning and heuristic design for each application scenario. In this work, we explore the potential of recent vision foundation models for zero-shot multi-animal tracking. By combining a Grounding Dino object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully designed heuristics, we develop a tracking framework that can be applied to new datasets without any retraining or hyperparameter adaptation. Evaluations on ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate strong and consistent performance across diverse species and environments. The code is available at <a href="https://github.com/ecker-lab/SAM2-Animal-Tracking">https://github.com/ecker-lab/SAM2-Animal-Tracking</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniChange: Unifying Change Detection with Multimodal Large Language Model</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02607v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02607v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02607v1">http://arxiv.org/abs/2511.02607v1</a><br>Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at <a href="https://github.com/Erxucomeon/UniChange">https://github.com/Erxucomeon/UniChange</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Robust Face Liveness Detection for Biometric Authentication using Single Image</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02645v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02645v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02645v1">http://arxiv.org/abs/2511.02645v1</a><br>Biometric technologies are widely adopted in security, legal, and financial systems. Face recognition can authenticate a person based on the unique facial features such as shape and texture. However, recent works have demonstrated the vulnerability of Face Recognition Systems (FRS) towards presentation attacks. Using spoofing (aka.,presentation attacks), a malicious actor can get illegitimate access to secure systems. This paper proposes a novel light-weight CNN framework to identify print&#x2F;display, video and wrap attacks. The proposed robust architecture provides seamless liveness detection ensuring faster biometric authentication (1-2 seconds on CPU). Further, this also presents a newly created 2D spoof attack dataset consisting of more than 500 videos collected from 60 subjects. To validate the effectiveness of this architecture, we provide a demonstration video depicting print&#x2F;display, video and wrap attack detection approaches. The demo can be viewed in the following link: <a href="https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa">https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02650v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02650v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02650v1">http://arxiv.org/abs/2511.02650v1</a><br>Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Differentiable Hierarchical Visual Tokenization</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02652v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02652v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02652v1">http://arxiv.org/abs/2511.02652v1</a><br>Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T45</tag>
      
      <tag>I.2.10; I.4.10; I.4.6; I.3.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02685v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02685v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02685v1">http://arxiv.org/abs/2511.02685v1</a><br>Visible-infrared person re-identification (VI-ReID) technique could associate the pedestrian images across visible and infrared modalities in the practical scenarios of background illumination changes. However, a substantial gap inherently exists between these two modalities. Besides, existing methods primarily rely on intermediate representations to align cross-modal features of the same person. The intermediate feature representations are usually create by generating intermediate images (kind of data enhancement), or fusing intermediate features (more parameters, lack of interpretability), and they do not make good use of the intermediate features. Thus, we propose a novel VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a middle generated image as a transmitter from visible to infrared modals, which are fully aligned with the original visible images and similar to the infrared modality. After that, using a modality-transition contrastive loss and a modality-query regularization loss for training, which could align the cross-modal features more effectively. Notably, our proposed framework does not need any additional parameters, which achieves the same inference speed to the backbone while improving its performance on VI-ReID task. Extensive experimental results illustrate that our model significantly and consistently outperforms existing SOTAs on three typical VI-ReID datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02712v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02712v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02712v1">http://arxiv.org/abs/2511.02712v1</a><br>Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02717v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02717v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02717v1">http://arxiv.org/abs/2511.02717v1</a><br>The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamic Reflections: Probing Video Representations with Text Alignment</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02767v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02767v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02767v1">http://arxiv.org/abs/2511.02767v1</a><br>The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at <a href="https://video-prh.github.io/">https://video-prh.github.io/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02777v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02777v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02777v1">http://arxiv.org/abs/2511.02777v1</a><br>We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.   Project Page: <a href="https://antoniooroz.github.io/PercHead">https://antoniooroz.github.io/PercHead</a> Video: <a href="https://www.youtube.com/watch?v=4hFybgTk4kE">https://www.youtube.com/watch?v=4hFybgTk4kE</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02778v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02778v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02778v1">http://arxiv.org/abs/2511.02778v1</a><br>Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model’s intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at <a href="https://github.com/CSU-JPG/VCode">https://github.com/CSU-JPG/VCode</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02779v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02779v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02779v1">http://arxiv.org/abs/2511.02779v1</a><br>We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through “drawing to think”. To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLEXICORP: End-user Explainability of Convolutional Neural Networks</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02720v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02720v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02720v1">http://arxiv.org/abs/2511.02720v1</a><br>Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.   To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.   We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI-Generated Image Detection: An Empirical Study and Future Research Directions</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02791v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02791v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02791v1">http://arxiv.org/abs/2511.02791v1</a><br>The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PLUTO-4: Frontier Pathology Foundation Models</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02826v2/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02826v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02826v2">http://arxiv.org/abs/2511.02826v2</a><br>Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4’s potential to transform real-world applications as a backbone for translational research and diagnostic use cases.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02830v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02830v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02830v1">http://arxiv.org/abs/2511.02830v1</a><br>We propose DenseMarks - a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube. In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of-the-art point tracker over a collection of diverse in-the-wild talking heads videos, and guide the mapping via a contrastive loss, encouraging matched points to have close embeddings. We further employ multi-task learning with face landmarks and segmentation constraints, as well as imposing spatial continuity of embeddings through latent cube features, which results in an interpretable and queryable canonical space. The representation can be used for finding common semantic parts, face&#x2F;head tracking, and stereo reconstruction. Due to the strong supervision, our method is robust to pose variations and covers the entire head, including hair. Additionally, the canonical space bottleneck makes sure the obtained representations are consistent across diverse poses and individuals. We demonstrate state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models. The code and the model checkpoint will be made available to the public.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02832v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02832v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02832v1">http://arxiv.org/abs/2511.02832v1</a><br>Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at <a href="https://yanjieze.com/TWIST2">https://yanjieze.com/TWIST2</a> . Our collected dataset is also open-sourced at <a href="https://twist-data.github.io/">https://twist-data.github.io</a> .</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NEF-NET+: Adapting Electrocardio panorama in the wild</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02880v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02880v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02880v1">http://arxiv.org/abs/2511.02880v1</a><br>Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and compensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Optimizing the nnU-Net model for brain tumor (Glioma) segmentation Using a BraTS Sub-Saharan Africa (SSA) dataset</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02893v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02893v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02893v1">http://arxiv.org/abs/2511.02893v1</a><br>Medical image segmentation is a critical achievement in modern medical science, developed over decades of research. It allows for the exact delineation of anatomical and pathological features in two- or three-dimensional pictures by utilizing notions like pixel intensity, texture, and anatomical context. With the advent of automated segmentation, physicians and radiologists may now concentrate on diagnosis and treatment planning while intelligent computers perform routine image processing tasks.   This study used the BraTS Sub-Saharan Africa dataset, a selected subset of the BraTS dataset that included 60 multimodal MRI cases from patients with glioma. Surprisingly, the nnU Net model trained on the initial 60 instances performed better than the network trained on an offline-augmented dataset of 360 cases. Hypothetically, the offline augmentations introduced artificial anatomical variances or intensity distributions, reducing generalization. In contrast, the original dataset, when paired with nnU Net’s robust online augmentation procedures, maintained realistic variability and produced better results. The study achieved a Dice score of 0.84 for whole tumor segmentation. These findings highlight the significance of data quality and proper augmentation approaches in constructing accurate, generalizable medical picture segmentation models, particularly for under-represented locations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cropland Mapping using Geospatial Embeddings</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02923v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02923v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02923v1">http://arxiv.org/abs/2511.02923v1</a><br>Accurate and up-to-date land cover maps are essential for understanding land use change, a key driver of climate change. Geospatial embeddings offer a more efficient and accessible way to map landscape features, yet their use in real-world mapping applications remains underexplored. In this work, we evaluated the utility of geospatial embeddings for cropland mapping in Togo. We produced cropland maps using embeddings from Presto and AlphaEarth. Our findings show that geospatial embeddings can simplify workflows, achieve high-accuracy cropland classification and ultimately support better assessments of land use change and its climate impacts.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02928v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02928v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02928v1">http://arxiv.org/abs/2511.02928v1</a><br>Glioma segmentation is critical for diagnosis and treatment planning, yet remains challenging in Sub-Saharan Africa due to limited MRI infrastructure and heterogeneous acquisition protocols that induce severe domain shift. We propose SegFormer3D-plus, a radiomics-guided transformer architecture designed for robust segmentation under domain variability. Our method combines: (1) histogram matching for intensity harmonization across scanners, (2) radiomic feature extraction with PCA-reduced k-means for domain-aware stratified sampling, (3) a dual-pathway encoder with frequency-aware feature extraction and spatial-channel attention, and (4) composite Dice-Cross-Entropy loss for boundary refinement. Pretrained on BraTS 2023 and fine-tuned on BraTS-Africa data, SegFormer3D-plus demonstrates improved tumor subregion delineation and boundary localization across heterogeneous African clinical scans, highlighting the value of radiomics-guided domain adaptation for resource-limited settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>I.2.10; I.4.8; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Hints</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02933v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02933v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02933v1">http://arxiv.org/abs/2511.02933v1</a><br>Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02946v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02946v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02946v1">http://arxiv.org/abs/2511.02946v1</a><br>We introduce ProM3E, a probabilistic masked multimodal embedding model for any-to-any generation of multimodal representations for ecology. ProM3E is based on masked modality reconstruction in the embedding space, learning to infer missing modalities given a few context modalities. By design, our model supports modality inversion in the embedding space. The probabilistic nature of our model allows us to analyse the feasibility of fusing various modalities for given downstream tasks, essentially learning what to fuse. Using these features of our model, we propose a novel cross-modal retrieval approach that mixes inter-modal and intra-modal similarities to achieve superior performance across all retrieval tasks. We further leverage the hidden representation from our model to perform linear probing tasks and demonstrate the superior representation learning capability of our model. All our code, datasets and model will be released at <a href="https://vishu26.github.io/prom3e">https://vishu26.github.io/prom3e</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02953v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02953v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02953v1">http://arxiv.org/abs/2511.02953v1</a><br>Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model’s ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02992v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02992v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02992v1">http://arxiv.org/abs/2511.02992v1</a><br>Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02996v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02996v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02996v1">http://arxiv.org/abs/2511.02996v1</a><br>Vision-language models (VLMs) have demonstrated strong cross-modal capabilities, yet most work remains limited to 2D data and assumes binary supervision (i.e., positive vs. negative pairs), overlooking the continuous and structured dependencies present in volumetric data such as CT. Existing approaches often treat volumetric scans as independent 2D slices, compromising spatial coherence and underutilizing rich clinical semantics. We propose SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework that integrates (i) volumetric spatial semantics to preserve anatomical structure and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide alignment. This yields structurally consistent and semantically grounded representations under limited supervision, demonstrating strong cross-task transferability (retrieval, report generation, and classification), and cross-domain generalizability with consistent gains without further fine-tuning. In particular, compared to the previous state of the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval, improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an out-of-domain external dataset, we observe consistent gains, indicating the cross-task and cross-domain generalization ability of SCALE-VLP.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_03004v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_03004v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03004v1">http://arxiv.org/abs/2511.03004v1</a><br>Deep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the “Bootstrap Your Own Latent” pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_02994v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_02994v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02994v1">http://arxiv.org/abs/2511.02994v1</a><br>For developing safe Autonomous Driving Systems (ADS), rigorous testing is required before they are deemed safe for road deployments. Since comprehensive conventional physical testing is impractical due to cost and safety concerns, Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing VTE-generated sensor outputs against their real-world analogues can be a strong indication that the VTE accurately represents reality. Correspondingly, this work explores a comprehensive experimental approach to finding evaluation metrics suitable for comparing real-world and simulated LiDAR scans. The metrics were tested in terms of sensitivity and accuracy with different noise, density, distortion, sensor orientation, and channel settings. From comparing the metrics, we found that Density Aware Chamfer Distance (DCD) works best across all cases. In the second step of the research, a Virtual Testing Environment was generated using real LiDAR scan data. The data was collected in a controlled environment with only static objects using an instrumented vehicle equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans were compared in terms of model perception and geometric similarity. Actual and simulated LiDAR scans have a similar semantic segmentation output with a mIoU of 21% with corrected intensity and an average density aware chamfer distance (DCD) of 0.63. This indicates a slight difference in the geometric properties of simulated and real LiDAR scans and a significant difference between model outputs. During the comparison, density-aware chamfer distance was found to be the most correlated among the metrics with perception methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Foundation Model for Brain MRI with Dynamic Modality Integration</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_03014v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_03014v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03014v1">http://arxiv.org/abs/2511.03014v1</a><br>We present a foundation model for brain MRI that can work with different combinations of imaging sequences. The model uses one encoder with learnable modality embeddings, conditional layer normalization, and a masked autoencoding objective that accounts for missing modalities. A variance-covariance regularizer is applied to stabilize feature learning and improve representation diversity. This design removes the need for separate models for each modality and allows the network to adapt when some sequences are missing or unseen. It is trained on about 60,000 multi-center MRIs using self-supervised reconstruction and modality imputation to learn flexible representations. A learnable modality embedding guides feature extraction so the encoder can adjust to different inputs. We describe our planned evaluation on brain tumor and multiple sclerosis segmentation, as well as lesion classification, under various modality settings. Preliminary results show that the method works feasibly, and further experiments are planned to study its performance in more detail. All code and pretrained models are available at <a href="https://github.com/BrainFM/brainfm">https://github.com/BrainFM/brainfm</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data-Efficient Realized Volatility Forecasting with Vision Transformers</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_03046v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_03046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03046v1">http://arxiv.org/abs/2511.03046v1</a><br>Recent work in financial machine learning has shown the virtue of complexity: the phenomenon by which deep learning methods capable of learning highly nonlinear relationships outperform simpler approaches in financial forecasting. While transformer architectures like Informer have shown promise for financial time series forecasting, the application of transformer models for options data remains largely unexplored. We conduct preliminary studies towards the development of a transformer model for options data by training the Vision Transformer (ViT) architecture, typically used in modern image recognition and classification systems, to predict the realized volatility of an asset over the next 30 days from its implied volatility surface (augmented with date information) for a single day. We show that the ViT can learn seasonal patterns and nonlinear features from the IV surface, suggesting a promising direction for model development.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>I.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_03019v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_03019v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03019v1">http://arxiv.org/abs/2511.03019v1</a><br>Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth</title>
    <link href="/2025/11/04/cs.CV/2025-11-04-2511_03053v1/"/>
    <url>/2025/11/04/cs.CV/2025-11-04-2511_03053v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03053v1">http://arxiv.org/abs/2511.03053v1</a><br>Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds’ uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning</title>
    <link href="/2025/11/04/eess.IV/2025-11-04-2511_02210v1/"/>
    <url>/2025/11/04/eess.IV/2025-11-04-2511_02210v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02210v1">http://arxiv.org/abs/2511.02210v1</a><br>Segmental longitudinal strain (SLS) of the left ventricle (LV) is an important prognostic indicator for evaluating regional LV dysfunction, in particular for diagnosing and managing myocardial ischemia. Current techniques for strain estimation require significant manual intervention and expertise, limiting their efficiency and making them too resource-intensive for monitoring purposes. This study introduces the first automated pipeline, autoStrain, for SLS estimation in transesophageal echocardiography (TEE) using deep learning (DL) methods for motion estimation. We present a comparative analysis of two DL approaches: TeeFlow, based on the RAFT optical flow model for dense frame-to-frame predictions, and TeeTracker, based on the CoTracker point trajectory model for sparse long-sequence predictions.   As ground truth motion data from real echocardiographic sequences are hardly accessible, we took advantage of a unique simulation pipeline (SIMUS) to generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with ground truth myocardial motion to train and evaluate both models. Our evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.   Clinical validation on 16 patients further demonstrated that SLS estimation with our autoStrain pipeline aligned with clinical references, achieving a mean difference (95% limits of agreement) of 1.09% (-8.90% to 11.09%). Incorporation of simulated ischemia in the synTEE data improved the accuracy of the models in quantifying abnormal deformation. Our findings indicate that integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>High-Resolution Magnetic Particle Imaging System Matrix Recovery Using a Vision Transformer with Residual Feature Network</title>
    <link href="/2025/11/04/eess.IV/2025-11-04-2511_02212v1/"/>
    <url>/2025/11/04/eess.IV/2025-11-04-2511_02212v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02212v1">http://arxiv.org/abs/2511.02212v1</a><br>This study presents a hybrid deep learning framework, the Vision Transformer with Residual Feature Network (VRF-Net), for recovering high-resolution system matrices in Magnetic Particle Imaging (MPI). MPI resolution often suffers from downsampling and coil sensitivity variations. VRF-Net addresses these challenges by combining transformer-based global attention with residual convolutional refinement, enabling recovery of both large-scale structures and fine details. To reflect realistic MPI conditions, the system matrix is degraded using a dual-stage downsampling strategy. Training employed paired-image super-resolution on the public Open MPI dataset and a simulated dataset incorporating variable coil sensitivity profiles. For system matrix recovery on the Open MPI dataset, VRF-Net achieved nRMSE &#x3D; 0.403, pSNR &#x3D; 39.08 dB, and SSIM &#x3D; 0.835 at 2x scaling, and maintained strong performance even at challenging scale 8x (pSNR &#x3D; 31.06 dB, SSIM &#x3D; 0.717). For the simulated dataset, VRF-Net achieved nRMSE &#x3D; 4.44, pSNR &#x3D; 28.52 dB, and SSIM &#x3D; 0.771 at 2x scaling, with stable performance at higher scales. On average, it reduced nRMSE by 88.2%, increased pSNR by 44.7%, and improved SSIM by 34.3% over interpolation and CNN-based methods. In image reconstruction of Open MPI phantoms, VRF-Net further reduced reconstruction error to nRMSE &#x3D; 1.79 at 2x scaling, while preserving structural fidelity (pSNR &#x3D; 41.58 dB, SSIM &#x3D; 0.960), outperforming existing methods. These findings demonstrate that VRF-Net enables sharper, artifact-free system matrix recovery and robust image reconstruction across multiple scales, offering a promising direction for future in vivo applications.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>physics.med-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</title>
    <link href="/2025/11/04/eess.IV/2025-11-04-2511_02400v1/"/>
    <url>/2025/11/04/eess.IV/2025-11-04-2511_02400v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02400v1">http://arxiv.org/abs/2511.02400v1</a><br>The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: <a href="https://github.com/Minds-R-Lab/MammoClean">https://github.com/Minds-R-Lab/MammoClean</a>.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</title>
    <link href="/2025/11/04/eess.IV/2025-11-04-2511_02576v1/"/>
    <url>/2025/11/04/eess.IV/2025-11-04-2511_02576v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02576v1">http://arxiv.org/abs/2511.02576v1</a><br>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over&#x2F;under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: <a href="https://gitlab.inria.fr/adelangl/SCORE">https://gitlab.inria.fr/adelangl/SCORE</a>.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NEF-NET+: Adapting Electrocardio panorama in the wild</title>
    <link href="/2025/11/04/eess.IV/2025-11-04-2511_02880v1/"/>
    <url>/2025/11/04/eess.IV/2025-11-04-2511_02880v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02880v1">http://arxiv.org/abs/2511.02880v1</a><br>Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and compensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Optimizing the nnU-Net model for brain tumor (Glioma) segmentation Using a BraTS Sub-Saharan Africa (SSA) dataset</title>
    <link href="/2025/11/04/eess.IV/2025-11-04-2511_02893v1/"/>
    <url>/2025/11/04/eess.IV/2025-11-04-2511_02893v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02893v1">http://arxiv.org/abs/2511.02893v1</a><br>Medical image segmentation is a critical achievement in modern medical science, developed over decades of research. It allows for the exact delineation of anatomical and pathological features in two- or three-dimensional pictures by utilizing notions like pixel intensity, texture, and anatomical context. With the advent of automated segmentation, physicians and radiologists may now concentrate on diagnosis and treatment planning while intelligent computers perform routine image processing tasks.   This study used the BraTS Sub-Saharan Africa dataset, a selected subset of the BraTS dataset that included 60 multimodal MRI cases from patients with glioma. Surprisingly, the nnU Net model trained on the initial 60 instances performed better than the network trained on an offline-augmented dataset of 360 cases. Hypothetically, the offline augmentations introduced artificial anatomical variances or intensity distributions, reducing generalization. In contrast, the original dataset, when paired with nnU Net’s robust online augmentation procedures, maintained realistic variability and produced better results. The study achieved a Dice score of 0.84 for whole tumor segmentation. These findings highlight the significance of data quality and proper augmentation approaches in constructing accurate, generalizable medical picture segmentation models, particularly for under-represented locations.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI</title>
    <link href="/2025/11/04/eess.IV/2025-11-04-2511_02928v1/"/>
    <url>/2025/11/04/eess.IV/2025-11-04-2511_02928v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02928v1">http://arxiv.org/abs/2511.02928v1</a><br>Glioma segmentation is critical for diagnosis and treatment planning, yet remains challenging in Sub-Saharan Africa due to limited MRI infrastructure and heterogeneous acquisition protocols that induce severe domain shift. We propose SegFormer3D-plus, a radiomics-guided transformer architecture designed for robust segmentation under domain variability. Our method combines: (1) histogram matching for intensity harmonization across scanners, (2) radiomic feature extraction with PCA-reduced k-means for domain-aware stratified sampling, (3) a dual-pathway encoder with frequency-aware feature extraction and spatial-channel attention, and (4) composite Dice-Cross-Entropy loss for boundary refinement. Pretrained on BraTS 2023 and fine-tuned on BraTS-Africa data, SegFormer3D-plus demonstrates improved tumor subregion delineation and boundary localization across heterogeneous African clinical scans, highlighting the value of radiomics-guided domain adaptation for resource-limited settings.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>I.2.10; I.4.8; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Kullback-Leibler divergence method for input-system-state identification</title>
    <link href="/2025/11/04/eess.SP/2025-11-04-2511_02426v1/"/>
    <url>/2025/11/04/eess.SP/2025-11-04-2511_02426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02426v1">http://arxiv.org/abs/2511.02426v1</a><br>The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
    <link href="/2025/11/04/eess.SP/2025-11-04-2511_02717v1/"/>
    <url>/2025/11/04/eess.SP/2025-11-04-2511_02717v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02717v1">http://arxiv.org/abs/2511.02717v1</a><br>The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NEF-NET+: Adapting Electrocardio panorama in the wild</title>
    <link href="/2025/11/04/eess.SP/2025-11-04-2511_02880v1/"/>
    <url>/2025/11/04/eess.SP/2025-11-04-2511_02880v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02880v1">http://arxiv.org/abs/2511.02880v1</a><br>Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and compensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Kullback-Leibler divergence method for input-system-state identification</title>
    <link href="/2025/11/04/eess.SY/2025-11-04-2511_02426v1/"/>
    <url>/2025/11/04/eess.SY/2025-11-04-2511_02426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02426v1">http://arxiv.org/abs/2511.02426v1</a><br>The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
    <link href="/2025/11/04/eess.SY/2025-11-04-2511_02717v1/"/>
    <url>/2025/11/04/eess.SY/2025-11-04-2511_02717v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02717v1">http://arxiv.org/abs/2511.02717v1</a><br>The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Instance Segmentation to 3D Growth Trajectory Reconstruction in Planktonic Foraminifera</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02142v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02142v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02142v1">http://arxiv.org/abs/2511.02142v1</a><br>Planktonic foraminifera, marine protists characterized by their intricate chambered shells, serve as valuable indicators of past and present environmental conditions. Understanding their chamber growth trajectory provides crucial insights into organismal development and ecological adaptation under changing environments. However, automated tracing of chamber growth from imaging data remains largely unexplored, with existing approaches relying heavily on manual segmentation of each chamber, which is time-consuming and subjective. In this study, we propose an end-to-end pipeline that integrates instance segmentation, a computer vision technique not extensively explored in foraminifera, with a dedicated chamber ordering algorithm to automatically reconstruct three-dimensional growth trajectories from high-resolution computed tomography scans. We quantitatively and qualitatively evaluate multiple instance segmentation methods, each optimized for distinct spatial features of the chambers, and examine their downstream influence on growth-order reconstruction accuracy. Experimental results on expert-annotated datasets demonstrate that the proposed pipeline substantially reduces manual effort while maintaining biologically meaningful accuracy. Although segmentation models exhibit under-segmentation in smaller chambers due to reduced voxel fidelity and subtle inter-chamber connectivity, the chamber-ordering algorithm remains robust, achieving consistent reconstruction of developmental trajectories even under partial segmentation. This work provides the first fully automated and reproducible pipeline for digital foraminiferal growth analysis, establishing a foundation for large-scale, data-driven ecological studies.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02144v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02144v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02144v1">http://arxiv.org/abs/2511.02144v1</a><br>Accurate quantification of pavement crack width plays a pivotal role in assessing structural integrity and guiding maintenance interventions. However, achieving precise crack width measurements presents significant challenges due to: (1) the complex, non-uniform morphology of crack boundaries, which limits the efficacy of conventional approaches, and (2) the demand for rapid measurement capabilities from arbitrary pixel locations to facilitate comprehensive pavement condition evaluation. To overcome these limitations, this study introduces a cascaded framework integrating Principal Component Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from digital images. The proposed methodology comprises three sequential stages: (1) initial crack segmentation using established detection algorithms to generate a binary representation, (2) determination of the primary orientation axis for quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations were conducted across three publicly available datasets, demonstrating that the proposed approach achieves superior performance in both computational efficiency and measurement accuracy compared to existing state-of-the-art techniques.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Autobiasing Event Cameras for Flickering Mitigation</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02180v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02180v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02180v1">http://arxiv.org/abs/2511.02180v1</a><br>Understanding and mitigating flicker effects caused by rapid variations in light intensity is critical for enhancing the performance of event cameras in diverse environments. This paper introduces an innovative autonomous mechanism for tuning the biases of event cameras, effectively addressing flicker across a wide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on additional hardware or software for flicker filtering, our approach leverages the event cameras inherent bias settings. Utilizing a simple Convolutional Neural Networks -CNNs, the system identifies instances of flicker in a spatial space and dynamically adjusts specific biases to minimize its impact. The efficacy of this autobiasing system was robustly tested using a face detector framework under both well-lit and low-light conditions, as well as across various frequencies. The results demonstrated significant improvements: enhanced YOLO confidence metrics for face detection, and an increased percentage of frames capturing detected faces. Moreover, the average gradient, which serves as an indicator of flicker presence through edge detection, decreased by 38.2 percent in well-lit conditions and by 53.6 percent in low-light conditions. These findings underscore the potential of our approach to significantly improve the functionality of event cameras in a range of adverse lighting scenarios.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02182v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02182v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02182v1">http://arxiv.org/abs/2511.02182v1</a><br>In this technical report, we introduce a framework to address Grounded Video Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The GVQA task demands robust multimodal models capable of complex reasoning over video content, grounding the resulting answers visually, and tracking the referenced objects temporally. To achieve this capability, our proposed approach decomposes the GVQA task into a three-stage pipeline: (1) Video Reasoning &amp; QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key contribution is the introduction of a trigger moment, derived from our proposed CORTEX prompt, which pinpoints the single most visible frame of a target object to serve as a robust anchor for grounding and tracking. To this end, we achieve the HOTA score of 0.4968, which marks a significant improvement over the previous year’s winning score of 0.2704 on GVQA task.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02193v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02193v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02193v1">http://arxiv.org/abs/2511.02193v1</a><br>Accurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 $%$ on DRIVE and 1.25 $%$ on STARE, demonstrating its effectiveness and advancement. The project code is public via <a href="https://github.com/liujiawen-jpg/MM-UNet">https://github.com/liujiawen-jpg/MM-UNet</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02206v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02206v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02206v1">http://arxiv.org/abs/2511.02206v1</a><br>Background: Alzheimer’s disease (AD) diagnosis heavily relies on amyloid-beta positron emission tomography (Abeta-PET), which is limited by high cost and limited accessibility. This study explores whether Abeta-PET spatial patterns can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566 participants. A language-enhanced generative model, driven by a large language model (LLM) and multimodal information fusion, was developed to synthesize PET images. Synthesized images were evaluated for image quality, diagnostic consistency, and clinical applicability within a fully automated diagnostic pipeline. Findings: The synthetic PET images closely resemble real PET scans in both structural details (SSIM &#x3D; 0.920 +&#x2F;- 0.003) and regional patterns (Pearson’s r &#x3D; 0.955 +&#x2F;- 0.007). Diagnostic outcomes using synthetic PET show high agreement with real PET-based diagnoses (accuracy &#x3D; 0.80). Using synthetic PET, we developed a fully automatic AD diagnostic pipeline integrating PET synthesis and classification. The synthetic PET-based model (AUC &#x3D; 0.78) outperforms T1-based (AUC &#x3D; 0.68) and BBM-based (AUC &#x3D; 0.73) models, while combining synthetic PET and BBMs further improved performance (AUC &#x3D; 0.79). Ablation analysis supports the advantages of LLM integration and prompt engineering. Interpretation: Our language-enhanced generative model synthesizes realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial pattern assessment and improving the diagnostic workflow for Alzheimer’s disease.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02207v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02207v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02207v1">http://arxiv.org/abs/2511.02207v1</a><br>Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02210v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02210v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02210v1">http://arxiv.org/abs/2511.02210v1</a><br>Segmental longitudinal strain (SLS) of the left ventricle (LV) is an important prognostic indicator for evaluating regional LV dysfunction, in particular for diagnosing and managing myocardial ischemia. Current techniques for strain estimation require significant manual intervention and expertise, limiting their efficiency and making them too resource-intensive for monitoring purposes. This study introduces the first automated pipeline, autoStrain, for SLS estimation in transesophageal echocardiography (TEE) using deep learning (DL) methods for motion estimation. We present a comparative analysis of two DL approaches: TeeFlow, based on the RAFT optical flow model for dense frame-to-frame predictions, and TeeTracker, based on the CoTracker point trajectory model for sparse long-sequence predictions.   As ground truth motion data from real echocardiographic sequences are hardly accessible, we took advantage of a unique simulation pipeline (SIMUS) to generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with ground truth myocardial motion to train and evaluate both models. Our evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.   Clinical validation on 16 patients further demonstrated that SLS estimation with our autoStrain pipeline aligned with clinical references, achieving a mean difference (95% limits of agreement) of 1.09% (-8.90% to 11.09%). Incorporation of simulated ischemia in the synTEE data improved the accuracy of the models in quantifying abnormal deformation. Our findings indicate that integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>High-Resolution Magnetic Particle Imaging System Matrix Recovery Using a Vision Transformer with Residual Feature Network</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02212v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02212v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02212v1">http://arxiv.org/abs/2511.02212v1</a><br>This study presents a hybrid deep learning framework, the Vision Transformer with Residual Feature Network (VRF-Net), for recovering high-resolution system matrices in Magnetic Particle Imaging (MPI). MPI resolution often suffers from downsampling and coil sensitivity variations. VRF-Net addresses these challenges by combining transformer-based global attention with residual convolutional refinement, enabling recovery of both large-scale structures and fine details. To reflect realistic MPI conditions, the system matrix is degraded using a dual-stage downsampling strategy. Training employed paired-image super-resolution on the public Open MPI dataset and a simulated dataset incorporating variable coil sensitivity profiles. For system matrix recovery on the Open MPI dataset, VRF-Net achieved nRMSE &#x3D; 0.403, pSNR &#x3D; 39.08 dB, and SSIM &#x3D; 0.835 at 2x scaling, and maintained strong performance even at challenging scale 8x (pSNR &#x3D; 31.06 dB, SSIM &#x3D; 0.717). For the simulated dataset, VRF-Net achieved nRMSE &#x3D; 4.44, pSNR &#x3D; 28.52 dB, and SSIM &#x3D; 0.771 at 2x scaling, with stable performance at higher scales. On average, it reduced nRMSE by 88.2%, increased pSNR by 44.7%, and improved SSIM by 34.3% over interpolation and CNN-based methods. In image reconstruction of Open MPI phantoms, VRF-Net further reduced reconstruction error to nRMSE &#x3D; 1.79 at 2x scaling, while preserving structural fidelity (pSNR &#x3D; 41.58 dB, SSIM &#x3D; 0.960), outperforming existing methods. These findings demonstrate that VRF-Net enables sharper, artifact-free system matrix recovery and robust image reconstruction across multiple scales, offering a promising direction for future in vivo applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>physics.med-ph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Can Foundation Models Revolutionize Mobile AR Sparse Sensing?</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02215v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02215v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02215v1">http://arxiv.org/abs/2511.02215v1</a><br>Mobile sensing systems have long faced a fundamental trade-off between sensing quality and efficiency due to constraints in computation, power, and other limitations. Sparse sensing, which aims to acquire and process only a subset of sensor data, has been a key strategy for maintaining performance under such constraints. However, existing sparse sensing methods often suffer from reduced accuracy, as missing information across space and time introduces uncertainty into many sensing systems. In this work, we investigate whether foundation models can change the landscape of mobile sparse sensing. Using real-world mobile AR data, our evaluations demonstrate that foundation models offer significant improvements in geometry-aware image warping, a central technique for enabling accurate reuse of cross-frame information. Furthermore, our study demonstrates the scalability of foundation model-based sparse sensing and shows its leading performance in 3D scene reconstruction. Collectively, our study reveals critical aspects of the promises and the open challenges of integrating foundation models into mobile sparse sensing systems.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.ET</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer&#39;s Disease Diagnosis</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02228v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02228v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02228v1">http://arxiv.org/abs/2511.02228v1</a><br>Alzheimer’s disease (AD) is the most prevalent form of dementia, and its early diagnosis is essential for slowing disease progression. Recent studies on multimodal neuroimaging fusion using MRI and PET have achieved promising results by integrating multi-scale complementary features. However, most existing approaches primarily emphasize cross-modal complementarity while overlooking the diagnostic importance of modality-specific features. In addition, the inherent distributional differences between modalities often lead to biased and noisy representations, degrading classification performance. To address these challenges, we propose a Collaborative Attention and Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The proposed model introduces a learnable parameter representation (LPR) block to compensate for missing modality information, followed by a shared encoder and modality-independent encoders to preserve both shared and specific representations. Furthermore, a consistency-guided mechanism is employed to explicitly align the latent distributions across modalities. Experimental results on the ADNI dataset demonstrate that our method achieves superior diagnostic performance compared with existing fusion strategies.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02247v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02247v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02247v1">http://arxiv.org/abs/2511.02247v1</a><br>Monocular depth estimation (MDE) is a critical task to guide autonomous medical robots. However, obtaining absolute (metric) depth from an endoscopy camera in surgical scenes is difficult, which limits supervised learning of depth on real endoscopic images. Current image-level unsupervised domain adaptation methods translate synthetic images with known depth maps into the style of real endoscopic frames and train depth networks using these translated images with their corresponding depth maps. However a domain gap often remains between real and translated synthetic images. In this paper, we present a latent feature alignment method to improve absolute depth estimation by reducing this domain gap in the context of endoscopic videos of the central airway. Our methods are agnostic to the image translation process and focus on the depth estimation itself. Specifically, the depth network takes translated synthetic and real endoscopic frames as input and learns latent domain-invariant features via adversarial learning and directional feature consistency. The evaluation is conducted on endoscopic videos of central airway phantoms with manually aligned absolute depth maps. Compared to state-of-the-art MDE methods, our approach achieves superior performance on both absolute and relative depth metrics, and consistently improves results across various backbones and pretrained weights. Our code is available at <a href="https://github.com/MedICL-VU/MDE">https://github.com/MedICL-VU/MDE</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02271v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02271v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02271v1">http://arxiv.org/abs/2511.02271v1</a><br>Medical Report Generation (MRG) is a key part of modern medical diagnostics, as it automatically generates reports from radiological images to reduce radiologists’ burden. However, reliable MRG models for lesion description face three main challenges: insufficient domain knowledge understanding, poor text-visual entity embedding alignment, and spurious correlations from cross-modal biases. Previous work only addresses single challenges, while this paper tackles all three via a novel hierarchical task decomposition approach, proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into low-, mid-, and high-level tasks: 1) Low-level: align medical entity features with spatial locations to enhance domain knowledge for visual encoders; 2) Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling (images) to boost cross-modal alignment via mutual guidance; 3) High-level: a cross-modal causal intervention module (via front-door intervention) to reduce confounders and improve interpretability. Extensive experiments confirm HTSC-CIF’s effectiveness, significantly outperforming state-of-the-art (SOTA) MRG methods. Code will be made public upon paper acceptance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02277v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02277v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02277v1">http://arxiv.org/abs/2511.02277v1</a><br>Object pose estimation is a task that is of central importance in 3D Computer Vision. Given a target image and a canonical pose, a single point estimate may very often be sufficient; however, a probabilistic pose output is related to a number of benefits when pose is not unambiguous due to sensor and projection constraints or inherent object symmetries. With this paper, we explore the usefulness of using the well-known Euler angles parameterisation as a basis for a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation, 3D pose has been parameterized in a number of ways, either in or out of the context of parameter estimation. We explore the idea that Euler angles, despite their shortcomings, may lead to useful models in a number of aspects, compared to a model built on a more complex parameterisation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02280v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02280v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02280v1">http://arxiv.org/abs/2511.02280v1</a><br>We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at <a href="https://github.com/BytedanceDouyinContent/SAIL-RL">https://github.com/BytedanceDouyinContent/SAIL-RL</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02288v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02288v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02288v1">http://arxiv.org/abs/2511.02288v1</a><br>We propose a Graph Neural Network (GNN)-based approach for Handwritten Mathematical Expression (HME) recognition by modeling HMEs as graphs, where nodes represent symbols and edges capture spatial dependencies. A deep BLSTM network is used for symbol segmentation, recognition, and spatial relation classification, forming an initial primitive graph. A 2D-CFG parser then generates all possible spatial relations, while the GNN-based link prediction model refines the structure by removing unnecessary connections, ultimately forming the Symbol Label Graph. Experimental results demonstrate the effectiveness of our approach, showing promising performance in HME structure recognition.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3D Point Cloud Object Detection on Edge Devices for Split Computing</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02293v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02293v1">http://arxiv.org/abs/2511.02293v1</a><br>The field of autonomous driving technology is rapidly advancing, with deep learning being a key component. Particularly in the field of sensing, 3D point cloud data collected by LiDAR is utilized to run deep neural network models for 3D object detection. However, these state-of-the-art models are complex, leading to longer processing times and increased power consumption on edge devices. The objective of this study is to address these issues by leveraging Split Computing, a distributed machine learning inference method. Split Computing aims to lessen the computational burden on edge devices, thereby reducing processing time and power consumption. Furthermore, it minimizes the risk of data breaches by only transmitting intermediate data from the deep neural network model. Experimental results show that splitting after voxelization reduces the inference time by 70.8% and the edge device execution time by 90.0%. When splitting within the network, the inference time is reduced by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
      <tag>C.2.4; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02329v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02329v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02329v1">http://arxiv.org/abs/2511.02329v1</a><br>We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) – originally developed for group synchronization – to camera location estimation. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, showing that cycle consistency alone – without access to inter-camera distances – suffices to achieve the lowest sample complexity currently known. To further enhance robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation synchronization. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment.</p>]]></content>
    
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ME</tag>
      
      <tag>90C26, 90C17, 68Q87, 65C20, 90-08, 60-08</tag>
      
      <tag>G.1.6; I.4.0</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAFD-CC: Global-Aware Feature Decoupling with Confidence Calibration for OOD Detection</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02335v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02335v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02335v1">http://arxiv.org/abs/2511.02335v1</a><br>Out-of-distribution (OOD) detection is paramount to ensuring the reliability and robustness of learning models in real-world applications. Existing post-hoc OOD detection methods detect OOD samples by leveraging their features and logits information without retraining. However, they often overlook the inherent correlation between features and logits, which is crucial for effective OOD detection. To address this limitation, we propose Global-Aware Feature Decoupling with Confidence Calibration (GAFD-CC). GAFD-CC aims to refine decision boundaries and increase discriminative performance. Firstly, it performs global-aware feature decoupling guided by classification weights. This involves aligning features with the direction of global classification weights to decouple them. From this, GAFD-CC extracts two types of critical information: positively correlated features that promote in-distribution (ID)&#x2F;OOD boundary refinement and negatively correlated features that suppress false positives and tighten these boundaries. Secondly, it adaptively fuses these decoupled features with multi-scale logit-based confidence for comprehensive and robust OOD detection. Extensive experiments on large-scale benchmarks demonstrate GAFD-CC’s competitive performance and strong generalization ability compared to those of state-of-the-art methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02349v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02349v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02349v1">http://arxiv.org/abs/2511.02349v1</a><br>Portable physiological monitoring is essential for early detection and management of cardiovascular disease, but current methods often require specialized equipment that limits accessibility or impose impractical postures that patients cannot maintain. Video-based photoplethysmography on smartphones offers a convenient noninvasive alternative, yet it still faces reliability challenges caused by motion artifacts, lighting variations, and single-view constraints. Few studies have demonstrated reliable application to cardiovascular patients, and no widely used open datasets exist for cross-device accuracy. To address these limitations, we introduce the M3PD dataset, the first publicly available dual-view mobile photoplethysmography dataset, comprising synchronized facial and fingertip videos captured simultaneously via front and rear smartphone cameras from 60 participants (including 47 cardiovascular patients). Building on this dual-view setting, we further propose F3Mamba, which fuses the facial and fingertip views through Mamba-based temporal modeling. The model reduces heart-rate error by 21.9 to 30.2 percent over existing single-view baselines while improving robustness in challenging real-world scenarios. Data and code: <a href="https://github.com/Health-HCI-Group/F3Mamba">https://github.com/Health-HCI-Group/F3Mamba</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02360v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02360v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02360v1">http://arxiv.org/abs/2511.02360v1</a><br>In human cognition, there exist numerous thought processes that are tacit and beyond verbal expression, enabling us to understand and interact with the world in multiple ways. However, contemporary Vision-Language Models (VLMs) remain constrained to reasoning within the discrete and rigid space of linguistic tokens, thereby bottlenecking the rich, high-dimensional nature of visual perception. To bridge this gap, we propose CoCoVa (Chain of Continuous Vision-Language Thought), a novel framework for vision-language model that leverages continuous cross-modal reasoning for diverse vision-language tasks. The core of CoCoVa is an iterative reasoning cycle, where a novel Latent Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a chain of latent thought vectors through cross-modal fusion. To focus this process, a token selection mechanism dynamically identifies salient visual regions, mimicking attentional focus. To ensure these latent thoughts remain grounded, we train the model with a multi-task objective that combines contrastive learning and diffusion-based reconstruction, enforcing alignment between latent representations and both visual and textual modalities. Evaluations show CoCoVa improves accuracy and token efficiency over strong baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B models on almost all benchmarks. When scaled to 7B LLM backbones, it remains competitive with state-of-the-art models. Qualitative analysis validates that learned latent space captures interpretable and structured reasoning patterns, highlighting the potential of CoCoVa to bridge the representational gap between discrete language processing and the continuous nature of visual understanding.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02384v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02384v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02384v1">http://arxiv.org/abs/2511.02384v1</a><br>Large-scale chemical reaction datasets are crucial for AI research in chemistry. However, existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models. In response to this challenge, we propose the RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP). Our framework reformulates the traditional coordinate prediction driven parsing process into an image captioning problem, which Large Vision-Language Models (LVLMs) handle naturally. We introduce a strategy termed “BBox and Index as Visual Prompt” (BIVP), which uses our state-of-the-art molecular detector, MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the input image. This turns the downstream parsing into a natural-language description problem. Extensive experiments show that the BIVP strategy significantly improves structural extraction quality while simplifying model design. We further construct the RxnCaption-11k dataset, an order of magnitude larger than prior real-world literature benchmarks, with a balanced test subset across four layout archetypes. Experiments demonstrate that RxnCaption-VL achieves state-of-the-art performance on multiple metrics. We believe our method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry. We will release data, models, and code on GitHub.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02395v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02395v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02395v1">http://arxiv.org/abs/2511.02395v1</a><br>Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning. While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context. Radar sensors overcome this problem with their ability to provide a direct measurement of a point’s Doppler velocity, which can be exploited for single-scan moving object segmentation. However, radar point clouds are often sparse and noisy, making data annotation for use in supervised learning very tedious, time-consuming, and cost-intensive. To overcome this problem, we address the task of self-supervised moving object segmentation of sparse and noisy radar point clouds. We follow a two-step approach of contrastive self-supervised representation learning with subsequent supervised fine-tuning using limited amounts of annotated data. We propose a novel clustering-based contrastive loss function with cluster refinement based on dynamic points removal to pretrain the network to produce motion-aware representations of the radar data. Our method improves label efficiency after fine-tuning, effectively boosting state-of-the-art performance by self-supervised pretraining.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02205v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02205v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02205v1">http://arxiv.org/abs/2511.02205v1</a><br>Multimodal spatiotemporal learning on real-world experimental data is constrained by two challenges: within-modality measurements are sparse, irregular, and noisy (QA&#x2F;QC artifacts) but cross-modally correlated; the set of available modalities varies across space and time, shrinking the usable record unless models can adapt to arbitrary subsets at train and test time. We propose OmniField, a continuity-aware framework that learns a continuous neural field conditioned on available modalities and iteratively fuses cross-modal context. A multimodal crosstalk block architecture paired with iterative cross-modal refinement aligns signals prior to the decoder, enabling unified reconstruction, interpolation, forecasting, and cross-modal prediction without gridding or surrogate preprocessing. Extensive evaluations show that OmniField consistently outperforms eight strong multimodal spatiotemporal baselines. Under heavy simulated sensor noise, performance remains close to clean-input levels, highlighting robustness to corrupted measurements.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02400v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02400v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02400v1">http://arxiv.org/abs/2511.02400v1</a><br>The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: <a href="https://github.com/Minds-R-Lab/MammoClean">https://github.com/Minds-R-Lab/MammoClean</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02404v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02404v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02404v1">http://arxiv.org/abs/2511.02404v1</a><br>Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic cats) have vertically elongated pupils linked to ambush predation; yet, how such specializations manifest in downstream visual representations remains incompletely understood. We present a unified, frozen-encoder benchmark that quantifies feline-human cross-species representational alignment in the wild, across convolutional networks, supervised Vision Transformers, windowed transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel Alignment (linear and RBF) and Representational Similarity Analysis, with additional distributional and stability tests reported in the paper. Across models, DINO ViT-B&#x2F;16 attains the most substantial alignment (mean CKA-RBF $\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$), peaking at early blocks, indicating that token-level self-supervision induces early-stage features that bridge species-specific statistics. Supervised ViTs are competitive on CKA yet show weaker geometric correspondence than DINO (e.g., ViT-B&#x2F;16 RSA $\approx0.53$ at block8; ViT-L&#x2F;16 $\approx0.47$ at block14), revealing depth-dependent divergences between similarity and representational geometry. CNNs remain strong baselines but below plain ViTs on alignment, and windowed transformers underperform plain ViTs, implicating architectural inductive biases in cross-species alignment. Results indicate that self-supervision coupled with ViT inductive biases yields representational geometries that more closely align feline and human visual systems than widely used CNNs and windowed Transformers, providing testable neuroscientific hypotheses about where and how cross-species visual computations converge. We release our code and dataset for reference and reproducibility.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IllumFlow: Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex Decomposition</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02411v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02411v1">http://arxiv.org/abs/2511.02411v1</a><br>We present IllumFlow, a novel framework that synergizes conditional Rectified Flow (CRF) with Retinex theory for low-light image enhancement (LLIE). Our model addresses low-light enhancement through separate optimization of illumination and reflectance components, effectively handling both lighting variations and noise. Specifically, we first decompose an input image into reflectance and illumination components following Retinex theory. To model the wide dynamic range of illumination variations in low-light images, we propose a conditional rectified flow framework that represents illumination changes as a continuous flow field. While complex noise primarily resides in the reflectance component, we introduce a denoising network, enhanced by flow-derived data augmentation, to remove reflectance noise and chromatic aberration while preserving color fidelity. IllumFlow enables precise illumination adaptation across lighting conditions while naturally supporting customizable brightness enhancement. Extensive experiments on low-light enhancement and exposure correction demonstrate superior quantitative and qualitative performance over existing methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02415v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02415v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02415v1">http://arxiv.org/abs/2511.02415v1</a><br>Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&amp;A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Synthetic Crop-Weed Image Generation and its Impact on Model Generalization</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02417v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02417v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02417v1">http://arxiv.org/abs/2511.02417v1</a><br>Precise semantic segmentation of crops and weeds is necessary for agricultural weeding robots. However, training deep learning models requires large annotated datasets, which are costly to obtain in real fields. Synthetic data can reduce this burden, but the gap between simulated and real images remains a challenge. In this paper, we present a pipeline for procedural generation of synthetic crop-weed images using Blender, producing annotated datasets under diverse conditions of plant growth, weed density, lighting, and camera angle. We benchmark several state-of-the-art segmentation models on synthetic and real datasets and analyze their cross-domain generalization. Our results show that training on synthetic images leads to a sim-to-real gap of 10%, surpassing previous state-of-the-art methods. Moreover, synthetic data demonstrates good generalization properties, outperforming real datasets in cross-domain scenarios. These findings highlight the potential of synthetic agricultural datasets and support hybrid strategies for more efficient model training.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Kullback-Leibler divergence method for input-system-state identification</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02426v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02426v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02426v1">http://arxiv.org/abs/2511.02426v1</a><br>The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>cs.IT</tag>
      
      <tag>math.IT</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02427v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02427v1">http://arxiv.org/abs/2511.02427v1</a><br>Video Understanding, Scene Interpretation and Commonsense Reasoning are highly challenging tasks enabling the interpretation of visual information, allowing agents to perceive, interact with and make rational decisions in its environment. Large Language Models (LLMs) and Visual Language Models (VLMs) have shown remarkable advancements in these areas in recent years, enabling domain-specific applications as well as zero-shot open vocabulary tasks, combining multiple domains. However, the required computational complexity poses challenges for their application on edge devices and in the context of Mobile Robotics, especially considering the trade-off between accuracy and inference time. In this paper, we investigate the capabilities of state-of-the-art VLMs for the task of Scene Interpretation and Action Recognition, with special regard to small VLMs capable of being deployed to edge devices in the context of Mobile Robotics. The proposed pipeline is evaluated on a diverse dataset consisting of various real-world cityscape, on-campus and indoor scenarios. The experimental evaluation discusses the potential of these small models on edge devices, with particular emphasis on challenges, weaknesses, inherent model biases and the application of the gained information. Supplementary material is provided via the following repository: <a href="https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/">https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HAGI++: Head-Assisted Gaze Imputation and Generation</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02468v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02468v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02468v1">http://arxiv.org/abs/2511.02468v1</a><br>Mobile eye tracking plays a vital role in capturing human visual attention across both real-world and extended reality (XR) environments, making it an essential tool for applications ranging from behavioural research to human-computer interaction. However, missing values due to blinks, pupil detection errors, or illumination changes pose significant challenges for further gaze data analysis. To address this challenge, we introduce HAGI++ - a multi-modal diffusion-based approach for gaze data imputation that, for the first time, uses the integrated head orientation sensors to exploit the inherent correlation between head and eye movements. HAGI++ employs a transformer-based diffusion model to learn cross-modal dependencies between eye and head representations and can be readily extended to incorporate additional body movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D, and HOT3D datasets demonstrate that HAGI++ consistently outperforms conventional interpolation methods and deep learning-based time-series imputation baselines in gaze imputation. Furthermore, statistical analyses confirm that HAGI++ produces gaze velocity distributions that closely match actual human gaze behaviour, ensuring more realistic gaze imputations. Moreover, by incorporating wrist motion captured from commercial wearable devices, HAGI++ surpasses prior methods that rely on full-body motion capture in the extreme case of 100% missing gaze data (pure gaze generation). Our method paves the way for more complete and accurate eye gaze recordings in real-world settings and has significant potential for enhancing gaze-based analysis and interaction across various application domains.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02462v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02462v1">http://arxiv.org/abs/2511.02462v1</a><br>Satellite image inpainting is a crucial task in remote sensing, where accurately restoring missing or occluded regions is essential for robust image analysis. In this paper, we propose KAO, a novel framework that utilizes Kernel-Adaptive Optimization within diffusion models for satellite image inpainting. KAO is specifically designed to address the challenges posed by very high-resolution (VHR) satellite datasets, such as DeepGlobe and the Massachusetts Roads Dataset. Unlike existing methods that rely on preconditioned models requiring extensive retraining or postconditioned models with significant computational overhead, KAO introduces a Latent Space Conditioning approach, optimizing a compact latent space to achieve efficient and accurate inpainting. Furthermore, we incorporate Explicit Propagation into the diffusion process, facilitating forward-backward fusion, which improves the stability and precision of the method. Experimental results demonstrate that KAO sets a new benchmark for VHR satellite image restoration, providing a scalable, high-performance solution that balances the efficiency of preconditioned models with the flexibility of postconditioned models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02473v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02473v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02473v1">http://arxiv.org/abs/2511.02473v1</a><br>Multi-view action recognition aims to recognize human actions using multiple camera views and deals with occlusion caused by obstacles or crowds. In this task, cooperation among views, which generates a joint representation by combining multiple views, is vital. Previous studies have explored promising cooperation methods for improving performance. However, since their methods focus only on the task setting of recognizing a single action from an entire video, they are not applicable to the recently popular spatio-temporal action recognition~(STAR) setting, in which each person’s action is recognized sequentially. To address this problem, this paper proposes a multi-view action recognition method for the STAR setting, called MVAFormer. In MVAFormer, we introduce a novel transformer-based cooperation module among views. In contrast to previous studies, which utilize embedding vectors with lost spatial information, our module utilizes the feature map for effective cooperation in the STAR setting, which preserves the spatial information. Furthermore, in our module, we divide the self-attention for the same and different views to model the relationship between multiple views effectively. The results of experiments using a newly collected dataset demonstrate that MVAFormer outperforms the comparison baselines by approximately $4.4$ points on the F-measure.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02483v2/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02483v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02483v2">http://arxiv.org/abs/2511.02483v2</a><br>We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at <a href="https://vcai.mpi-inf.mpg.de/projects/OLATverse/">https://vcai.mpi-inf.mpg.de/projects/OLATverse/</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02489v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02489v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02489v1">http://arxiv.org/abs/2511.02489v1</a><br>With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems. However, in GNSS-denied areas, satellite-based localization methods are prone to failure. This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching. In typical pipelines, UAV visual localization is formulated as an image-retrieval problem: features are extracted to build a localization map, and the pose of a query image is estimated by matching it to a reference database with known poses. Because publicly available UAV localization datasets are limited, many approaches recast localization as a classification task and rely on scene labels in these datasets to ensure accuracy. Other methods seek to reduce cross-domain differences using polar-coordinate reprojection, perspective transformations, or generative adversarial networks; however, they can suffer from misalignment, content loss, and limited realism. In contrast, we leverage modern object detection to accurately extract salient instances from UAV and satellite images, and integrate a graph neural network to reason about inter-image and intra-image node relationships. Using a fine-grained, graph-based node-similarity metric, our method achieves strong retrieval and localization performance. Extensive experiments on public and real-world datasets show that our approach handles heterogeneous appearance differences effectively and generalizes well, making it applicable to scenarios with larger modality gaps, such as infrared-visible image matching. Our dataset will be publicly available at the following URL: <a href="https://github.com/liutao23/ODGNNLoc.git">https://github.com/liutao23/ODGNNLoc.git</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02503v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02503v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02503v1">http://arxiv.org/abs/2511.02503v1</a><br>The automation of workflows in advanced microscopy is a key goal where foundation models like Language Models (LLMs) and Vision-Language Models (VLMs) show great potential. However, adapting these general-purpose models for specialized scientific tasks is critical, and the optimal domain adaptation strategy is often unclear. To address this, we introduce PtychoBench, a new multi-modal, multi-task benchmark for ptychographic analysis. Using this benchmark, we systematically compare two specialization strategies: Supervised Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies on a visual artifact detection task with VLMs and a textual parameter recommendation task with LLMs in a data-scarce regime. Our findings reveal that the optimal specialization pathway is task-dependent. For the visual task, SFT and ICL are highly complementary, with a fine-tuned model guided by context-aware examples achieving the highest mean performance (Micro-F1 of 0.728). Conversely, for the textual task, ICL on a large base model is the superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a powerful “super-expert” SFT model (0-shot Micro-F1 of 0.839). We also confirm the superiority of context-aware prompting and identify a consistent contextual interference phenomenon in fine-tuned models. These results, benchmarked against strong baselines including GPT-4o and a DINOv3-based classifier, offer key observations for AI in science: the optimal specialization path in our benchmark is dependent on the task modality, offering a clear framework for developing more effective science-based agentic systems.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02495v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02495v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02495v1">http://arxiv.org/abs/2511.02495v1</a><br>Recent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at <a href="https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890">https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02507v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02507v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02507v1">http://arxiv.org/abs/2511.02507v1</a><br>Recent advancements in Deep Learning enable hardware-based cognitive systems, that is, mechatronic systems in general and robotics in particular with integrated Artificial Intelligence, to interact with dynamic and unstructured environments. While the results are impressive, the application of such systems to critical tasks like autonomous driving as well as service and care robotics necessitate the evaluation of large amount of heterogeneous data. Automated report generation for Mobile Robotics can play a crucial role in facilitating the evaluation and acceptance of such systems in various domains. In this paper, we propose a pipeline for generating automated reports in natural language utilizing various multi-modal sensors that solely relies on local models capable of being deployed on edge computing devices, thus preserving the privacy of all actors involved and eliminating the need for external services. In particular, we evaluate our implementation on a diverse dataset spanning multiple domains including indoor, outdoor and urban environments, providing quantitative as well as qualitative evaluation results. Various generated example reports and other supplementary materials are available via a public repository.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02505v2/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02505v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02505v2">http://arxiv.org/abs/2511.02505v2</a><br>Shot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator’s unique artistic expression in shot assembly. To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: <a href="https://sobeymil.github.io/esa.com">https://sobeymil.github.io/esa.com</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02510v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02510v1">http://arxiv.org/abs/2511.02510v1</a><br>Sparse-voxel rasterization is a fast, differentiable alternative for optimization-based scene reconstruction, but it tends to underfit low-frequency content, depends on brittle pruning heuristics, and can overgrow in ways that inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that makes SV rasterization both steadier and lighter. Our loss is made low-frequency aware via an inverse-Sobel reweighting with a mid-training gamma-ramp, shifting gradient budget to flat regions only after geometry stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning logic on maximum blending weight, stabilized by EMA-hysteresis guards and refines structure through ray-footprint-based, priority-driven subdivision under an explicit growth budget. Ablations and full-system results across Mip-NeRF 360 (6scenes) and Tanks &amp; Temples (3scenes) datasets show mitigation of errors in low-frequency regions and boundary instability while keeping PSNR&#x2F;SSIM, training time, and FPS comparable to a strong SVRaster pipeline. Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency detail that prior setups miss, enabling more predictable, memory-efficient training without sacrificing perceptual quality.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02541v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02541v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02541v1">http://arxiv.org/abs/2511.02541v1</a><br>Shearography is a non-destructive testing method for detecting subsurface defects, offering high sensitivity and full-field inspection capabilities. However, its industrial adoption remains limited due to the need for expert interpretation. To reduce reliance on labeled data and manual evaluation, this study explores unsupervised learning methods for automated anomaly detection in shearographic images. Three architectures are evaluated: a fully connected autoencoder, a convolutional autoencoder, and a student-teacher feature matching model. All models are trained solely on defect-free data. A controlled dataset was developed using a custom specimen with reproducible defect patterns, enabling systematic acquisition of shearographic measurements under both ideal and realistic deformation conditions. Two training subsets were defined: one containing only undistorted, defect-free samples, and one additionally including globally deformed, yet defect-free, data. The latter simulates practical inspection conditions by incorporating deformation-induced fringe patterns that may obscure localized anomalies. The models are evaluated in terms of binary classification and, for the student-teacher model, spatial defect localization. Results show that the student-teacher approach achieves superior classification robustness and enables precise localization. Compared to the autoencoder-based models, it demonstrates improved separability of feature representations, as visualized through t-SNE embeddings. Additionally, a YOLOv8 model trained on labeled defect data serves as a reference to benchmark localization quality. This study underscores the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02558v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02558v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02558v1">http://arxiv.org/abs/2511.02558v1</a><br>Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer’s disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant’s entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>q-bio.NC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02560v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02560v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02560v1">http://arxiv.org/abs/2511.02560v1</a><br>We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at <a href="https://github.com/microsoft/SigmaCollab">https://github.com/microsoft/SigmaCollab</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02563v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02563v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02563v1">http://arxiv.org/abs/2511.02563v1</a><br>This report describes the UVH-26 dataset, the first public release by AIM@IISc of a large-scale dataset of annotated traffic-camera images from India. The dataset comprises 26,646 high-resolution (1080p) images sampled from 2800 Bengaluru’s Safe-City CCTV cameras over a 4-week period, and subsequently annotated through a crowdsourced hackathon involving 565 college students from across India. In total, 1.8 million bounding boxes were labeled across 14 vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler (Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller, Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k consensus ground truth bounding boxes and labels were derived for distinct objects in the 26k images using Majority Voting and STAPLE algorithms. Further, we train multiple contemporary detectors, including YOLO11-S&#x2F;X, RT-DETR-S&#x2F;X, and DAMO-YOLO-T&#x2F;L using these datasets, and report accuracy based on mAP50, mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in mAP50:95 over equivalent baseline models trained on COCO dataset, with RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40 for COCO-trained weights for common classes (Car, Bus, and Truck). This demonstrates the benefits of domain-specific training data for Indian traffic scenarios. The release package provides the 26k images with consensus annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the 6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the heterogeneity of Indian urban mobility directly from operational traffic-camera streams, UVH-26 addresses a critical gap in existing global benchmarks, and offers a foundation for advancing detection, classification, and deployment of intelligent transportation systems in emerging nations with complex traffic conditions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02564v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02564v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02564v1">http://arxiv.org/abs/2511.02564v1</a><br>Video-based person re-identification (ReID) in cross-view domains (for example, aerial-ground surveillance) remains an open problem because of extreme viewpoint shifts, scale disparities, and temporal inconsistencies. To address these challenges, we propose MTF-CVReID, a parameter-efficient framework that introduces seven complementary modules over a ViT-B&#x2F;16 backbone. Specifically, we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment (IVFA) for perspective-invariant representation alignment; (6) Hierarchical Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities; and (7) Multi-View Identity Consistency Learning (MVICL) that enforces cross-view identity coherence using a contrastive learning paradigm. Despite adding only about 2 million parameters and 0.7 GFLOPs over the baseline, MTF-CVReID maintains real-time efficiency (189 FPS) and achieves state-of-the-art performance on the AG-VPReID benchmark across all altitude levels, with strong cross-dataset generalization to G2A-VReID and MARS datasets. These results show that carefully designed adapter-based modules can substantially enhance cross-view robustness and temporal consistency without compromising computational efficiency. The source code is available at <a href="https://github.com/MdRashidunnabi/MTF-CVReID">https://github.com/MdRashidunnabi/MTF-CVReID</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02565v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02565v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02565v1">http://arxiv.org/abs/2511.02565v1</a><br>Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02397v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02397v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02397v1">http://arxiv.org/abs/2511.02397v1</a><br>Color consistency correction for color point clouds is a fundamental yet important task in 3D rendering and compression applications. In the past, most previous color correction methods aimed at correcting color for color images. The purpose of this paper is to propose a grouping-based hybrid color correction algorithm for color point clouds. Our algorithm begins by estimating the overlapping rate between the aligned source and target point clouds, and then adaptively partitions the target points into two groups, namely the close proximity group Gcl and the moderate proximity group Gmod, or three groups, namely Gcl, Gmod, and the distant proximity group Gdist, when the estimated overlapping rate is low or high, respectively. To correct color for target points in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) method is proposed. To correct color for target points in Gmod, a joint KBI and the histogram equalization (JKHE) method is proposed. For target points in Gdist, a histogram equalization (HE) method is proposed for color correction. Finally, we discuss the grouping-effect free property and the ablation study in our algorithm. The desired color consistency correction benefit of our algorithm has been justified through 1086 testing color point cloud pairs against the state-of-the-art methods. The C++ source code of our algorithm can be accessed from the website: <a href="https://github.com/ivpml84079/Point-cloud-color-correction">https://github.com/ivpml84079/Point-cloud-color-correction</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02576v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02576v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02576v1">http://arxiv.org/abs/2511.02576v1</a><br>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over&#x2F;under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: <a href="https://gitlab.inria.fr/adelangl/SCORE">https://gitlab.inria.fr/adelangl/SCORE</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zero-Shot Multi-Animal Tracking in the Wild</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02591v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02591v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02591v1">http://arxiv.org/abs/2511.02591v1</a><br>Multi-animal tracking is crucial for understanding animal ecology and behavior. However, it remains a challenging task due to variations in habitat, motion patterns, and species appearance. Traditional approaches typically require extensive model fine-tuning and heuristic design for each application scenario. In this work, we explore the potential of recent vision foundation models for zero-shot multi-animal tracking. By combining a Grounding Dino object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully designed heuristics, we develop a tracking framework that can be applied to new datasets without any retraining or hyperparameter adaptation. Evaluations on ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate strong and consistent performance across diverse species and environments. The code is available at <a href="https://github.com/ecker-lab/SAM2-Animal-Tracking">https://github.com/ecker-lab/SAM2-Animal-Tracking</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02580v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02580v1">http://arxiv.org/abs/2511.02580v1</a><br>Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniChange: Unifying Change Detection with Multimodal Large Language Model</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02607v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02607v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02607v1">http://arxiv.org/abs/2511.02607v1</a><br>Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at <a href="https://github.com/Erxucomeon/UniChange">https://github.com/Erxucomeon/UniChange</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Robust Face Liveness Detection for Biometric Authentication using Single Image</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02645v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02645v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02645v1">http://arxiv.org/abs/2511.02645v1</a><br>Biometric technologies are widely adopted in security, legal, and financial systems. Face recognition can authenticate a person based on the unique facial features such as shape and texture. However, recent works have demonstrated the vulnerability of Face Recognition Systems (FRS) towards presentation attacks. Using spoofing (aka.,presentation attacks), a malicious actor can get illegitimate access to secure systems. This paper proposes a novel light-weight CNN framework to identify print&#x2F;display, video and wrap attacks. The proposed robust architecture provides seamless liveness detection ensuring faster biometric authentication (1-2 seconds on CPU). Further, this also presents a newly created 2D spoof attack dataset consisting of more than 500 videos collected from 60 subjects. To validate the effectiveness of this architecture, we provide a demonstration video depicting print&#x2F;display, video and wrap attack detection approaches. The demo can be viewed in the following link: <a href="https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa">https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02650v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02650v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02650v1">http://arxiv.org/abs/2511.02650v1</a><br>Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Differentiable Hierarchical Visual Tokenization</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02652v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02652v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02652v1">http://arxiv.org/abs/2511.02652v1</a><br>Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T45</tag>
      
      <tag>I.2.10; I.4.10; I.4.6; I.3.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02685v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02685v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02685v1">http://arxiv.org/abs/2511.02685v1</a><br>Visible-infrared person re-identification (VI-ReID) technique could associate the pedestrian images across visible and infrared modalities in the practical scenarios of background illumination changes. However, a substantial gap inherently exists between these two modalities. Besides, existing methods primarily rely on intermediate representations to align cross-modal features of the same person. The intermediate feature representations are usually create by generating intermediate images (kind of data enhancement), or fusing intermediate features (more parameters, lack of interpretability), and they do not make good use of the intermediate features. Thus, we propose a novel VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a middle generated image as a transmitter from visible to infrared modals, which are fully aligned with the original visible images and similar to the infrared modality. After that, using a modality-transition contrastive loss and a modality-query regularization loss for training, which could align the cross-modal features more effectively. Notably, our proposed framework does not need any additional parameters, which achieves the same inference speed to the backbone while improving its performance on VI-ReID task. Extensive experimental results illustrate that our model significantly and consistently outperforms existing SOTAs on three typical VI-ReID datasets.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02712v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02712v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02712v1">http://arxiv.org/abs/2511.02712v1</a><br>Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02717v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02717v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02717v1">http://arxiv.org/abs/2511.02717v1</a><br>The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>68T05 (Learning and adaptive systems)</tag>
      
      <tag>eess.AS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLEXICORP: End-user Explainability of Convolutional Neural Networks</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02720v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02720v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02720v1">http://arxiv.org/abs/2511.02720v1</a><br>Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.   To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.   We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamic Reflections: Probing Video Representations with Text Alignment</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02767v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02767v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02767v1">http://arxiv.org/abs/2511.02767v1</a><br>The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at <a href="https://video-prh.github.io/">https://video-prh.github.io/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02778v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02778v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02778v1">http://arxiv.org/abs/2511.02778v1</a><br>Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model’s intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at <a href="https://github.com/CSU-JPG/VCode">https://github.com/CSU-JPG/VCode</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02777v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02777v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02777v1">http://arxiv.org/abs/2511.02777v1</a><br>We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.   Project Page: <a href="https://antoniooroz.github.io/PercHead">https://antoniooroz.github.io/PercHead</a> Video: <a href="https://www.youtube.com/watch?v=4hFybgTk4kE">https://www.youtube.com/watch?v=4hFybgTk4kE</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI-Generated Image Detection: An Empirical Study and Future Research Directions</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02791v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02791v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02791v1">http://arxiv.org/abs/2511.02791v1</a><br>The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PLUTO-4: Frontier Pathology Foundation Models</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02826v2/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02826v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02826v2">http://arxiv.org/abs/2511.02826v2</a><br>Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4’s potential to transform real-world applications as a backbone for translational research and diagnostic use cases.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02830v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02830v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02830v1">http://arxiv.org/abs/2511.02830v1</a><br>We propose DenseMarks - a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube. In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of-the-art point tracker over a collection of diverse in-the-wild talking heads videos, and guide the mapping via a contrastive loss, encouraging matched points to have close embeddings. We further employ multi-task learning with face landmarks and segmentation constraints, as well as imposing spatial continuity of embeddings through latent cube features, which results in an interpretable and queryable canonical space. The representation can be used for finding common semantic parts, face&#x2F;head tracking, and stereo reconstruction. Due to the strong supervision, our method is robust to pose variations and covers the entire head, including hair. Additionally, the canonical space bottleneck makes sure the obtained representations are consistent across diverse poses and individuals. We demonstrate state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models. The code and the model checkpoint will be made available to the public.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02779v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02779v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02779v1">http://arxiv.org/abs/2511.02779v1</a><br>We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through “drawing to think”. To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02832v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02832v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02832v1">http://arxiv.org/abs/2511.02832v1</a><br>Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at <a href="https://yanjieze.com/TWIST2">https://yanjieze.com/TWIST2</a> . Our collected dataset is also open-sourced at <a href="https://twist-data.github.io/">https://twist-data.github.io</a> .</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NEF-NET+: Adapting Electrocardio panorama in the wild</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02880v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02880v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02880v1">http://arxiv.org/abs/2511.02880v1</a><br>Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and compensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Optimizing the nnU-Net model for brain tumor (Glioma) segmentation Using a BraTS Sub-Saharan Africa (SSA) dataset</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02893v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02893v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02893v1">http://arxiv.org/abs/2511.02893v1</a><br>Medical image segmentation is a critical achievement in modern medical science, developed over decades of research. It allows for the exact delineation of anatomical and pathological features in two- or three-dimensional pictures by utilizing notions like pixel intensity, texture, and anatomical context. With the advent of automated segmentation, physicians and radiologists may now concentrate on diagnosis and treatment planning while intelligent computers perform routine image processing tasks.   This study used the BraTS Sub-Saharan Africa dataset, a selected subset of the BraTS dataset that included 60 multimodal MRI cases from patients with glioma. Surprisingly, the nnU Net model trained on the initial 60 instances performed better than the network trained on an offline-augmented dataset of 360 cases. Hypothetically, the offline augmentations introduced artificial anatomical variances or intensity distributions, reducing generalization. In contrast, the original dataset, when paired with nnU Net’s robust online augmentation procedures, maintained realistic variability and produced better results. The study achieved a Dice score of 0.84 for whole tumor segmentation. These findings highlight the significance of data quality and proper augmentation approaches in constructing accurate, generalizable medical picture segmentation models, particularly for under-represented locations.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cropland Mapping using Geospatial Embeddings</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02923v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02923v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02923v1">http://arxiv.org/abs/2511.02923v1</a><br>Accurate and up-to-date land cover maps are essential for understanding land use change, a key driver of climate change. Geospatial embeddings offer a more efficient and accessible way to map landscape features, yet their use in real-world mapping applications remains underexplored. In this work, we evaluated the utility of geospatial embeddings for cropland mapping in Togo. We produced cropland maps using embeddings from Presto and AlphaEarth. Our findings show that geospatial embeddings can simplify workflows, achieve high-accuracy cropland classification and ultimately support better assessments of land use change and its climate impacts.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02928v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02928v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02928v1">http://arxiv.org/abs/2511.02928v1</a><br>Glioma segmentation is critical for diagnosis and treatment planning, yet remains challenging in Sub-Saharan Africa due to limited MRI infrastructure and heterogeneous acquisition protocols that induce severe domain shift. We propose SegFormer3D-plus, a radiomics-guided transformer architecture designed for robust segmentation under domain variability. Our method combines: (1) histogram matching for intensity harmonization across scanners, (2) radiomic feature extraction with PCA-reduced k-means for domain-aware stratified sampling, (3) a dual-pathway encoder with frequency-aware feature extraction and spatial-channel attention, and (4) composite Dice-Cross-Entropy loss for boundary refinement. Pretrained on BraTS 2023 and fine-tuned on BraTS-Africa data, SegFormer3D-plus demonstrates improved tumor subregion delineation and boundary localization across heterogeneous African clinical scans, highlighting the value of radiomics-guided domain adaptation for resource-limited settings.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
      <tag>I.2.10; I.4.8; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Hints</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02933v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02933v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02933v1">http://arxiv.org/abs/2511.02933v1</a><br>Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02946v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02946v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02946v1">http://arxiv.org/abs/2511.02946v1</a><br>We introduce ProM3E, a probabilistic masked multimodal embedding model for any-to-any generation of multimodal representations for ecology. ProM3E is based on masked modality reconstruction in the embedding space, learning to infer missing modalities given a few context modalities. By design, our model supports modality inversion in the embedding space. The probabilistic nature of our model allows us to analyse the feasibility of fusing various modalities for given downstream tasks, essentially learning what to fuse. Using these features of our model, we propose a novel cross-modal retrieval approach that mixes inter-modal and intra-modal similarities to achieve superior performance across all retrieval tasks. We further leverage the hidden representation from our model to perform linear probing tasks and demonstrate the superior representation learning capability of our model. All our code, datasets and model will be released at <a href="https://vishu26.github.io/prom3e">https://vishu26.github.io/prom3e</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02953v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02953v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02953v1">http://arxiv.org/abs/2511.02953v1</a><br>Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model’s ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02992v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02992v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02992v1">http://arxiv.org/abs/2511.02992v1</a><br>Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02996v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02996v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02996v1">http://arxiv.org/abs/2511.02996v1</a><br>Vision-language models (VLMs) have demonstrated strong cross-modal capabilities, yet most work remains limited to 2D data and assumes binary supervision (i.e., positive vs. negative pairs), overlooking the continuous and structured dependencies present in volumetric data such as CT. Existing approaches often treat volumetric scans as independent 2D slices, compromising spatial coherence and underutilizing rich clinical semantics. We propose SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework that integrates (i) volumetric spatial semantics to preserve anatomical structure and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide alignment. This yields structurally consistent and semantically grounded representations under limited supervision, demonstrating strong cross-task transferability (retrieval, report generation, and classification), and cross-domain generalizability with consistent gains without further fine-tuning. In particular, compared to the previous state of the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval, improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an out-of-domain external dataset, we observe consistent gains, indicating the cross-task and cross-domain generalization ability of SCALE-VLP.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_02994v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_02994v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02994v1">http://arxiv.org/abs/2511.02994v1</a><br>For developing safe Autonomous Driving Systems (ADS), rigorous testing is required before they are deemed safe for road deployments. Since comprehensive conventional physical testing is impractical due to cost and safety concerns, Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing VTE-generated sensor outputs against their real-world analogues can be a strong indication that the VTE accurately represents reality. Correspondingly, this work explores a comprehensive experimental approach to finding evaluation metrics suitable for comparing real-world and simulated LiDAR scans. The metrics were tested in terms of sensitivity and accuracy with different noise, density, distortion, sensor orientation, and channel settings. From comparing the metrics, we found that Density Aware Chamfer Distance (DCD) works best across all cases. In the second step of the research, a Virtual Testing Environment was generated using real LiDAR scan data. The data was collected in a controlled environment with only static objects using an instrumented vehicle equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans were compared in terms of model perception and geometric similarity. Actual and simulated LiDAR scans have a similar semantic segmentation output with a mIoU of 21% with corrected intensity and an average density aware chamfer distance (DCD) of 0.63. This indicates a slight difference in the geometric properties of simulated and real LiDAR scans and a significant difference between model outputs. During the comparison, density-aware chamfer distance was found to be the most correlated among the metrics with perception methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_03004v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_03004v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03004v1">http://arxiv.org/abs/2511.03004v1</a><br>Deep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the “Bootstrap Your Own Latent” pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Foundation Model for Brain MRI with Dynamic Modality Integration</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_03014v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_03014v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03014v1">http://arxiv.org/abs/2511.03014v1</a><br>We present a foundation model for brain MRI that can work with different combinations of imaging sequences. The model uses one encoder with learnable modality embeddings, conditional layer normalization, and a masked autoencoding objective that accounts for missing modalities. A variance-covariance regularizer is applied to stabilize feature learning and improve representation diversity. This design removes the need for separate models for each modality and allows the network to adapt when some sequences are missing or unseen. It is trained on about 60,000 multi-center MRIs using self-supervised reconstruction and modality imputation to learn flexible representations. A learnable modality embedding guides feature extraction so the encoder can adjust to different inputs. We describe our planned evaluation on brain tumor and multiple sclerosis segmentation, as well as lesion classification, under various modality settings. Preliminary results show that the method works feasibly, and further experiments are planned to study its performance in more detail. All code and pretrained models are available at <a href="https://github.com/BrainFM/brainfm">https://github.com/BrainFM/brainfm</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_03019v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_03019v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03019v1">http://arxiv.org/abs/2511.03019v1</a><br>Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data-Efficient Realized Volatility Forecasting with Vision Transformers</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_03046v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_03046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03046v1">http://arxiv.org/abs/2511.03046v1</a><br>Recent work in financial machine learning has shown the virtue of complexity: the phenomenon by which deep learning methods capable of learning highly nonlinear relationships outperform simpler approaches in financial forecasting. While transformer architectures like Informer have shown promise for financial time series forecasting, the application of transformer models for options data remains largely unexplored. We conduct preliminary studies towards the development of a transformer model for options data by training the Vision Transformer (ViT) architecture, typically used in modern image recognition and classification systems, to predict the realized volatility of an asset over the next 30 days from its implied volatility surface (augmented with date information) for a single day. We show that the ViT can learn seasonal patterns and nonlinear features from the IV surface, suggesting a promising direction for model development.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>I.4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth</title>
    <link href="/2025/11/04/highlights/2025-11-04-2511_03053v1/"/>
    <url>/2025/11/04/highlights/2025-11-04-2511_03053v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03053v1">http://arxiv.org/abs/2511.03053v1</a><br>Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds’ uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01140v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01140v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01140v1">http://arxiv.org/abs/2511.01140v1</a><br>Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01143v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01143v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01143v1">http://arxiv.org/abs/2511.01143v1</a><br>Early and accurate segmentation of colorectal polyps is critical for reducing colorectal cancer mortality, which has been extensively explored by academia and industry. However, current deep learning-based polyp segmentation models either compromise clinical decision-making by providing ambiguous polyp margins in segmentation outputs or rely on heavy architectures with high computational complexity, resulting in insufficient inference speeds for real-time colorectal endoscopic applications. To address this problem, we propose MicroAUNet, a light-weighted attention-based segmentation network that combines depthwise-separable dilated convolutions with a single-path, parameter-shared channel-spatial attention block to strengthen multi-scale boundary features. On the basis of it, a progressive two-stage knowledge-distillation scheme is introduced to transfer semantic and boundary cues from a high-capacity teacher. Extensive experiments on benchmarks also demonstrate the state-of-the-art accuracy under extremely low model complexity, indicating that MicroAUNet is suitable for real-time clinical polyp segmentation. The code is publicly available at <a href="https://github.com/JeremyXSC/MicroAUNet">https://github.com/JeremyXSC/MicroAUNet</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Architectures for Human Activity Recognition</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01139v2/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01139v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01139v2">http://arxiv.org/abs/2511.01139v2</a><br>We propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. We introduce a symmetry category that jointly represents cyclic time shifts, positive gain scalings, and the sensor-hierarchy poset, capturing the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01194v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01194v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01194v1">http://arxiv.org/abs/2511.01194v1</a><br>Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07 (Artificial neural networks and deep learning), 68U10
  (Computer graphics, computational geometry)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01213v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01213v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01213v1">http://arxiv.org/abs/2511.01213v1</a><br>The immense diversity in the culture and culinary of Indian cuisines calls attention to the major shortcoming of the existing Visual Question Answering(VQA) systems which are inclined towards the foods from Western region. Recent attempt towards building a VQA dataset for Indian food is a step towards addressing this challenge. However, their approach towards VQA follows a two-step process in which the answer is generated first, followed by the explanation of the expected answer. In this work, we claim that food VQA requires to follow a multi-step reasoning process to arrive at an accurate answer, especially in the context of India food, which involves understanding complex culinary context and identifying relationships between various food items. With this hypothesis we create reasoning chains upon the QA with minimal human intervention. We fine-tune smaller LLMs and VLMs with auto-validated reasoning chains and further train them using reinforcement learning with larger data. With augmentation of reasoning chains, we observed accuracy improvement of an average 10 percentage points on the baseline. We provide detailed analysis in terms the effect of addition of reasoning chains for the Indian Food VQA task.   Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge Graph.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Eyes on Target: Gaze-Aware Object Detection in Egocentric Video</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01237v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01237v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01237v1">http://arxiv.org/abs/2511.01237v1</a><br>Human gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01284v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01284v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01284v1">http://arxiv.org/abs/2511.01284v1</a><br>Foundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01307v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01307v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01307v1">http://arxiv.org/abs/2511.01307v1</a><br>Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at <a href="https://github.com/KU-VGI/APDM">https://github.com/KU-VGI/APDM</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01357v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01357v1">http://arxiv.org/abs/2511.01357v1</a><br>Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model’s capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at <a href="https://github.com/BioMedIA-repo/CMI-MTL">https://github.com/BioMedIA-repo/CMI-MTL</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01390v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01390v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01390v1">http://arxiv.org/abs/2511.01390v1</a><br>Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23%-86% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at <a href="https://github.com/Sweet4tars/seps.git">https://github.com/Sweet4tars/seps.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01425v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01425v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01425v1">http://arxiv.org/abs/2511.01425v1</a><br>Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18% compared to a non-interactive baseline. To validate the faithfulness of the agent’s explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\Delta$Brier&#x3D;+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.6; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniSOT: A Unified Framework for Multi-Modality Single Object Tracking</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01427v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01427v1">http://arxiv.org/abs/2511.01427v1</a><br>Single object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0% main metric across all three RGB+X video modalities.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01449v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01449v1">http://arxiv.org/abs/2511.01449v1</a><br>To effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.   Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01450v2/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01450v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01450v2">http://arxiv.org/abs/2511.01450v2</a><br>Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO loss to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01458v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01458v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01458v1">http://arxiv.org/abs/2511.01458v1</a><br>Safety and reliability are essential for deploying Visual Question Answering (VQA) in surgery, where incorrect or ambiguous responses can harm the patient. Most surgical VQA research focuses on accuracy or linguistic quality while overlooking safety behaviors such as ambiguity awareness, referral to human experts, or triggering a second opinion. Inspired by Automatic Failure Detection (AFD), we study uncertainty estimation as a key enabler of safer decision making. We introduce Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question semantics into prediction confidence. It measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. We evaluate five models, including domain specific Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models degrade under mild paraphrasing, while LVLMs are more resilient. Across three LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template settings and enhances hallucination detection. The Area Under the ROC Curve (AUROC) increases by 15-38% for zero-shot models, with gains maintained under out-of-template stress. QA-SNNE offers a practical and interpretable step toward AFD in surgical VQA by linking semantic uncertainty to question context. Combining LVLM backbones with question aligned uncertainty estimation can improve safety and clinician trust. The code and model are available at <a href="https://github.com/DennisPierantozzi/QASNNE">https://github.com/DennisPierantozzi/QASNNE</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Efficiently Training A Flat Neural Network Before It has been Quantizated</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01462v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01462v1">http://arxiv.org/abs/2511.01462v1</a><br>Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error for PTQ. However, it is unclear how to efficiently train a model-agnostic neural network which is tailored for a predefined precision low-bit model. In this paper, we firstly discover that a flat full precision neural network is crucial for low-bit quantization. To achieve this, we propose a framework that proactively pre-conditions the model by measuring and disentangling the error sources. Specifically, both the Activation Quantization Error (AQE) and the Weight Quantization Error (WQE) are statistically modeled as independent Gaussian noises. We study several noise injection optimization methods to obtain a flat minimum. Experimental results attest to the effectiveness of our approach. These results open novel pathways for obtaining low-bit PTQ models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01463v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01463v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01463v1">http://arxiv.org/abs/2511.01463v1</a><br>The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
      <tag>68T45</tag>
      
      <tag>I.2.10; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Driving scenario generation and evaluation using a structured layer representation and foundational models</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01541v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01541v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01541v1">http://arxiv.org/abs/2511.01541v1</a><br>Rare and challenging driving scenarios are critical for autonomous vehicle development. Since they are difficult to encounter, simulating or generating them using generative models is a popular approach. Following previous efforts to structure driving scenario representations in a layer model, we propose a structured five-layer model to improve the evaluation and generation of rare scenarios. We use this model alongside large foundational models to generate new driving scenarios using a data augmentation strategy. Unlike previous representations, our structure introduces subclasses and characteristics for every agent of the scenario, allowing us to compare them using an embedding specific to our layer-model. We study and adapt two metrics to evaluate the relevance of a synthetic dataset in the context of a structured representation: the diversity score estimates how different the scenarios of a dataset are from one another, while the originality score calculates how similar a synthetic dataset is from a real reference set. This paper showcases both metrics in different generation setup, as well as a qualitative evaluation of synthetic videos generated from structured scenario descriptions. The code and extended results can be found at <a href="https://github.com/Valgiz/5LMSG">https://github.com/Valgiz/5LMSG</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01610v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01610v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01610v1">http://arxiv.org/abs/2511.01610v1</a><br>Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01775v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01775v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01775v1">http://arxiv.org/abs/2511.01775v1</a><br>Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct “plausibility gap”: while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fractional Diffusion Bridge Models</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01795v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01795v1">http://arxiv.org/abs/2511.01795v1</a><br>We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr&quot;{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fr&#39;echet Inception Distance (FID) in unpaired image translation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_02046v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_02046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02046v1">http://arxiv.org/abs/2511.02046v1</a><br>Creation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image</title>
    <link href="/2025/11/03/cs.AI/2025-11-03-2511_01767v1/"/>
    <url>/2025/11/03/cs.AI/2025-11-03-2511_01767v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01767v1">http://arxiv.org/abs/2511.01767v1</a><br>In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at <a href="https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus">https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$&#92;left|&#92;,&#92;circlearrowright&#92;,&#92;boxed{&#92;text{BUS}}&#92;,&#92;right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles</title>
    <link href="/2025/11/03/cs.CL/2025-11-03-2511_01340v1/"/>
    <url>/2025/11/03/cs.CL/2025-11-03-2511_01340v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01340v1">http://arxiv.org/abs/2511.01340v1</a><br>Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$, a large and diverse benchmark of $1,333$ English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$ by $2.1-4.1%$ and $20-30%$ using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</title>
    <link href="/2025/11/03/cs.CL/2025-11-03-2511_01618v1/"/>
    <url>/2025/11/03/cs.CL/2025-11-03-2511_01618v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01618v1">http://arxiv.org/abs/2511.01618v1</a><br>Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Boosting performance of computer vision applications through embedded GPUs on the edge</title>
    <link href="/2025/11/03/cs.DC/2025-11-03-2511_01129v1/"/>
    <url>/2025/11/03/cs.DC/2025-11-03-2511_01129v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01129v1">http://arxiv.org/abs/2511.01129v1</a><br>Computer vision applications, especially those using augmented reality technology, are becoming quite popular in mobile devices. However, this type of application is known as presenting significant demands regarding resources. In order to enable its utilization in devices with more modest resources, edge computing can be used to offload certain high intensive tasks. Still, edge computing is usually composed of devices with limited capacity, which may impact in users quality of experience when using computer vision applications. This work proposes the use of embedded devices with graphics processing units (GPUs) to overcome such limitation. Experiments performed shown that GPUs can attain a performance gain when compared to using only CPUs, which guarantee a better experience to users using such kind of application.</p>]]></content>
    
    
    <categories>
      
      <category>cs.DC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark</title>
    <link href="/2025/11/03/cs.GR/2025-11-03-2511_01233v1/"/>
    <url>/2025/11/03/cs.GR/2025-11-03-2511_01233v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01233v1">http://arxiv.org/abs/2511.01233v1</a><br>We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models – each trained by its original authors – across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies – enabling new evaluations without model reimplementation required – alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.GR</tag>
      
      <tag>I.3; I.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</title>
    <link href="/2025/11/03/cs.GR/2025-11-03-2511_01463v1/"/>
    <url>/2025/11/03/cs.GR/2025-11-03-2511_01463v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01463v1">http://arxiv.org/abs/2511.01463v1</a><br>The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
      <tag>68T45</tag>
      
      <tag>I.2.10; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Example-Based Feature Painting on Textures</title>
    <link href="/2025/11/03/cs.GR/2025-11-03-2511_01513v1/"/>
    <url>/2025/11/03/cs.GR/2025-11-03-2511_01513v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01513v1">http://arxiv.org/abs/2511.01513v1</a><br>In this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: <a href="https://reality.tf.fau.de/pub/ardelean2025examplebased.html">https://reality.tf.fau.de/pub/ardelean2025examplebased.html</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Architectures for Human Activity Recognition</title>
    <link href="/2025/11/03/cs.HC/2025-11-03-2511_01139v2/"/>
    <url>/2025/11/03/cs.HC/2025-11-03-2511_01139v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01139v2">http://arxiv.org/abs/2511.01139v2</a><br>We propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. We introduce a symmetry category that jointly represents cyclic time shifts, positive gain scalings, and the sensor-hierarchy poset, capturing the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark</title>
    <link href="/2025/11/03/cs.HC/2025-11-03-2511_01233v1/"/>
    <url>/2025/11/03/cs.HC/2025-11-03-2511_01233v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01233v1">http://arxiv.org/abs/2511.01233v1</a><br>We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models – each trained by its original authors – across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies – enabling new evaluations without model reimplementation required – alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.GR</tag>
      
      <tag>I.3; I.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers</title>
    <link href="/2025/11/03/cs.IR/2025-11-03-2511_01617v1/"/>
    <url>/2025/11/03/cs.IR/2025-11-03-2511_01617v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01617v1">http://arxiv.org/abs/2511.01617v1</a><br>In the retrieval domain, candidates’ fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates’ representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM’s prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) &#x2F; 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: <a href="https://github.com/mohammad2012191/ViC">https://github.com/mohammad2012191/ViC</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.IR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Architectures for Human Activity Recognition</title>
    <link href="/2025/11/03/cs.LG/2025-11-03-2511_01139v2/"/>
    <url>/2025/11/03/cs.LG/2025-11-03-2511_01139v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01139v2">http://arxiv.org/abs/2511.01139v2</a><br>We propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. We introduce a symmetry category that jointly represents cyclic time shifts, positive gain scalings, and the sensor-hierarchy poset, capturing the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
    <link href="/2025/11/03/cs.LG/2025-11-03-2511_01140v1/"/>
    <url>/2025/11/03/cs.LG/2025-11-03-2511_01140v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01140v1">http://arxiv.org/abs/2511.01140v1</a><br>Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Extremal Contours: Gradient-driven contours for compact visual attribution</title>
    <link href="/2025/11/03/cs.LG/2025-11-03-2511_01411v1/"/>
    <url>/2025/11/03/cs.LG/2025-11-03-2511_01411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01411v1">http://arxiv.org/abs/2511.01411v1</a><br>Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve&#x2F;delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MotionStream: Real-Time Video Generation with Interactive Motion Controls</title>
    <link href="/2025/11/03/cs.LG/2025-11-03-2511_01266v1/"/>
    <url>/2025/11/03/cs.LG/2025-11-03-2511_01266v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01266v1">http://arxiv.org/abs/2511.01266v1</a><br>Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization</title>
    <link href="/2025/11/03/cs.LG/2025-11-03-2511_01588v1/"/>
    <url>/2025/11/03/cs.LG/2025-11-03-2511_01588v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01588v1">http://arxiv.org/abs/2511.01588v1</a><br>Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Probabilistic Robustness for Free? Revisiting Training via a Benchmark</title>
    <link href="/2025/11/03/cs.LG/2025-11-03-2511_01724v1/"/>
    <url>/2025/11/03/cs.LG/2025-11-03-2511_01724v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01724v1">http://arxiv.org/abs/2511.01724v1</a><br>Deep learning models are notoriously vulnerable to imperceptible perturbations. Most existing research centers on adversarial robustness (AR), which evaluates models under worst-case scenarios by examining the existence of deterministic adversarial examples (AEs). In contrast, probabilistic robustness (PR) adopts a statistical perspective, measuring the probability that predictions remain correct under stochastic perturbations. While PR is widely regarded as a practical complement to AR, dedicated training methods for improving PR are still relatively underexplored, albeit with emerging progress. Among the few PR-targeted training methods, we identify three limitations: i non-comparable evaluation protocols; ii limited comparisons to strong AT baselines despite anecdotal PR gains from AT; and iii no unified framework to compare the generalization of these methods. Thus, we introduce PRBench, the first benchmark dedicated to evaluating improvements in PR achieved by different robustness training methods. PRBench empirically compares most common AT and PR-targeted training methods using a comprehensive set of metrics, including clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). We also provide theoretical analysis on the GE of PR performance across different training methods. Main findings revealed by PRBench include: AT methods are more versatile than PR-targeted training methods in terms of improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower GE and higher clean accuracy. A leaderboard comprising 222 trained models across 7 datasets and 10 model architectures is publicly available at <a href="https://tmpspace.github.io/PRBenchLeaderboard/">https://tmpspace.github.io/PRBenchLeaderboard/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fractional Diffusion Bridge Models</title>
    <link href="/2025/11/03/cs.LG/2025-11-03-2511_01795v1/"/>
    <url>/2025/11/03/cs.LG/2025-11-03-2511_01795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01795v1">http://arxiv.org/abs/2511.01795v1</a><br>We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr&quot;{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fr&#39;echet Inception Distance (FID) in unpaired image translation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</title>
    <link href="/2025/11/03/cs.MM/2025-11-03-2511_01390v1/"/>
    <url>/2025/11/03/cs.MM/2025-11-03-2511_01390v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01390v1">http://arxiv.org/abs/2511.01390v1</a><br>Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23%-86% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at <a href="https://github.com/Sweet4tars/seps.git">https://github.com/Sweet4tars/seps.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</title>
    <link href="/2025/11/03/cs.MM/2025-11-03-2511_01775v1/"/>
    <url>/2025/11/03/cs.MM/2025-11-03-2511_01775v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01775v1">http://arxiv.org/abs/2511.01775v1</a><br>Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct “plausibility gap”: while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Locally-Supervised Global Image Restoration</title>
    <link href="/2025/11/03/cs.NA/2025-11-03-2511_01998v1/"/>
    <url>/2025/11/03/cs.NA/2025-11-03-2511_01998v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01998v1">http://arxiv.org/abs/2511.01998v1</a><br>We address the problem of image reconstruction from incomplete measurements, encompassing both upsampling and inpainting, within a learning-based framework. Conventional supervised approaches require fully sampled ground truth data, while self-supervised methods allow incomplete ground truth but typically rely on random sampling that, in expectation, covers the entire image. In contrast, we consider fixed, deterministic sampling patterns with inherently incomplete coverage, even in expectation. To overcome this limitation, we exploit multiple invariances of the underlying image distribution, which theoretically allows us to achieve the same reconstruction performance as fully supervised approaches. We validate our method on optical-resolution image upsampling in photoacoustic microscopy (PAM), demonstrating competitive or superior results while requiring substantially less ground truth data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.NA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01186v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01186v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01186v1">http://arxiv.org/abs/2511.01186v1</a><br>Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01210v2/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01210v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01210v2">http://arxiv.org/abs/2511.01210v2</a><br>Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01223v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01223v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01223v1">http://arxiv.org/abs/2511.01223v1</a><br>Domain adaptation is required for automated driving models to generalize well across diverse road conditions. This paper explores a training method for domain adaptation to adapt PilotNet, an end-to-end deep learning-based model, for left-hand driving conditions using real-world Australian highway data. Four training methods were evaluated: (1) a baseline model trained on U.S. right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a model pretrained on flipped U.S. data and then finetuned on Australian highways. This setup examines whether incorporating flipped data enhances the model adaptation by providing an initial left-hand driving alignment. The paper compares model performance regarding steering prediction accuracy and attention, using saliency-based analysis to measure attention shifts across significant road regions. Results show that pretraining on flipped data alone worsens prediction stability due to misaligned feature representations, but significantly improves adaptation when followed by fine-tuning, leading to lower prediction error and stronger focus on left-side cues. To validate this approach across different architectures, the same experiments were done on ResNet, which confirmed similar adaptation trends. These findings emphasize the importance of preprocessing techniques, such as flipped-data pretraining, followed by fine-tuning to improve model adaptation with minimal retraining requirements.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01294v2/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01294v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01294v2">http://arxiv.org/abs/2511.01294v2</a><br>A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EREBUS: End-to-end Robust Event Based Underwater Simulation</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01381v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01381v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01381v1">http://arxiv.org/abs/2511.01381v1</a><br>The underwater domain presents a vast array of challenges for roboticists and computer vision researchers alike, such as poor lighting conditions and high dynamic range scenes. In these adverse conditions, traditional vision techniques struggle to adapt and lead to suboptimal performance. Event-based cameras present an attractive solution to this problem, mitigating the issues of traditional cameras by tracking changes in the footage on a frame-by-frame basis. In this paper, we introduce a pipeline which can be used to generate realistic synthetic data of an event-based camera mounted to an AUV (Autonomous Underwater Vehicle) in an underwater environment for training vision models. We demonstrate the effectiveness of our pipeline using the task of rock detection with poor visibility and suspended particulate matter, but the approach can be generalized to other underwater tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01501v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01501v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01501v1">http://arxiv.org/abs/2511.01501v1</a><br>Object pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01502v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01502v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01502v1">http://arxiv.org/abs/2511.01502v1</a><br>Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group&#x2F;DiMoDE upon publication.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01571v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01571v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01571v1">http://arxiv.org/abs/2511.01571v1</a><br>Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01718v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01718v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01718v1">http://arxiv.org/abs/2511.01718v1</a><br>Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act – reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at <a href="https://irpn-eai.github.io/UD-VLA.github.io/">https://irpn-eai.github.io/UD-VLA.github.io/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01594v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01594v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01594v1">http://arxiv.org/abs/2511.01594v1</a><br>Multimodal large language models (MLLMs) have shown remarkable capabilities in cross-modal understanding and reasoning, offering new opportunities for intelligent assistive systems, yet existing systems still struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic System powered by MLLMs for assistive intelligence and designed for smart home robots supporting people with disabilities. The system integrates four agents: a visual perception agent for extracting semantic and spatial features from environment images, a risk assessment agent for identifying and prioritizing hazards, a planning agent for generating executable action sequences, and an evaluation agent for iterative optimization. By combining multimodal perception with hierarchical multi-agent decision-making, the framework enables adaptive, risk-aware, and personalized assistance in dynamic indoor environments. Experiments on multiple datasets demonstrate the superior overall performance of the proposed system in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models. The proposed approach also highlights the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>I.2.9; I.2.11; I.2.6; I.4.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3EED: Ground Everything Everywhere in 3D</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01755v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01755v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01755v1">http://arxiv.org/abs/2511.01755v1</a><br>Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes – 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fractional Diffusion Bridge Models</title>
    <link href="/2025/11/03/cs.RO/2025-11-03-2511_01795v1/"/>
    <url>/2025/11/03/cs.RO/2025-11-03-2511_01795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01795v1">http://arxiv.org/abs/2511.01795v1</a><br>We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr&quot;{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fr&#39;echet Inception Distance (FID) in unpaired image translation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Boosting performance of computer vision applications through embedded GPUs on the edge</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01129v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01129v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01129v1">http://arxiv.org/abs/2511.01129v1</a><br>Computer vision applications, especially those using augmented reality technology, are becoming quite popular in mobile devices. However, this type of application is known as presenting significant demands regarding resources. In order to enable its utilization in devices with more modest resources, edge computing can be used to offload certain high intensive tasks. Still, edge computing is usually composed of devices with limited capacity, which may impact in users quality of experience when using computer vision applications. This work proposes the use of embedded devices with graphics processing units (GPUs) to overcome such limitation. Experiments performed shown that GPUs can attain a performance gain when compared to using only CPUs, which guarantee a better experience to users using such kind of application.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01131v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01131v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01131v1">http://arxiv.org/abs/2511.01131v1</a><br>Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01140v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01140v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01140v1">http://arxiv.org/abs/2511.01140v1</a><br>Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Architectures for Human Activity Recognition</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01139v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01139v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01139v2">http://arxiv.org/abs/2511.01139v2</a><br>We propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. We introduce a symmetry category that jointly represents cyclic time shifts, positive gain scalings, and the sensor-hierarchy poset, capturing the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01143v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01143v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01143v1">http://arxiv.org/abs/2511.01143v1</a><br>Early and accurate segmentation of colorectal polyps is critical for reducing colorectal cancer mortality, which has been extensively explored by academia and industry. However, current deep learning-based polyp segmentation models either compromise clinical decision-making by providing ambiguous polyp margins in segmentation outputs or rely on heavy architectures with high computational complexity, resulting in insufficient inference speeds for real-time colorectal endoscopic applications. To address this problem, we propose MicroAUNet, a light-weighted attention-based segmentation network that combines depthwise-separable dilated convolutions with a single-path, parameter-shared channel-spatial attention block to strengthen multi-scale boundary features. On the basis of it, a progressive two-stage knowledge-distillation scheme is introduced to transfer semantic and boundary cues from a high-capacity teacher. Extensive experiments on benchmarks also demonstrate the state-of-the-art accuracy under extremely low model complexity, indicating that MicroAUNet is suitable for real-time clinical polyp segmentation. The code is publicly available at <a href="https://github.com/JeremyXSC/MicroAUNet">https://github.com/JeremyXSC/MicroAUNet</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01163v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01163v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01163v1">http://arxiv.org/abs/2511.01163v1</a><br>Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Web-Scale Collection of Video Data for 4D Animal Reconstruction</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01169v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01169v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01169v1">http://arxiv.org/abs/2511.01169v1</a><br>Computer vision for animals holds great promise for wildlife research but often depends on large-scale data, while existing collection methods rely on controlled capture setups. Recent data-driven approaches show the potential of single-view, non-invasive analysis, yet current animal video datasets are limited–offering as few as 2.4K 15-frame clips and lacking key processing for animal-centric 3D&#x2F;4D tasks. We introduce an automated pipeline that mines YouTube videos and processes them into object-centric clips, along with auxiliary annotations valuable for downstream tasks like pose estimation, tracking, and 3D&#x2F;4D reconstruction. Using this pipeline, we amass 30K videos (2M frames)–an order of magnitude more than prior works. To demonstrate its utility, we focus on the 4D quadruped animal reconstruction task. To support this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually filtered sequences with 11K frames showcasing clean, diverse animal motions. We evaluate state-of-the-art model-based and model-free methods on Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic 3D shapes, while the latter yields more natural reconstructions but scores lower–revealing a gap in current evaluation. To address this, we enhance a recent model-free approach with sequence-level optimization, establishing the first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and baseline aim to advance large-scale, markerless 4D animal reconstruction and related tasks from in-the-wild videos. Code and datasets are available at <a href="https://github.com/briannlongzhao/Animal-in-Motion">https://github.com/briannlongzhao/Animal-in-Motion</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.2.10; I.4.5</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01175v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01175v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01175v2">http://arxiv.org/abs/2511.01175v2</a><br>Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image superresolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multiscale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multiscale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in low-frequency and high-frequency sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01186v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01186v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01186v1">http://arxiv.org/abs/2511.01186v1</a><br>Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoSa: Motion Generation with Scalable Autoregressive Modeling</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01200v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01200v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01200v1">http://arxiv.org/abs/2511.01200v1</a><br>We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask’s 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at <a href="https://mosa-web.github.io/MoSa-web">https://mosa-web.github.io/MoSa-web</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01194v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01194v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01194v1">http://arxiv.org/abs/2511.01194v1</a><br>Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07 (Artificial neural networks and deep learning), 68U10
  (Computer graphics, computational geometry)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01210v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01210v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01210v2">http://arxiv.org/abs/2511.01210v2</a><br>Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01213v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01213v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01213v1">http://arxiv.org/abs/2511.01213v1</a><br>The immense diversity in the culture and culinary of Indian cuisines calls attention to the major shortcoming of the existing Visual Question Answering(VQA) systems which are inclined towards the foods from Western region. Recent attempt towards building a VQA dataset for Indian food is a step towards addressing this challenge. However, their approach towards VQA follows a two-step process in which the answer is generated first, followed by the explanation of the expected answer. In this work, we claim that food VQA requires to follow a multi-step reasoning process to arrive at an accurate answer, especially in the context of India food, which involves understanding complex culinary context and identifying relationships between various food items. With this hypothesis we create reasoning chains upon the QA with minimal human intervention. We fine-tune smaller LLMs and VLMs with auto-validated reasoning chains and further train them using reinforcement learning with larger data. With augmentation of reasoning chains, we observed accuracy improvement of an average 10 percentage points on the baseline. We provide detailed analysis in terms the effect of addition of reasoning chains for the Indian Food VQA task.   Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge Graph.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01223v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01223v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01223v1">http://arxiv.org/abs/2511.01223v1</a><br>Domain adaptation is required for automated driving models to generalize well across diverse road conditions. This paper explores a training method for domain adaptation to adapt PilotNet, an end-to-end deep learning-based model, for left-hand driving conditions using real-world Australian highway data. Four training methods were evaluated: (1) a baseline model trained on U.S. right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a model pretrained on flipped U.S. data and then finetuned on Australian highways. This setup examines whether incorporating flipped data enhances the model adaptation by providing an initial left-hand driving alignment. The paper compares model performance regarding steering prediction accuracy and attention, using saliency-based analysis to measure attention shifts across significant road regions. Results show that pretraining on flipped data alone worsens prediction stability due to misaligned feature representations, but significantly improves adaptation when followed by fine-tuning, leading to lower prediction error and stronger focus on left-side cues. To validate this approach across different architectures, the same experiments were done on ResNet, which confirmed similar adaptation trends. These findings emphasize the importance of preprocessing techniques, such as flipped-data pretraining, followed by fine-tuning to improve model adaptation with minimal retraining requirements.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01233v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01233v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01233v1">http://arxiv.org/abs/2511.01233v1</a><br>We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models – each trained by its original authors – across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies – enabling new evaluations without model reimplementation required – alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.GR</tag>
      
      <tag>I.3; I.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Eyes on Target: Gaze-Aware Object Detection in Egocentric Video</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01237v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01237v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01237v1">http://arxiv.org/abs/2511.01237v1</a><br>Human gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01240v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01240v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01240v1">http://arxiv.org/abs/2511.01240v1</a><br>Transferable attacks generate adversarial examples on surrogate models to fool unknown victim models, posing real-world threats and growing research interest. Despite focusing on flat losses for transferable adversarial examples, recent studies still fall into suboptimal regions, especially the flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce a novel black-box gradient-based transferable attack from a perspective of dual-order information. Specifically, we feasibly propose Adversarial Flatness (AF) to the deceptive flatness problem and a theoretical assurance for adversarial transferability. Based on this, using an efficient approximation of our objective, we instantiate our attack as Adversarial Flatness Attack (AFA), addressing the altered gradient sign issue. Additionally, to further improve the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by enhancing the inner-loop sampling efficiency. The comprehensive results on ImageNet-compatible dataset demonstrate superiority over six baselines, generating adversarial examples in flatter regions and boosting transferability across model architectures. When tested on input transformation attacks or the Baidu Cloud API, our method outperforms baselines.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01243v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01243v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01243v1">http://arxiv.org/abs/2511.01243v1</a><br>Brain lesion segmentation remains challenging due to small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities. We propose CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and trains only lightweight adapters for efficient fine-tuning. At its core is the CenterMamba encoder, which employs a novel 3x3 corner-axis-center short-sequence scanning strategy to enable center-prioritized, axis-reinforced, and diagonally compensated information aggregation. This design enhances sensitivity to weak boundaries and tiny foci while maintaining sparse yet effective feature representation. A memory-driven structural prompt generator maintains a prototype bank across neighboring slices, enabling automatic synthesis of reliable prompts without user interaction, thereby improving inter-slice coherence. The memory-augmented multi-scale decoder integrates memory attention modules at multiple levels, combining deep supervision with progressive refinement to restore fine details while preserving global consistency. Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01250v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01250v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01250v2">http://arxiv.org/abs/2511.01250v2</a><br>LiDAR semantic segmentation degrades in adverse weather because refraction, scattering, and point dropouts corrupt geometry. Prior work in weather simulation, mixing-based augmentation, domain randomization, and uncertainty or boundary regularization improves robustness but still overlooks structural vulnerabilities near boundaries, corners, and sparse regions. We present a Light Geometry-aware adapter. The module aligns azimuth and applies horizontal circular padding to preserve neighbor continuity across the 0~360 degree wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points and computes simple local statistics, which are compressed into compact geometry-aware cues. During training, these cues drive region-aware regularization that stabilizes predictions in structurally fragile areas. The adapter is plug and play, complements augmentation, and can be enabled only during training with negligible inference cost. We adopt a source-only cross-weather setup where models train on SemanticKITTI and are evaluated on SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by 7.9 percentage points over the data-centric augmentation baseline and by 0.6 points over the class-centric regularization baseline. These results indicate that geometry-driven regularization is a key direction for all-weather LiDAR segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MotionStream: Real-Time Video Generation with Interactive Motion Controls</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01266v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01266v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01266v1">http://arxiv.org/abs/2511.01266v1</a><br>Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01274v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01274v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01274v1">http://arxiv.org/abs/2511.01274v1</a><br>Ancient Chinese paintings are a valuable cultural heritage that is damaged by irreversible color degradation. Reviving color-degraded paintings is extraordinarily difficult due to the complex chemistry mechanism. Progress is further slowed by the lack of comprehensive, high-quality datasets, which hampers the creation of end-to-end digital restoration tools. To revive colors, we propose PRevivor, a prior-guided color transformer that learns from recent paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and Song Dynasty). To develop PRevivor, we decompose color restoration into two sequential sub-tasks: luminance enhancement and hue correction. For luminance enhancement, we employ two variational U-Nets and a multi-scale mapping module to translate faded luminance into restored counterparts. For hue correction, we design a dual-branch color query module guided by localized hue priors extracted from faded paintings. Specifically, one branch focuses attention on regions guided by masked priors, enforcing localized hue correction, whereas the other branch remains unconstrained to maintain a global reasoning capability. To evaluate PRevivor, we conduct extensive experiments against state-of-the-art colorization methods. The results demonstrate superior performance both quantitatively and qualitatively.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01284v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01284v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01284v1">http://arxiv.org/abs/2511.01284v1</a><br>Foundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Detecting Generated Images by Fitting Natural Image Distributions</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01293v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01293v1">http://arxiv.org/abs/2511.01293v1</a><br>The increasing realism of generated images has raised significant concerns about their potential misuse, necessitating robust detection methods. Current approaches mainly rely on training binary classifiers, which depend heavily on the quantity and quality of available generated images. In this work, we propose a novel framework that exploits geometric differences between the data manifolds of natural and generated images. To exploit this difference, we employ a pair of functions engineered to yield consistent outputs for natural images but divergent outputs for generated ones, leveraging the property that their gradients reside in mutually orthogonal subspaces. This design enables a simple yet effective detection method: an image is identified as generated if a transformation along its data manifold induces a significant change in the loss value of a self-supervised model pre-trained on natural images. Further more, to address diminishing manifold disparities in advanced generative models, we leverage normalizing flows to amplify detectable differences by extruding generated images away from the natural image manifold. Extensive experiments demonstrate the efficacy of this method. Code is available at <a href="https://github.com/tmlr-group/ConV">https://github.com/tmlr-group/ConV</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01294v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01294v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01294v2">http://arxiv.org/abs/2511.01294v2</a><br>A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniREditBench: A Unified Reasoning-based Image Editing Benchmark</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01295v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01295v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01295v1">http://arxiv.org/abs/2511.01295v1</a><br>Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>REASON: Probability map-guided dual-branch fusion framework for gastric content assessment</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01302v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01302v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01302v1">http://arxiv.org/abs/2511.01302v1</a><br>Accurate assessment of gastric content from ultrasound is critical for stratifying aspiration risk at induction of general anesthesia. However, traditional methods rely on manual tracing of gastric antra and empirical formulas, which face significant limitations in both efficiency and accuracy. To address these challenges, a novel two-stage probability map-guided dual-branch fusion framework (REASON) for gastric content assessment is proposed. In stage 1, a segmentation model generates probability maps that suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch classifier fuses information from two standard views, right lateral decubitus (RLD) and supine (SUP), to improve the discrimination of learned features. Experimental results on a self-collected dataset demonstrate that the proposed framework outperforms current state-of-the-art approaches by a significant margin. This framework shows great promise for automated preoperative aspiration risk assessment, offering a more robust, efficient, and accurate solution for clinical practice.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01304v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01304v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01304v2">http://arxiv.org/abs/2511.01304v2</a><br>Multiple instance learning (MIL) has been widely used for representing whole-slide pathology images. However, spatial, semantic, and decision entanglements among instances limit its representation and interpretability. To address these challenges, we propose a latent factor grouping-boosted cluster-reasoning instance disentangled learning framework for whole-slide image (WSI) interpretable representation in three phases. First, we introduce a novel positive semi-definite latent factor grouping that maps instances into a latent subspace, effectively mitigating spatial entanglement in MIL. To alleviate semantic entanglement, we employs instance probability counterfactual inference and optimization via cluster-reasoning instance disentangling. Finally, we employ a generalized linear weighted decision via instance effect re-weighting to address decision entanglement. Extensive experiments on multicentre datasets demonstrate that our model outperforms all state-of-the-art models. Moreover, it attains pathologist-aligned interpretability through disentangled representations and a transparent decision-making process.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01307v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01307v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01307v1">http://arxiv.org/abs/2511.01307v1</a><br>Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at <a href="https://github.com/KU-VGI/APDM">https://github.com/KU-VGI/APDM</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MVSMamba: Multi-View Stereo with State Space Model</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01315v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01315v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01315v1">http://arxiv.org/abs/2511.01315v1</a><br>Robust feature representations are essential for learning-based Multi-View Stereo (MVS), which relies on accurate feature matching. Recent MVS methods leverage Transformers to capture long-range dependencies based on local features extracted by conventional feature pyramid networks. However, the quadratic complexity of Transformer-based MVS methods poses challenges to balance performance and efficiency. Motivated by the global modeling capability and linear complexity of the Mamba architecture, we propose MVSMamba, the first Mamba-based MVS network. MVSMamba enables efficient global feature aggregation with minimal computational overhead. To fully exploit Mamba’s potential in MVS, we propose a Dynamic Mamba module (DM-module) based on a novel reference-centered dynamic scanning strategy, which enables: (1) Efficient intra- and inter-view feature interaction from the reference to source views, (2) Omnidirectional multi-view feature representations, and (3) Multi-scale global feature aggregation. Extensive experimental results demonstrate MVSMamba outperforms state-of-the-art MVS methods on the DTU dataset and the Tanks-and-Temples benchmark with both superior performance and efficiency. The source code is available at <a href="https://github.com/JianfeiJ/MVSMamba">https://github.com/JianfeiJ/MVSMamba</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01317v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01317v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01317v1">http://arxiv.org/abs/2511.01317v1</a><br>The rapid growth of deep learning has brought about powerful models that can handle various tasks, like identifying images and understanding language. However, adversarial attacks, an unnoticed alteration, can deceive models, leading to inaccurate predictions. In this paper, a generative adversarial attack method is proposed that uses the CLIP model to create highly effective and visually imperceptible adversarial perturbations. The CLIP model’s ability to align text and image representation helps incorporate natural language semantics with a guided loss to generate effective adversarial examples that look identical to the original inputs. This integration allows extensive scene manipulation, creating perturbations in multi-object environments specifically designed to deceive multilabel classifiers. Our approach integrates the concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with the dissimilar text embeddings similar to Generative Adversarial Multi-Object Scene Attacks (GAMA), resulting in perturbations that both deceive classification models and maintain high structural similarity to the original images. The model was tested on various tasks across diverse black-box victim models. The experimental results show that our method performs competitively, achieving comparable or superior results to existing techniques, while preserving greater visual fidelity.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01328v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01328v1">http://arxiv.org/abs/2511.01328v1</a><br>Medical image segmentation is essential for computer-assisted diagnosis and treatment planning, yet substantial anatomical variability and boundary ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet, a segmentation network that unifies local modeling with global context to strengthen boundary delineation and detail preservation. RDTE-UNet employs a hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for adaptive boundary enhancement, HVDA for fine-grained feature modeling, and EulerFF for fusion weighting guided by Euler’s formula. Together, these components improve structural consistency and boundary accuracy across morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has achieved a comparable level in terms of segmentation accuracy and boundary quality.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$&#92;left|&#92;,&#92;circlearrowright&#92;,&#92;boxed{&#92;text{BUS}}&#92;,&#92;right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01340v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01340v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01340v1">http://arxiv.org/abs/2511.01340v1</a><br>Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$, a large and diverse benchmark of $1,333$ English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$ by $2.1-4.1%$ and $20-30%$ using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01345v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01345v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01345v1">http://arxiv.org/abs/2511.01345v1</a><br>Accurate segmentation of medical images is fundamental to tumor diagnosis and treatment planning. SAM-based interactive segmentation has gained attention for its strong generalization, but most methods follow a single-point-to-single-object paradigm, which limits multi-lesion segmentation. Moreover, ViT backbones capture global context but often miss high-fidelity local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework with a competitive query optimization strategy that shifts from single-point-to-single-mask to single-point-to-multi-instance. A prompt-conditioned instance-query generator transforms a single point prompt into multiple specialized queries, enabling retrieval of all semantically similar lesions across the 3D volume from a single exemplar. A hybrid CNN-Transformer encoder injects CNN-derived boundary saliency into ViT self-attention via spatial gating. A competitively optimized query decoder then enables end-to-end, parallel, multi-instance prediction through inter-query competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels and exhibits strong robustness to prompts, providing a practical solution for efficient annotation of clinically relevant multi-lesion cases.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01355v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01355v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01355v1">http://arxiv.org/abs/2511.01355v1</a><br>Recent advancements in text-to-image diffusion models have significantly improved the personalization and stylization of generated images. However, previous studies have only assessed content similarity under a single style intensity. In our experiments, we observe that increasing style intensity leads to a significant loss of content features, resulting in a suboptimal content-style frontier. To address this, we propose a novel approach to expand the content-style frontier by leveraging Content-Style Subspace Blending and a Content-Style Balance loss. Our method improves content similarity across varying style intensities, significantly broadening the content-style frontier. Extensive experiments demonstrate that our approach outperforms existing techniques in both qualitative and quantitative evaluations, achieving superior content-style trade-off with significantly lower Inverted Generational Distance (IGD) and Generational Distance (GD) scores compared to current methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01357v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01357v1">http://arxiv.org/abs/2511.01357v1</a><br>Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model’s capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at <a href="https://github.com/BioMedIA-repo/CMI-MTL">https://github.com/BioMedIA-repo/CMI-MTL</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EREBUS: End-to-end Robust Event Based Underwater Simulation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01381v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01381v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01381v1">http://arxiv.org/abs/2511.01381v1</a><br>The underwater domain presents a vast array of challenges for roboticists and computer vision researchers alike, such as poor lighting conditions and high dynamic range scenes. In these adverse conditions, traditional vision techniques struggle to adapt and lead to suboptimal performance. Event-based cameras present an attractive solution to this problem, mitigating the issues of traditional cameras by tracking changes in the footage on a frame-by-frame basis. In this paper, we introduce a pipeline which can be used to generate realistic synthetic data of an event-based camera mounted to an AUV (Autonomous Underwater Vehicle) in an underwater environment for training vision models. We demonstrate the effectiveness of our pipeline using the task of rock detection with poor visibility and suspended particulate matter, but the approach can be generalized to other underwater tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01390v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01390v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01390v1">http://arxiv.org/abs/2511.01390v1</a><br>Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23%-86% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at <a href="https://github.com/Sweet4tars/seps.git">https://github.com/Sweet4tars/seps.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01399v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01399v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01399v1">http://arxiv.org/abs/2511.01399v1</a><br>Inventory management of firefighting assets is crucial for emergency preparedness, risk assessment, and on-site fire response. However, conventional methods are inefficient due to limited capabilities in automated asset recognition and reconstruction. To address the challenge, this research introduces the Fire-ART dataset and develops a panoramic image-based reconstruction approach for semantic enrichment of firefighting assets into BIM models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626 images and 6,627 instances, making it an extensive and publicly accessible dataset for asset recognition. In addition, the reconstruction approach integrates modified cube-map conversion and radius-based spherical camera projection to enhance recognition and localization accuracy. Through validations with two real-world case studies, the proposed approach achieves F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters, respectively. The Fire-ART dataset and the reconstruction approach offer valuable resources and robust technical solutions to enhance the accurate digital management of fire safety equipment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Extremal Contours: Gradient-driven contours for compact visual attribution</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01411v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01411v1">http://arxiv.org/abs/2511.01411v1</a><br>Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve&#x2F;delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards One-step Causal Video Generation via Adversarial Self-Distillation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01419v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01419v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01419v1">http://arxiv.org/abs/2511.01419v1</a><br>Recent hybrid video generation models combine autoregressive temporal dynamics with diffusion-based spatial denoising, but their sequential, iterative nature leads to error accumulation and long inference times. In this work, we propose a distillation-based framework for efficient causal video generation that enables high-quality synthesis with extremely limited denoising steps. Our approach builds upon the Distribution Matching Distillation (DMD) framework and proposes a novel Adversarial Self-Distillation (ASD) strategy, which aligns the outputs of the student model’s n-step denoising process with its (n+1)-step version at the distribution level. This design provides smoother supervision by bridging small intra-student gaps and more informative guidance by combining teacher knowledge with locally consistent student behavior, substantially improving training stability and generation quality in extremely few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame Enhancement (FFE) strategy, which allocates more denoising steps to the initial frames to mitigate error propagation while applying larger skipping steps to later frames. Extensive experiments on VBench demonstrate that our method surpasses state-of-the-art approaches in both one-step and two-step video generation. Notably, our framework produces a single distilled model that flexibly supports multiple inference-step settings, eliminating the need for repeated re-distillation and enabling efficient, high-quality video synthesis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01425v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01425v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01425v1">http://arxiv.org/abs/2511.01425v1</a><br>Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18% compared to a non-interactive baseline. To validate the faithfulness of the agent’s explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\Delta$Brier&#x3D;+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.6; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniSOT: A Unified Framework for Multi-Modality Single Object Tracking</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01427v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01427v1">http://arxiv.org/abs/2511.01427v1</a><br>Single object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0% main metric across all three RGB+X video modalities.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01434v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01434v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01434v1">http://arxiv.org/abs/2511.01434v1</a><br>Off-road semantic segmentation suffers from thick, inconsistent boundaries, sparse supervision for rare classes, and pervasive label noise. Designs that fuse only at low resolution blur edges and propagate local errors, whereas maintaining high-resolution pathways or repeating high-resolution fusions is costly and fragile to noise. We introduce a resolutionaware token decoder that balances global semantics, local consistency, and boundary fidelity under imperfect supervision. Most computation occurs at a low-resolution bottleneck; a gated cross-attention injects fine-scale detail, and only a sparse, uncertainty-selected set of pixels is refined. The components are co-designed and tightly integrated: global self-attention with lightweight dilated depthwise refinement restores local coherence; a gated cross-attention integrates fine-scale features from a standard high-resolution encoder stream without amplifying noise; and a class-aware point refinement corrects residual ambiguities with negligible overhead. During training, we add a boundary-band consistency regularizer that encourages coherent predictions in a thin neighborhood around annotated edges, with no inference-time cost. Overall, the results indicate competitive performance and improved stability across transitions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Contrast-Guided Cross-Modal Distillation for Thermal Object Detection</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01435v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01435v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01435v1">http://arxiv.org/abs/2511.01435v1</a><br>Robust perception at night remains challenging for thermal-infrared detection: low contrast and weak high-frequency cues lead to duplicate, overlapping boxes, missed small objects, and class confusion. Prior remedies either translate TIR to RGB and hope pixel fidelity transfers to detection – making performance fragile to color or structure artifacts – or fuse RGB and TIR at test time, which requires extra sensors, precise calibration, and higher runtime cost. Both lines can help in favorable conditions, but do not directly shape the thermal representation used by the detector. We keep mono-modality inference and tackle the root causes during training. Specifically, we introduce training-only objectives that sharpen instance-level decision boundaries by pulling together features of the same class and pushing apart those of different classes – suppressing duplicate and confusing detections – and that inject cross-modal semantic priors by aligning the student’s multi-level pyramid features with an RGB-trained teacher, thereby strengthening texture-poor thermal features without visible input at test time. In experiments, our method outperformed prior approaches and achieved state-of-the-art performance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01449v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01449v1">http://arxiv.org/abs/2511.01449v1</a><br>To effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.   Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01450v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01450v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01450v2">http://arxiv.org/abs/2511.01450v2</a><br>Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO loss to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01458v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01458v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01458v1">http://arxiv.org/abs/2511.01458v1</a><br>Safety and reliability are essential for deploying Visual Question Answering (VQA) in surgery, where incorrect or ambiguous responses can harm the patient. Most surgical VQA research focuses on accuracy or linguistic quality while overlooking safety behaviors such as ambiguity awareness, referral to human experts, or triggering a second opinion. Inspired by Automatic Failure Detection (AFD), we study uncertainty estimation as a key enabler of safer decision making. We introduce Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question semantics into prediction confidence. It measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. We evaluate five models, including domain specific Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models degrade under mild paraphrasing, while LVLMs are more resilient. Across three LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template settings and enhances hallucination detection. The Area Under the ROC Curve (AUROC) increases by 15-38% for zero-shot models, with gains maintained under out-of-template stress. QA-SNNE offers a practical and interpretable step toward AFD in surgical VQA by linking semantic uncertainty to question context. Combining LVLM backbones with question aligned uncertainty estimation can improve safety and clinician trust. The code and model are available at <a href="https://github.com/DennisPierantozzi/QASNNE">https://github.com/DennisPierantozzi/QASNNE</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Efficiently Training A Flat Neural Network Before It has been Quantizated</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01462v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01462v1">http://arxiv.org/abs/2511.01462v1</a><br>Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error for PTQ. However, it is unclear how to efficiently train a model-agnostic neural network which is tailored for a predefined precision low-bit model. In this paper, we firstly discover that a flat full precision neural network is crucial for low-bit quantization. To achieve this, we propose a framework that proactively pre-conditions the model by measuring and disentangling the error sources. Specifically, both the Activation Quantization Error (AQE) and the Weight Quantization Error (WQE) are statistically modeled as independent Gaussian noises. We study several noise injection optimization methods to obtain a flat minimum. Experimental results attest to the effectiveness of our approach. These results open novel pathways for obtaining low-bit PTQ models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01463v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01463v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01463v1">http://arxiv.org/abs/2511.01463v1</a><br>The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
      <tag>68T45</tag>
      
      <tag>I.2.10; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01466v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01466v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01466v1">http://arxiv.org/abs/2511.01466v1</a><br>Deep joint source-channel coding (JSCC) has emerged as a promising paradigm for semantic communication, delivering significant performance gains over conventional separate coding schemes. However, existing JSCC frameworks remain vulnerable to physical-layer adversarial threats, such as pilot spoofing and subcarrier jamming, compromising semantic fidelity. In this paper, we propose SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly enhances the security and robustness of deep JSCC under adversarial wireless environments. Different from prior diffusion-guided JSCC methods that suffer from high inference latency, SecDiff employs pseudoinverse-guided sampling and adaptive guidance weighting, enabling flexible step-size control and efficient semantic reconstruction. To counter jamming attacks, we introduce a power-based subcarrier masking strategy and recast recovery as a masked inpainting problem, solved via diffusion guidance. For pilot spoofing, we formulate channel estimation as a blind inverse problem and develop an expectation-minimization (EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and a channel operator. Notably, our method alternates between pilot recovery and channel estimation, enabling joint refinement of both variables throughout the diffusion process. Extensive experiments over orthogonal frequency-division multiplexing (OFDM) channels under adversarial conditions show that SecDiff outperforms existing secure and generative JSCC baselines by achieving a favorable trade-off between reconstruction quality and computational cost. This balance makes SecDiff a promising step toward practical, low-latency, and attack-resilient semantic communications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01498v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01498v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01498v1">http://arxiv.org/abs/2511.01498v1</a><br>Person re-identification (ReID) plays a pivotal role in computer vision, particularly in surveillance and security applications within IoT-enabled smart environments. This study introduces the Enhanced Pedestrian Alignment Network (EPAN), tailored for robust ReID across diverse IoT surveillance conditions. EPAN employs a dual-branch architecture to mitigate the impact of perspective and environmental changes, extracting alignment information under varying scales and viewpoints. Here, we demonstrate EPAN’s strong feature extraction capabilities, achieving outstanding performance on the Inspection-Personnel dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of 78.82%. This highlights EPAN’s potential for real-world IoT applications, enabling effective and reliable person ReID across diverse cameras in surveillance and security systems. The code and data are available at: <a href="https://github.com/ggboy2580/EPAN">https://github.com/ggboy2580/EPAN</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01501v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01501v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01501v1">http://arxiv.org/abs/2511.01501v1</a><br>Object pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01502v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01502v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01502v1">http://arxiv.org/abs/2511.01502v1</a><br>Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group&#x2F;DiMoDE upon publication.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01510v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01510v1">http://arxiv.org/abs/2511.01510v1</a><br>Low-light image enhancement (LLIE) faces persistent challenges in balancing reconstruction fidelity with cross-scenario generalization. While existing methods predominantly focus on deterministic pixel-level mappings between paired low&#x2F;normal-light images, they often neglect the continuous physical process of luminance transitions in real-world environments, leading to performance drop when normal-light references are unavailable. Inspired by empirical analysis of natural luminance dynamics revealing power-law distributed intensity transitions, this paper introduces Luminance-Aware Statistical Quantification (LASQ), a novel framework that reformulates LLIE as a statistical sampling process over hierarchical luminance distributions. Our LASQ re-conceptualizes luminance transition as a power-law distribution in intensity coordinate space that can be approximated by stratified power functions, therefore, replacing deterministic mappings with probabilistic sampling over continuous luminance layers. A diffusion forward process is designed to autonomously discover optimal transition paths between luminance layers, achieving unsupervised distribution emulation without normal-light references. In this way, it considerably improves the performance in practical situations, enabling more adaptable and versatile light restoration. This framework is also readily applicable to cases with normal-light references, where it achieves superior performance on domain-specific datasets alongside better generalization-ability across non-reference datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Example-Based Feature Painting on Textures</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01513v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01513v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01513v1">http://arxiv.org/abs/2511.01513v1</a><br>In this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: <a href="https://reality.tf.fau.de/pub/ardelean2025examplebased.html">https://reality.tf.fau.de/pub/ardelean2025examplebased.html</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01517v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01517v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01517v1">http://arxiv.org/abs/2511.01517v1</a><br>Current text conditioned image generation methods output realistic looking images, but they fail to capture specific styles. Simply finetuning them on the target style datasets still struggles to grasp the style features. In this work, we present a novel contrastive learning framework to improve the stylization capability of large text-to-image diffusion models. Motivated by the astonishing advance in image generation models that makes synthetic data an intrinsic part of model training in various computer vision tasks, we exploit synthetic image generation in our approach. Usually, the generated synthetic data is dependent on the task, and most of the time it is used to enlarge the available real training dataset. With NSYNC, alternatively, we focus on generating negative synthetic sets to be used in a novel contrastive training scheme along with real positive images. In our proposed training setup, we forward negative data along with positive data and obtain negative and positive gradients, respectively. We then refine the positive gradient by subtracting its projection onto the negative gradient to get the orthogonal component, based on which the parameters are updated. This orthogonal component eliminates the trivial attributes that are present in both positive and negative data and directs the model towards capturing a more unique style. Experiments on various styles of painters and illustrators show that our approach improves the performance over the baseline methods both quantitatively and qualitatively. Our code is available at <a href="https://github.com/giddyyupp/NSYNC">https://github.com/giddyyupp/NSYNC</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Driving scenario generation and evaluation using a structured layer representation and foundational models</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01541v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01541v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01541v1">http://arxiv.org/abs/2511.01541v1</a><br>Rare and challenging driving scenarios are critical for autonomous vehicle development. Since they are difficult to encounter, simulating or generating them using generative models is a popular approach. Following previous efforts to structure driving scenario representations in a layer model, we propose a structured five-layer model to improve the evaluation and generation of rare scenarios. We use this model alongside large foundational models to generate new driving scenarios using a data augmentation strategy. Unlike previous representations, our structure introduces subclasses and characteristics for every agent of the scenario, allowing us to compare them using an embedding specific to our layer-model. We study and adapt two metrics to evaluate the relevance of a synthetic dataset in the context of a structured representation: the diversity score estimates how different the scenarios of a dataset are from one another, while the originality score calculates how similar a synthetic dataset is from a real reference set. This paper showcases both metrics in different generation setup, as well as a qualitative evaluation of synthetic videos generated from structured scenario descriptions. The code and extended results can be found at <a href="https://github.com/Valgiz/5LMSG">https://github.com/Valgiz/5LMSG</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PCD-ReID: Occluded Person Re-Identification for Base Station Inspection</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01546v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01546v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01546v1">http://arxiv.org/abs/2511.01546v1</a><br>Occluded pedestrian re-identification (ReID) in base station environments is a critical task in computer vision, particularly for surveillance and security applications. This task faces numerous challenges, as occlusions often obscure key body features, increasing the complexity of identification. Traditional ResNet-based ReID algorithms often fail to address occlusions effectively, necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component Discrepancy) algorithm to address these issues. The contributions of this work are as follows: To tackle the occlusion problem, we design a Transformer-based PCD network capable of extracting shared component features, such as helmets and uniforms. To mitigate overfitting on public datasets, we collected new real-world patrol surveillance images for model training, covering six months, 10,000 individuals, and over 50,000 images. Comparative experiments with existing ReID algorithms demonstrate that our model achieves a mean Average Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1 improvement over ResNet50-based methods. Experimental evaluations indicate that PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in tower inspection scenarios, highlighting its potential for practical deployment in surveillance and security applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NOA: a versatile, extensible tool for AI-based organoid analysis</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01549v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01549v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01549v1">http://arxiv.org/abs/2511.01549v1</a><br>AI tools can greatly enhance the analysis of organoid microscopy images, from detection and segmentation to feature extraction and classification. However, their limited accessibility to biologists without programming experience remains a major barrier, resulting in labor-intensive and largely manual workflows. Although a few AI models for organoid analysis have been developed, most existing tools remain narrowly focused on specific tasks. In this work, we introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user interface to simplify AI-based organoid analysis. NOA integrates modules for detection, segmentation, tracking, feature extraction, custom feature annotation and ML-based feature prediction. It interfaces multiple state-of-the-art algorithms and is implemented as an open-source napari plugin for maximal flexibility and extensibility. We demonstrate the versatility of NOA through three case studies, involving the quantification of morphological changes during organoid differentiation, assessment of phototoxicity effects, and prediction of organoid viability and differentiation state. Together, these examples illustrate how NOA enables comprehensive, AI-driven organoid image analysis within an accessible and extensible framework.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01574v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01574v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01574v1">http://arxiv.org/abs/2511.01574v1</a><br>Compared to traditional methods, Deep Learning (DL) becomes a key technology for computer vision tasks. Synthetic data generation is an interesting use case for DL, especially in the field of medical imaging such as Magnetic Resonance Imaging (MRI). The need for this task since the original MRI data is limited. The generation of realistic medical images is completely difficult and challenging. Generative Adversarial Networks (GANs) are useful for creating synthetic medical images. In this paper, we propose a DL based methodology for creating synthetic MRI data using the Deep Convolutional Generative Adversarial Network (DC-GAN) to address the problem of limited data. We also employ a Convolutional Neural Network (CNN) classifier to classify the brain tumor using synthetic data and real MRI data. CNN is used to evaluate the quality and utility of the synthetic images. The classification result demonstrates comparable performance on real and synthetic images, which validates the effectiveness of GAN-generated images for downstream tasks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01588v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01588v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01588v1">http://arxiv.org/abs/2511.01588v1</a><br>Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01571v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01571v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01571v1">http://arxiv.org/abs/2511.01571v1</a><br>Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01593v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01593v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01593v1">http://arxiv.org/abs/2511.01593v1</a><br>The unification of understanding and generation within a single multi-modal large model (MLLM) remains one significant challenge, largely due to the dichotomy between continuous and discrete visual tokenizations. Continuous tokenizer (CT) achieves strong performance by bridging multiple independently-trained understanding modules and generation modules, but suffers from complex multi-stage pipelines and substantial engineering overhead. Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by quantizing each image into a primitive, but inevitably leading to information loss and performance degradation. To resolve this tension, we question the binary choice between CT and DT, inspired by the wave-particle duality of light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT). We treat visual data as a flexible composition of image primitives derived from quantized codebooks, with the crucial insight that the primitive number assigned to each visual sample is adaptively determined according to its complexity: simple instances use a few primitives, emulating discrete tokenization, while complex instances use many, approximating continuous tokenization. Two core components are designed: Diverse Quantitative Primitives, which encourage primitives orthogonality to better populate information space, and Dynamic Primitive Allocator, which assesses sample complexity to determine the optimal set of primitives. Extensive experiments on reconstruction, retrieval and classification show that CDD-VT achieves superior performance over to specialized CT and DT, effectively getting strong result within a concise and scalable MLLM.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01594v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01594v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01594v1">http://arxiv.org/abs/2511.01594v1</a><br>Multimodal large language models (MLLMs) have shown remarkable capabilities in cross-modal understanding and reasoning, offering new opportunities for intelligent assistive systems, yet existing systems still struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic System powered by MLLMs for assistive intelligence and designed for smart home robots supporting people with disabilities. The system integrates four agents: a visual perception agent for extracting semantic and spatial features from environment images, a risk assessment agent for identifying and prioritizing hazards, a planning agent for generating executable action sequences, and an evaluation agent for iterative optimization. By combining multimodal perception with hierarchical multi-agent decision-making, the framework enables adaptive, risk-aware, and personalized assistance in dynamic indoor environments. Experiments on multiple datasets demonstrate the superior overall performance of the proposed system in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models. The proposed approach also highlights the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>I.2.9; I.2.11; I.2.6; I.4.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01600v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01600v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01600v1">http://arxiv.org/abs/2511.01600v1</a><br>Accurate tumor size measurement is a cornerstone of evaluating cancer treatment response. The most widely adopted standard for this purpose is the Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on measuring the longest tumor diameter in a single plane. However, volumetric measurements have been shown to provide a more reliable assessment of treatment effect. Their clinical adoption has been limited, though, due to the labor-intensive nature of manual volumetric annotation. In this paper, we present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed for efficient volumetric tumor segmentation from CT scans annotated with RECIST annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1: Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of 63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an average inference time of 14.4 s on CPU on the public validation dataset.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01610v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01610v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01610v1">http://arxiv.org/abs/2511.01610v1</a><br>Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmark-Ready 3D Anatomical Shape Classification</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01613v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01613v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01613v1">http://arxiv.org/abs/2511.01613v1</a><br>Progress in anatomical 3D shape classification is limited by the complexity of mesh data and the lack of standardized benchmarks, highlighting the need for robust learning methods and reproducible evaluation. We introduce two key steps toward clinically and benchmark-ready anatomical shape classification via self-supervised graph autoencoding. We propose Precomputed Structural Pooling (PSPooling), a non-learnable mesh pooling operator designed for efficient and structure-preserving graph coarsening in 3D anatomical shape analysis. PSPooling precomputes node correspondence sets based on geometric proximity, enabling parallelizable and reversible pooling and unpooling operations with guaranteed support structure. This design avoids the sparsity and reconstruction issues of selection-based methods and the sequential overhead of edge contraction approaches, making it particularly suitable for high-resolution medical meshes. To demonstrate its effectiveness, we integrate PSPooling into a self-supervised graph autoencoder that learns anatomy-aware representations from unlabeled surface meshes. We evaluate the downstream benefits on MedShapeNet19, a new curated benchmark dataset we derive from MedShapeNet, consisting of 19 anatomical classes with standardized training, validation, and test splits. Experiments show that PSPooling significantly improves reconstruction fidelity and classification accuracy in low-label regimes, establishing a strong baseline for medical 3D shape learning. We hope that MedShapeNet19 will serve as a widely adopted benchmark for anatomical shape classification and further research in medical 3D shape analysis. Access the complete codebase, model weights, and dataset information here: <a href="https://github.com/TomasKrsicka/MedShapeNet19-PSPooling">https://github.com/TomasKrsicka/MedShapeNet19-PSPooling</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01617v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01617v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01617v1">http://arxiv.org/abs/2511.01617v1</a><br>In the retrieval domain, candidates’ fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates’ representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM’s prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) &#x2F; 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: <a href="https://github.com/mohammad2012191/ViC">https://github.com/mohammad2012191/ViC</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01618v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01618v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01618v1">http://arxiv.org/abs/2511.01618v1</a><br>Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01645v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01645v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01645v1">http://arxiv.org/abs/2511.01645v1</a><br>Reinforcement Learning (RL) has recently been incorporated into diffusion models, e.g., tasks such as text-to-image. However, directly applying existing RL methods to diffusion-based image restoration models is suboptimal, as the objective of restoration fundamentally differs from that of pure generation: it places greater emphasis on fidelity. In this paper, we investigate how to effectively integrate RL into diffusion-based restoration models. First, through extensive experiments with various reward functions, we find that an effective reward can be derived from an Image Quality Assessment (IQA) model, instead of intuitive ground-truth-based supervision, which has already been optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover, our strategy focuses on using RL for challenging samples that are significantly distant from the ground truth, and our RL approach is innovatively implemented using MLLM-based IQA models to align distributions with high-quality images initially. As the samples approach the ground truth’s distribution, RL is adaptively combined with SFT for more fine-grained alignment. This dynamic process is facilitated through an automatic weighting strategy that adjusts based on the relative difficulty of the training samples. Our strategy is plug-and-play that can be seamlessly applied to diffusion-based restoration models, boosting its performance across various restoration tasks. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed RL framework.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01678v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01678v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01678v1">http://arxiv.org/abs/2511.01678v1</a><br>Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at <a href="https://github.com/alibaba-damo-academy/Lumos-Custom">https://github.com/alibaba-damo-academy/Lumos-Custom</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Progressive Translation of H&amp;E to IHC with Enhanced Structural Fidelity</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01698v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01698v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01698v1">http://arxiv.org/abs/2511.01698v1</a><br>Compared to hematoxylin-eosin (H&amp;E) staining, immunohistochemistry (IHC) not only maintains the structural features of tissue samples, but also provides high-resolution protein localization, which is essential for aiding in pathology diagnosis. Despite its diagnostic value, IHC remains a costly and labor-intensive technique. Its limited scalability and constraints in multiplexing further hinder widespread adoption, especially in resource-limited settings. Consequently, researchers are increasingly exploring computational stain translation techniques to synthesize IHC-equivalent images from H&amp;E-stained slides, aiming to extract protein-level information more efficiently and cost-effectively. However, most existing stain translation techniques rely on a linearly weighted summation of multiple loss terms within a single objective function, strategy that often overlooks the interdepedence among these components-resulting in suboptimal image quality and an inability to simultaneously preserve structural authenticity and color fidelity. To address this limitation, we propose a novel network architecture that follows a progressive structure, incorporating color and cell border generation logic, which enables each visual aspect to be optimized in a stage-wise and decoupled manner. To validate the effectiveness of our proposed network architecture, we build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We introduce additional loss functions based on 3,3’-diaminobenzidine (DAB) chromogen concentration and image gradient, enhancing color fidelity and cell boundary clarity in the generated IHC images. By reconstructing the generation pipeline using our structure-color-cell boundary progressive mechanism, experiments on HER2 and ER datasets demonstrated that the model significantly improved visual quality and achieved finer structural details.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01704v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01704v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01704v1">http://arxiv.org/abs/2511.01704v1</a><br>Under-display ToF imaging aims to achieve accurate depth sensing through a ToF camera placed beneath a screen panel. However, transparent OLED (TOLED) layers introduce severe degradations-such as signal attenuation, multi-path interference (MPI), and temporal noise-that significantly compromise depth quality. To alleviate this drawback, we propose Learnable Fractional Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the expressive power of neural networks with the interpretability of physical modeling. Specifically, we implement a time-fractional reaction-diffusion module that enables iterative depth refinement with dynamically generated differential orders, capturing long-term dependencies. In addition, we introduce an efficient continuous convolution operator via coefficient prediction and repeated differentiation to further improve restoration quality. Experiments on four benchmark datasets demonstrate the effectiveness of our approach. The code is publicly available at <a href="https://github.com/wudiqx106/LFRD2">https://github.com/wudiqx106/LFRD2</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01718v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01718v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01718v1">http://arxiv.org/abs/2511.01718v1</a><br>Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act – reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at <a href="https://irpn-eai.github.io/UD-VLA.github.io/">https://irpn-eai.github.io/UD-VLA.github.io/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Probabilistic Robustness for Free? Revisiting Training via a Benchmark</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01724v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01724v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01724v1">http://arxiv.org/abs/2511.01724v1</a><br>Deep learning models are notoriously vulnerable to imperceptible perturbations. Most existing research centers on adversarial robustness (AR), which evaluates models under worst-case scenarios by examining the existence of deterministic adversarial examples (AEs). In contrast, probabilistic robustness (PR) adopts a statistical perspective, measuring the probability that predictions remain correct under stochastic perturbations. While PR is widely regarded as a practical complement to AR, dedicated training methods for improving PR are still relatively underexplored, albeit with emerging progress. Among the few PR-targeted training methods, we identify three limitations: i non-comparable evaluation protocols; ii limited comparisons to strong AT baselines despite anecdotal PR gains from AT; and iii no unified framework to compare the generalization of these methods. Thus, we introduce PRBench, the first benchmark dedicated to evaluating improvements in PR achieved by different robustness training methods. PRBench empirically compares most common AT and PR-targeted training methods using a comprehensive set of metrics, including clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). We also provide theoretical analysis on the GE of PR performance across different training methods. Main findings revealed by PRBench include: AT methods are more versatile than PR-targeted training methods in terms of improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower GE and higher clean accuracy. A leaderboard comprising 222 trained models across 7 datasets and 10 model architectures is publicly available at <a href="https://tmpspace.github.io/PRBenchLeaderboard/">https://tmpspace.github.io/PRBenchLeaderboard/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toward Strategy Identification and Subtask Decomposition In Task Exploration</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01728v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01728v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01728v1">http://arxiv.org/abs/2511.01728v1</a><br>This research builds on work in anticipatory human-machine interaction, a subfield of human-machine interaction where machines can facilitate advantageous interactions by anticipating a user’s future state. The aim of this research is to further a machine’s understanding of user knowledge, skill, and behavior in pursuit of implicit coordination. A task explorer pipeline was developed that uses clustering techniques, paired with factor analysis and string edit distance, to automatically identify key global and local strategies that are used to complete tasks. Global strategies identify generalized sets of actions used to complete tasks, while local strategies identify sequences that used those sets of actions in a similar composition. Additionally, meaningful subtasks of various lengths are identified within the tasks. The task explorer pipeline was able to automatically identify key strategies used to complete tasks and encode user runs with hierarchical subtask structures. In addition, a Task Explorer application was developed to easily review pipeline results. The task explorer pipeline can be easily modified to any action-based time-series data and the identified strategies and subtasks help to inform humans and machines on user knowledge, skill, and behavior.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01730v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01730v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01730v2">http://arxiv.org/abs/2511.01730v2</a><br>Pneumonia remains a leading cause of morbidity and mortality worldwide, necessitating accurate and efficient automated detection systems. While recent transformer-based detectors like RT-DETR have shown promise in object detection tasks, their application to medical imaging, particularly pneumonia detection in chest X-rays, remains underexplored. This paper presents CGF-DETR, an enhanced real-time detection transformer specifically designed for pneumonia detection. We introduce XFABlock in the backbone to improve multi-scale feature extraction through convolutional attention mechanisms integrated with CSP architecture. To achieve efficient feature aggregation, we propose SPGA module that replaces standard multi-head attention with dynamic gating mechanisms and single-head self-attention. Additionally, GCFC3 is designed for the neck to enhance feature representation through multi-path convolution fusion while maintaining real-time performance via structural re-parameterization. Extensive experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR achieves 82.2% <a href="mailto:&#109;&#65;&#80;&#x40;&#48;&#x2e;&#x35;">mAP@0.5</a>, outperforming the baseline RT-DETR-l by 3.7% while maintaining comparable inference speed at 48.1 FPS. Our ablation studies confirm that each proposed module contributes meaningfully to the overall performance improvement, with the complete model achieving 50.4% mAP@[0.5:0.95]</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01756v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01756v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01756v1">http://arxiv.org/abs/2511.01756v1</a><br>2D-to-3D human pose lifting is a fundamental challenge for 3D human pose estimation in monocular video, where graph convolutional networks (GCNs) and attention mechanisms have proven to be inherently suitable for encoding the spatial-temporal correlations of skeletal joints. However, depth ambiguity and errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous studies have attempted to restrict jitters in the time domain, for instance, by constraining the differences between adjacent frames while neglecting the global spatial-temporal correlations of skeletal joint motion. To tackle this problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid feature aggregation and 3D trajectory consistency in the frequency domain. Specifically, we propose a hop-hybrid graph attention (HGA) module and a Transformer encoder to model global joint spatial-temporal correlations. The HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group to enlarge the receptive field and applies the attention mechanism to discover the latent correlations of these groups globally. We then exploit global temporal correlations by constraining trajectory consistency in the frequency domain. To provide 3D information for depth inference across frames and maintain coherence over time, a preliminary network is applied to estimate the 3D pose. Extensive experiments were conducted on two standard benchmark datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional accuracy and temporal consistency.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3EED: Ground Everything Everywhere in 3D</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01755v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01755v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01755v1">http://arxiv.org/abs/2511.01755v1</a><br>Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes – 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01767v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01767v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01767v1">http://arxiv.org/abs/2511.01767v1</a><br>In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at <a href="https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus">https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01768v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01768v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01768v1">http://arxiv.org/abs/2511.01768v1</a><br>Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at <a href="https://github.com/happinesslz/UniLION">https://github.com/happinesslz/UniLION</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01775v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01775v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01775v1">http://arxiv.org/abs/2511.01775v1</a><br>Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct “plausibility gap”: while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fractional Diffusion Bridge Models</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01795v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01795v1">http://arxiv.org/abs/2511.01795v1</a><br>We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr&quot;{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fr&#39;echet Inception Distance (FID) in unpaired image translation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01802v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01802v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01802v1">http://arxiv.org/abs/2511.01802v1</a><br>Retrieval-Augmented Generation (RAG) has become a robust framework for enhancing Large Language Models (LLMs) with external knowledge. Recent advances in RAG have investigated graph based retrieval for intricate reasoning; however, the influence of prompt design on enhancing the retrieval and reasoning process is still considerably under-examined. In this paper, we present a prompt-driven GraphRAG framework that underscores the significance of prompt formulation in facilitating entity extraction, fact selection, and passage reranking for multi-hop question answering. Our approach creates a symbolic knowledge graph from text data by encoding entities and factual relationships as structured facts triples. We use LLMs selectively during online retrieval to perform semantic filtering and answer generation. We also use entity-guided graph traversal through Personalized PageRank (PPR) to support efficient, scalable retrieval based on the knowledge graph we built. Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA, with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%, respectively. These results show that prompt design is an important part of improving retrieval accuracy and response quality. This research lays the groundwork for more efficient and comprehensible multi-hop question-answering systems, highlighting the importance of prompt-aware graph reasoning.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01817v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01817v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01817v1">http://arxiv.org/abs/2511.01817v1</a><br>The ability to connect visual patterns with the processes that form them represents one of the deepest forms of visual understanding. Textures of clouds and waves, the growth of cities and forests, or the formation of materials and landscapes are all examples of patterns emerging from underlying mechanisms. We present the Scitextures dataset, a large-scale collection of textures and visual patterns from all domains of science, tech, and art, along with the models and code that generate these images. Covering over 1,200 different models and 100,000 images of patterns and textures from physics, chemistry, biology, sociology, technology, mathematics, and art, this dataset offers a way to explore the connection between the visual patterns that shape our world and the mechanisms that produce them. Created by an agentic AI pipeline that autonomously collects and implements models in standardized form, we use SciTextures to evaluate the ability of leading AI models to link visual patterns to the models and code that generate them, and to identify different patterns that emerged from the same process. We also test AIs ability to infer and recreate the mechanisms behind visual patterns by providing a natural image of a real-world pattern and asking the AI to identify, model, and code the mechanism that formed the pattern, then run this code to generate a simulated image that is compared to the real image. These benchmarks show that vision-language models (VLMs) can understand and simulate the physical system beyond a visual pattern. The dataset and code are available at: <a href="https://zenodo.org/records/17485502">https://zenodo.org/records/17485502</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01990v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01990v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01990v2">http://arxiv.org/abs/2511.01990v2</a><br>Geo-Foundational Models (GFMs) enable fast and reliable extraction of spatiotemporal information from satellite imagery, improving flood inundation mapping by leveraging location and time embeddings. Despite their potential, it remains unclear whether GFMs outperform traditional models like U-Net. A systematic comparison across sensors and data availability scenarios is still lacking, which is an essential step to guide end-users in model selection. To address this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (a Prithvi variant), against TransNorm, U-Net, and Attention U-Net using PlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performance among all GFMs, with only 2-5% variation between the best and worst models across sensors. Clay outperforms others on PlanetScope (0.79 mIoU) and Sentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). In leave-one-region-out cross-validation across five regions, Clay shows slightly better performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07), 0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA (0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, and Sentinel-1, respectively. Across all 19 sites, leave-one-region-out cross-validation reveals a 4% improvement by Clay compared to U-Net. Visual inspection highlights Clay’s superior ability to retain fine details. Few-shot experiments show Clay achieves 0.64 mIoU on PlanetScope with just five training images, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computational time, Clay is a better choice due to its smaller model size (26M parameters), making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M). Contrary to previous findings, our results suggest GFMs offer small to moderate improvements in flood mapping accuracy at lower computational cost and labeling effort compared to traditional U-Net.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Locally-Supervised Global Image Restoration</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01998v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01998v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01998v1">http://arxiv.org/abs/2511.01998v1</a><br>We address the problem of image reconstruction from incomplete measurements, encompassing both upsampling and inpainting, within a learning-based framework. Conventional supervised approaches require fully sampled ground truth data, while self-supervised methods allow incomplete ground truth but typically rely on random sampling that, in expectation, covers the entire image. In contrast, we consider fixed, deterministic sampling patterns with inherently incomplete coverage, even in expectation. To overcome this limitation, we exploit multiple invariances of the underlying image distribution, which theoretically allows us to achieve the same reconstruction performance as fully supervised approaches. We validate our method on optical-resolution image upsampling in photoacoustic microscopy (PAM), demonstrating competitive or superior results while requiring substantially less ground truth data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_02014v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_02014v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02014v1">http://arxiv.org/abs/2511.02014v1</a><br>The detection of Protected Health Information (PHI) in medical imaging is critical for safeguarding patient privacy and ensuring compliance with regulatory frameworks. Traditional detection methodologies predominantly utilize Optical Character Recognition (OCR) models in conjunction with named entity recognition. However, recent advancements in Large Multimodal Model (LMM) present new opportunities for enhanced text extraction and semantic analysis. In this study, we systematically benchmark three prominent closed and open-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing two distinct pipeline configurations: one dedicated to text analysis alone and another integrating both OCR and semantic analysis. Our results indicate that LMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to conventional models like EasyOCR. However, this improvement in OCR performance does not consistently correlate with enhanced overall PHI detection accuracy. The strongest performance gains are observed on test cases with complex imprint patterns. In scenarios where text regions are well readable with sufficient contrast, and strong LMMs are employed for text analysis after OCR, different pipeline configurations yield similar results. Furthermore, we provide empirically grounded recommendations for LMM selection tailored to specific operational constraints and propose a deployment strategy that leverages scalable and modular infrastructure.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_02046v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_02046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02046v1">http://arxiv.org/abs/2511.02046v1</a><br>Creation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_01833v2/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_01833v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01833v2">http://arxiv.org/abs/2511.01833v2</a><br>The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_02065v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_02065v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02065v1">http://arxiv.org/abs/2511.02065v1</a><br>Opto-electronic neural networks integrate optical front-ends with electronic back-ends to enable fast and energy-efficient vision. However, conventional end-to-end optimization of both the optical and electronic modules is limited by costly simulations and large parameter spaces. We introduce a two-stage strategy for designing opto-electronic convolutional neural networks (CNNs): first, train a standard electronic CNN, then realize the optical front-end implemented as a metasurface array through direct kernel optimization of its first convolutional layer. This approach reduces computational and memory demands by hundreds of times and improves training stability compared to end-to-end optimization. On monocular depth estimation, the proposed two-stage design achieves twice the accuracy of end-to-end training under the same training time and resource constraints.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study</title>
    <link href="/2025/11/03/cs.CV/2025-11-03-2511_02086v1/"/>
    <url>/2025/11/03/cs.CV/2025-11-03-2511_02086v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02086v1">http://arxiv.org/abs/2511.02086v1</a><br>Purpose: In this paper, we develop and clinically evaluate a depth-only, markerless augmented reality (AR) registration pipeline on a head-mounted display, and assess accuracy across small or low-curvature anatomies in real-life operative settings. Methods: On HoloLens 2, we align Articulated HAnd Tracking (AHAT) depth to Computed Tomography (CT)-derived skin meshes via (i) depth-bias correction, (ii) brief human-in-the-loop initialization, (iii) global and local registration. We validated the surface-tracing error metric by comparing “skin-to-bone” relative distances to CT ground truth on leg and foot models, using an AR-tracked tool. We then performed seven intraoperative target trials (feet x2, ear x3, leg x2) during the initial stage of fibula free-flap harvest and mandibular reconstruction surgery, and collected 500+ data per trial. Results: Preclinical validation showed tight agreement between AR-traced and CT distances (leg: median |Delta d| 0.78 mm, RMSE 0.97 mm; feet: 0.80 mm, 1.20 mm). Clinically, per-point error had a median of 3.9 mm. Median errors by anatomy were 3.2 mm (feet), 4.3 mm (ear), and 5.3 mm (lower leg), with 5 mm coverage 92-95%, 84-90%, and 72-86%, respectively. Feet vs. lower leg differed significantly (Delta median ~1.1 mm; p &lt; 0.001). Conclusion: A depth-only, markerless AR pipeline on HMDs achieved ~3-4 mm median error across feet, ear, and lower leg in live surgical settings without fiducials, approaching typical clinical error thresholds for moderate-risk tasks. Human-guided initialization plus global-to-local registration enabled accurate alignment on small or low-curvature targets, improving the clinical readiness of markerless AR guidance.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
    <link href="/2025/11/03/eess.IV/2025-11-03-2511_01140v1/"/>
    <url>/2025/11/03/eess.IV/2025-11-03-2511_01140v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01140v1">http://arxiv.org/abs/2511.01140v1</a><br>Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization</title>
    <link href="/2025/11/03/eess.IV/2025-11-03-2511_02065v1/"/>
    <url>/2025/11/03/eess.IV/2025-11-03-2511_02065v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02065v1">http://arxiv.org/abs/2511.02065v1</a><br>Opto-electronic neural networks integrate optical front-ends with electronic back-ends to enable fast and energy-efficient vision. However, conventional end-to-end optimization of both the optical and electronic modules is limited by costly simulations and large parameter spaces. We introduce a two-stage strategy for designing opto-electronic convolutional neural networks (CNNs): first, train a standard electronic CNN, then realize the optical front-end implemented as a metasurface array through direct kernel optimization of its first convolutional layer. This approach reduces computational and memory demands by hundreds of times and improves training stability compared to end-to-end optimization. On monocular depth estimation, the proposed two-stage design achieves twice the accuracy of end-to-end training under the same training time and resource constraints.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Boosting performance of computer vision applications through embedded GPUs on the edge</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01129v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01129v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01129v1">http://arxiv.org/abs/2511.01129v1</a><br>Computer vision applications, especially those using augmented reality technology, are becoming quite popular in mobile devices. However, this type of application is known as presenting significant demands regarding resources. In order to enable its utilization in devices with more modest resources, edge computing can be used to offload certain high intensive tasks. Still, edge computing is usually composed of devices with limited capacity, which may impact in users quality of experience when using computer vision applications. This work proposes the use of embedded devices with graphics processing units (GPUs) to overcome such limitation. Experiments performed shown that GPUs can attain a performance gain when compared to using only CPUs, which guarantee a better experience to users using such kind of application.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01131v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01131v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01131v1">http://arxiv.org/abs/2511.01131v1</a><br>Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Architectures for Human Activity Recognition</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01139v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01139v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01139v2">http://arxiv.org/abs/2511.01139v2</a><br>We propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. We introduce a symmetry category that jointly represents cyclic time shifts, positive gain scalings, and the sensor-hierarchy poset, capturing the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01140v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01140v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01140v1">http://arxiv.org/abs/2511.01140v1</a><br>Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Web-Scale Collection of Video Data for 4D Animal Reconstruction</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01169v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01169v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01169v1">http://arxiv.org/abs/2511.01169v1</a><br>Computer vision for animals holds great promise for wildlife research but often depends on large-scale data, while existing collection methods rely on controlled capture setups. Recent data-driven approaches show the potential of single-view, non-invasive analysis, yet current animal video datasets are limited–offering as few as 2.4K 15-frame clips and lacking key processing for animal-centric 3D&#x2F;4D tasks. We introduce an automated pipeline that mines YouTube videos and processes them into object-centric clips, along with auxiliary annotations valuable for downstream tasks like pose estimation, tracking, and 3D&#x2F;4D reconstruction. Using this pipeline, we amass 30K videos (2M frames)–an order of magnitude more than prior works. To demonstrate its utility, we focus on the 4D quadruped animal reconstruction task. To support this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually filtered sequences with 11K frames showcasing clean, diverse animal motions. We evaluate state-of-the-art model-based and model-free methods on Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic 3D shapes, while the latter yields more natural reconstructions but scores lower–revealing a gap in current evaluation. To address this, we enhance a recent model-free approach with sequence-level optimization, establishing the first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and baseline aim to advance large-scale, markerless 4D animal reconstruction and related tasks from in-the-wild videos. Code and datasets are available at <a href="https://github.com/briannlongzhao/Animal-in-Motion">https://github.com/briannlongzhao/Animal-in-Motion</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>I.2.10; I.4.5</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01163v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01163v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01163v1">http://arxiv.org/abs/2511.01163v1</a><br>Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01175v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01175v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01175v2">http://arxiv.org/abs/2511.01175v2</a><br>Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image superresolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multiscale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multiscale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in low-frequency and high-frequency sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01186v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01186v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01186v1">http://arxiv.org/abs/2511.01186v1</a><br>Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01194v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01194v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01194v1">http://arxiv.org/abs/2511.01194v1</a><br>Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07 (Artificial neural networks and deep learning), 68U10
  (Computer graphics, computational geometry)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoSa: Motion Generation with Scalable Autoregressive Modeling</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01200v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01200v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01200v1">http://arxiv.org/abs/2511.01200v1</a><br>We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask’s 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at <a href="https://mosa-web.github.io/MoSa-web">https://mosa-web.github.io/MoSa-web</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01210v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01210v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01210v2">http://arxiv.org/abs/2511.01210v2</a><br>Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01213v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01213v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01213v1">http://arxiv.org/abs/2511.01213v1</a><br>The immense diversity in the culture and culinary of Indian cuisines calls attention to the major shortcoming of the existing Visual Question Answering(VQA) systems which are inclined towards the foods from Western region. Recent attempt towards building a VQA dataset for Indian food is a step towards addressing this challenge. However, their approach towards VQA follows a two-step process in which the answer is generated first, followed by the explanation of the expected answer. In this work, we claim that food VQA requires to follow a multi-step reasoning process to arrive at an accurate answer, especially in the context of India food, which involves understanding complex culinary context and identifying relationships between various food items. With this hypothesis we create reasoning chains upon the QA with minimal human intervention. We fine-tune smaller LLMs and VLMs with auto-validated reasoning chains and further train them using reinforcement learning with larger data. With augmentation of reasoning chains, we observed accuracy improvement of an average 10 percentage points on the baseline. We provide detailed analysis in terms the effect of addition of reasoning chains for the Indian Food VQA task.   Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge Graph.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01223v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01223v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01223v1">http://arxiv.org/abs/2511.01223v1</a><br>Domain adaptation is required for automated driving models to generalize well across diverse road conditions. This paper explores a training method for domain adaptation to adapt PilotNet, an end-to-end deep learning-based model, for left-hand driving conditions using real-world Australian highway data. Four training methods were evaluated: (1) a baseline model trained on U.S. right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a model pretrained on flipped U.S. data and then finetuned on Australian highways. This setup examines whether incorporating flipped data enhances the model adaptation by providing an initial left-hand driving alignment. The paper compares model performance regarding steering prediction accuracy and attention, using saliency-based analysis to measure attention shifts across significant road regions. Results show that pretraining on flipped data alone worsens prediction stability due to misaligned feature representations, but significantly improves adaptation when followed by fine-tuning, leading to lower prediction error and stronger focus on left-side cues. To validate this approach across different architectures, the same experiments were done on ResNet, which confirmed similar adaptation trends. These findings emphasize the importance of preprocessing techniques, such as flipped-data pretraining, followed by fine-tuning to improve model adaptation with minimal retraining requirements.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01233v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01233v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01233v1">http://arxiv.org/abs/2511.01233v1</a><br>We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models – each trained by its original authors – across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies – enabling new evaluations without model reimplementation required – alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
      <tag>cs.GR</tag>
      
      <tag>I.3; I.2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Eyes on Target: Gaze-Aware Object Detection in Egocentric Video</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01237v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01237v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01237v1">http://arxiv.org/abs/2511.01237v1</a><br>Human gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01243v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01243v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01243v1">http://arxiv.org/abs/2511.01243v1</a><br>Brain lesion segmentation remains challenging due to small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities. We propose CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and trains only lightweight adapters for efficient fine-tuning. At its core is the CenterMamba encoder, which employs a novel 3x3 corner-axis-center short-sequence scanning strategy to enable center-prioritized, axis-reinforced, and diagonally compensated information aggregation. This design enhances sensitivity to weak boundaries and tiny foci while maintaining sparse yet effective feature representation. A memory-driven structural prompt generator maintains a prototype bank across neighboring slices, enabling automatic synthesis of reliable prompts without user interaction, thereby improving inter-slice coherence. The memory-augmented multi-scale decoder integrates memory attention modules at multiple levels, combining deep supervision with progressive refinement to restore fine details while preserving global consistency. Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01240v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01240v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01240v1">http://arxiv.org/abs/2511.01240v1</a><br>Transferable attacks generate adversarial examples on surrogate models to fool unknown victim models, posing real-world threats and growing research interest. Despite focusing on flat losses for transferable adversarial examples, recent studies still fall into suboptimal regions, especially the flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce a novel black-box gradient-based transferable attack from a perspective of dual-order information. Specifically, we feasibly propose Adversarial Flatness (AF) to the deceptive flatness problem and a theoretical assurance for adversarial transferability. Based on this, using an efficient approximation of our objective, we instantiate our attack as Adversarial Flatness Attack (AFA), addressing the altered gradient sign issue. Additionally, to further improve the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by enhancing the inner-loop sampling efficiency. The comprehensive results on ImageNet-compatible dataset demonstrate superiority over six baselines, generating adversarial examples in flatter regions and boosting transferability across model architectures. When tested on input transformation attacks or the Baidu Cloud API, our method outperforms baselines.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01250v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01250v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01250v2">http://arxiv.org/abs/2511.01250v2</a><br>LiDAR semantic segmentation degrades in adverse weather because refraction, scattering, and point dropouts corrupt geometry. Prior work in weather simulation, mixing-based augmentation, domain randomization, and uncertainty or boundary regularization improves robustness but still overlooks structural vulnerabilities near boundaries, corners, and sparse regions. We present a Light Geometry-aware adapter. The module aligns azimuth and applies horizontal circular padding to preserve neighbor continuity across the 0~360 degree wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points and computes simple local statistics, which are compressed into compact geometry-aware cues. During training, these cues drive region-aware regularization that stabilizes predictions in structurally fragile areas. The adapter is plug and play, complements augmentation, and can be enabled only during training with negligible inference cost. We adopt a source-only cross-weather setup where models train on SemanticKITTI and are evaluated on SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by 7.9 percentage points over the data-centric augmentation baseline and by 0.6 points over the class-centric regularization baseline. These results indicate that geometry-driven regularization is a key direction for all-weather LiDAR segmentation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MotionStream: Real-Time Video Generation with Interactive Motion Controls</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01266v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01266v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01266v1">http://arxiv.org/abs/2511.01266v1</a><br>Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01274v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01274v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01274v1">http://arxiv.org/abs/2511.01274v1</a><br>Ancient Chinese paintings are a valuable cultural heritage that is damaged by irreversible color degradation. Reviving color-degraded paintings is extraordinarily difficult due to the complex chemistry mechanism. Progress is further slowed by the lack of comprehensive, high-quality datasets, which hampers the creation of end-to-end digital restoration tools. To revive colors, we propose PRevivor, a prior-guided color transformer that learns from recent paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and Song Dynasty). To develop PRevivor, we decompose color restoration into two sequential sub-tasks: luminance enhancement and hue correction. For luminance enhancement, we employ two variational U-Nets and a multi-scale mapping module to translate faded luminance into restored counterparts. For hue correction, we design a dual-branch color query module guided by localized hue priors extracted from faded paintings. Specifically, one branch focuses attention on regions guided by masked priors, enforcing localized hue correction, whereas the other branch remains unconstrained to maintain a global reasoning capability. To evaluate PRevivor, we conduct extensive experiments against state-of-the-art colorization methods. The results demonstrate superior performance both quantitatively and qualitatively.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01284v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01284v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01284v1">http://arxiv.org/abs/2511.01284v1</a><br>Foundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Detecting Generated Images by Fitting Natural Image Distributions</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01293v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01293v1">http://arxiv.org/abs/2511.01293v1</a><br>The increasing realism of generated images has raised significant concerns about their potential misuse, necessitating robust detection methods. Current approaches mainly rely on training binary classifiers, which depend heavily on the quantity and quality of available generated images. In this work, we propose a novel framework that exploits geometric differences between the data manifolds of natural and generated images. To exploit this difference, we employ a pair of functions engineered to yield consistent outputs for natural images but divergent outputs for generated ones, leveraging the property that their gradients reside in mutually orthogonal subspaces. This design enables a simple yet effective detection method: an image is identified as generated if a transformation along its data manifold induces a significant change in the loss value of a self-supervised model pre-trained on natural images. Further more, to address diminishing manifold disparities in advanced generative models, we leverage normalizing flows to amplify detectable differences by extruding generated images away from the natural image manifold. Extensive experiments demonstrate the efficacy of this method. Code is available at <a href="https://github.com/tmlr-group/ConV">https://github.com/tmlr-group/ConV</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01294v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01294v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01294v2">http://arxiv.org/abs/2511.01294v2</a><br>A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniREditBench: A Unified Reasoning-based Image Editing Benchmark</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01295v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01295v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01295v1">http://arxiv.org/abs/2511.01295v1</a><br>Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01304v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01304v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01304v2">http://arxiv.org/abs/2511.01304v2</a><br>Multiple instance learning (MIL) has been widely used for representing whole-slide pathology images. However, spatial, semantic, and decision entanglements among instances limit its representation and interpretability. To address these challenges, we propose a latent factor grouping-boosted cluster-reasoning instance disentangled learning framework for whole-slide image (WSI) interpretable representation in three phases. First, we introduce a novel positive semi-definite latent factor grouping that maps instances into a latent subspace, effectively mitigating spatial entanglement in MIL. To alleviate semantic entanglement, we employs instance probability counterfactual inference and optimization via cluster-reasoning instance disentangling. Finally, we employ a generalized linear weighted decision via instance effect re-weighting to address decision entanglement. Extensive experiments on multicentre datasets demonstrate that our model outperforms all state-of-the-art models. Moreover, it attains pathologist-aligned interpretability through disentangled representations and a transparent decision-making process.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>REASON: Probability map-guided dual-branch fusion framework for gastric content assessment</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01302v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01302v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01302v1">http://arxiv.org/abs/2511.01302v1</a><br>Accurate assessment of gastric content from ultrasound is critical for stratifying aspiration risk at induction of general anesthesia. However, traditional methods rely on manual tracing of gastric antra and empirical formulas, which face significant limitations in both efficiency and accuracy. To address these challenges, a novel two-stage probability map-guided dual-branch fusion framework (REASON) for gastric content assessment is proposed. In stage 1, a segmentation model generates probability maps that suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch classifier fuses information from two standard views, right lateral decubitus (RLD) and supine (SUP), to improve the discrimination of learned features. Experimental results on a self-collected dataset demonstrate that the proposed framework outperforms current state-of-the-art approaches by a significant margin. This framework shows great promise for automated preoperative aspiration risk assessment, offering a more robust, efficient, and accurate solution for clinical practice.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01307v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01307v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01307v1">http://arxiv.org/abs/2511.01307v1</a><br>Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at <a href="https://github.com/KU-VGI/APDM">https://github.com/KU-VGI/APDM</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MVSMamba: Multi-View Stereo with State Space Model</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01315v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01315v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01315v1">http://arxiv.org/abs/2511.01315v1</a><br>Robust feature representations are essential for learning-based Multi-View Stereo (MVS), which relies on accurate feature matching. Recent MVS methods leverage Transformers to capture long-range dependencies based on local features extracted by conventional feature pyramid networks. However, the quadratic complexity of Transformer-based MVS methods poses challenges to balance performance and efficiency. Motivated by the global modeling capability and linear complexity of the Mamba architecture, we propose MVSMamba, the first Mamba-based MVS network. MVSMamba enables efficient global feature aggregation with minimal computational overhead. To fully exploit Mamba’s potential in MVS, we propose a Dynamic Mamba module (DM-module) based on a novel reference-centered dynamic scanning strategy, which enables: (1) Efficient intra- and inter-view feature interaction from the reference to source views, (2) Omnidirectional multi-view feature representations, and (3) Multi-scale global feature aggregation. Extensive experimental results demonstrate MVSMamba outperforms state-of-the-art MVS methods on the DTU dataset and the Tanks-and-Temples benchmark with both superior performance and efficiency. The source code is available at <a href="https://github.com/JianfeiJ/MVSMamba">https://github.com/JianfeiJ/MVSMamba</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01317v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01317v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01317v1">http://arxiv.org/abs/2511.01317v1</a><br>The rapid growth of deep learning has brought about powerful models that can handle various tasks, like identifying images and understanding language. However, adversarial attacks, an unnoticed alteration, can deceive models, leading to inaccurate predictions. In this paper, a generative adversarial attack method is proposed that uses the CLIP model to create highly effective and visually imperceptible adversarial perturbations. The CLIP model’s ability to align text and image representation helps incorporate natural language semantics with a guided loss to generate effective adversarial examples that look identical to the original inputs. This integration allows extensive scene manipulation, creating perturbations in multi-object environments specifically designed to deceive multilabel classifiers. Our approach integrates the concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with the dissimilar text embeddings similar to Generative Adversarial Multi-Object Scene Attacks (GAMA), resulting in perturbations that both deceive classification models and maintain high structural similarity to the original images. The model was tested on various tasks across diverse black-box victim models. The experimental results show that our method performs competitively, achieving comparable or superior results to existing techniques, while preserving greater visual fidelity.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01328v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01328v1">http://arxiv.org/abs/2511.01328v1</a><br>Medical image segmentation is essential for computer-assisted diagnosis and treatment planning, yet substantial anatomical variability and boundary ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet, a segmentation network that unifies local modeling with global context to strengthen boundary delineation and detail preservation. RDTE-UNet employs a hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for adaptive boundary enhancement, HVDA for fine-grained feature modeling, and EulerFF for fusion weighting guided by Euler’s formula. Together, these components improve structural consistency and boundary accuracy across morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has achieved a comparable level in terms of segmentation accuracy and boundary quality.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>$&#92;left|&#92;,&#92;circlearrowright&#92;,&#92;boxed{&#92;text{BUS}}&#92;,&#92;right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01340v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01340v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01340v1">http://arxiv.org/abs/2511.01340v1</a><br>Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$, a large and diverse benchmark of $1,333$ English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$ by $2.1-4.1%$ and $20-30%$ using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01345v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01345v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01345v1">http://arxiv.org/abs/2511.01345v1</a><br>Accurate segmentation of medical images is fundamental to tumor diagnosis and treatment planning. SAM-based interactive segmentation has gained attention for its strong generalization, but most methods follow a single-point-to-single-object paradigm, which limits multi-lesion segmentation. Moreover, ViT backbones capture global context but often miss high-fidelity local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework with a competitive query optimization strategy that shifts from single-point-to-single-mask to single-point-to-multi-instance. A prompt-conditioned instance-query generator transforms a single point prompt into multiple specialized queries, enabling retrieval of all semantically similar lesions across the 3D volume from a single exemplar. A hybrid CNN-Transformer encoder injects CNN-derived boundary saliency into ViT self-attention via spatial gating. A competitively optimized query decoder then enables end-to-end, parallel, multi-instance prediction through inter-query competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels and exhibits strong robustness to prompts, providing a practical solution for efficient annotation of clinically relevant multi-lesion cases.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01355v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01355v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01355v1">http://arxiv.org/abs/2511.01355v1</a><br>Recent advancements in text-to-image diffusion models have significantly improved the personalization and stylization of generated images. However, previous studies have only assessed content similarity under a single style intensity. In our experiments, we observe that increasing style intensity leads to a significant loss of content features, resulting in a suboptimal content-style frontier. To address this, we propose a novel approach to expand the content-style frontier by leveraging Content-Style Subspace Blending and a Content-Style Balance loss. Our method improves content similarity across varying style intensities, significantly broadening the content-style frontier. Extensive experiments demonstrate that our approach outperforms existing techniques in both qualitative and quantitative evaluations, achieving superior content-style trade-off with significantly lower Inverted Generational Distance (IGD) and Generational Distance (GD) scores compared to current methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EREBUS: End-to-end Robust Event Based Underwater Simulation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01381v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01381v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01381v1">http://arxiv.org/abs/2511.01381v1</a><br>The underwater domain presents a vast array of challenges for roboticists and computer vision researchers alike, such as poor lighting conditions and high dynamic range scenes. In these adverse conditions, traditional vision techniques struggle to adapt and lead to suboptimal performance. Event-based cameras present an attractive solution to this problem, mitigating the issues of traditional cameras by tracking changes in the footage on a frame-by-frame basis. In this paper, we introduce a pipeline which can be used to generate realistic synthetic data of an event-based camera mounted to an AUV (Autonomous Underwater Vehicle) in an underwater environment for training vision models. We demonstrate the effectiveness of our pipeline using the task of rock detection with poor visibility and suspended particulate matter, but the approach can be generalized to other underwater tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01390v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01390v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01390v1">http://arxiv.org/abs/2511.01390v1</a><br>Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23%-86% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at <a href="https://github.com/Sweet4tars/seps.git">https://github.com/Sweet4tars/seps.git</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01399v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01399v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01399v1">http://arxiv.org/abs/2511.01399v1</a><br>Inventory management of firefighting assets is crucial for emergency preparedness, risk assessment, and on-site fire response. However, conventional methods are inefficient due to limited capabilities in automated asset recognition and reconstruction. To address the challenge, this research introduces the Fire-ART dataset and develops a panoramic image-based reconstruction approach for semantic enrichment of firefighting assets into BIM models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626 images and 6,627 instances, making it an extensive and publicly accessible dataset for asset recognition. In addition, the reconstruction approach integrates modified cube-map conversion and radius-based spherical camera projection to enhance recognition and localization accuracy. Through validations with two real-world case studies, the proposed approach achieves F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters, respectively. The Fire-ART dataset and the reconstruction approach offer valuable resources and robust technical solutions to enhance the accurate digital management of fire safety equipment.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Extremal Contours: Gradient-driven contours for compact visual attribution</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01411v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01411v1">http://arxiv.org/abs/2511.01411v1</a><br>Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve&#x2F;delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01143v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01143v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01143v1">http://arxiv.org/abs/2511.01143v1</a><br>Early and accurate segmentation of colorectal polyps is critical for reducing colorectal cancer mortality, which has been extensively explored by academia and industry. However, current deep learning-based polyp segmentation models either compromise clinical decision-making by providing ambiguous polyp margins in segmentation outputs or rely on heavy architectures with high computational complexity, resulting in insufficient inference speeds for real-time colorectal endoscopic applications. To address this problem, we propose MicroAUNet, a light-weighted attention-based segmentation network that combines depthwise-separable dilated convolutions with a single-path, parameter-shared channel-spatial attention block to strengthen multi-scale boundary features. On the basis of it, a progressive two-stage knowledge-distillation scheme is introduced to transfer semantic and boundary cues from a high-capacity teacher. Extensive experiments on benchmarks also demonstrate the state-of-the-art accuracy under extremely low model complexity, indicating that MicroAUNet is suitable for real-time clinical polyp segmentation. The code is publicly available at <a href="https://github.com/JeremyXSC/MicroAUNet">https://github.com/JeremyXSC/MicroAUNet</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01357v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01357v1">http://arxiv.org/abs/2511.01357v1</a><br>Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model’s capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at <a href="https://github.com/BioMedIA-repo/CMI-MTL">https://github.com/BioMedIA-repo/CMI-MTL</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards One-step Causal Video Generation via Adversarial Self-Distillation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01419v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01419v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01419v1">http://arxiv.org/abs/2511.01419v1</a><br>Recent hybrid video generation models combine autoregressive temporal dynamics with diffusion-based spatial denoising, but their sequential, iterative nature leads to error accumulation and long inference times. In this work, we propose a distillation-based framework for efficient causal video generation that enables high-quality synthesis with extremely limited denoising steps. Our approach builds upon the Distribution Matching Distillation (DMD) framework and proposes a novel Adversarial Self-Distillation (ASD) strategy, which aligns the outputs of the student model’s n-step denoising process with its (n+1)-step version at the distribution level. This design provides smoother supervision by bridging small intra-student gaps and more informative guidance by combining teacher knowledge with locally consistent student behavior, substantially improving training stability and generation quality in extremely few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame Enhancement (FFE) strategy, which allocates more denoising steps to the initial frames to mitigate error propagation while applying larger skipping steps to later frames. Extensive experiments on VBench demonstrate that our method surpasses state-of-the-art approaches in both one-step and two-step video generation. Notably, our framework produces a single distilled model that flexibly supports multiple inference-step settings, eliminating the need for repeated re-distillation and enabling efficient, high-quality video synthesis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01425v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01425v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01425v1">http://arxiv.org/abs/2511.01425v1</a><br>Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18% compared to a non-interactive baseline. To validate the faithfulness of the agent’s explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\Delta$Brier&#x3D;+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>I.2.6; I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniSOT: A Unified Framework for Multi-Modality Single Object Tracking</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01427v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01427v1">http://arxiv.org/abs/2511.01427v1</a><br>Single object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0% main metric across all three RGB+X video modalities.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01434v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01434v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01434v1">http://arxiv.org/abs/2511.01434v1</a><br>Off-road semantic segmentation suffers from thick, inconsistent boundaries, sparse supervision for rare classes, and pervasive label noise. Designs that fuse only at low resolution blur edges and propagate local errors, whereas maintaining high-resolution pathways or repeating high-resolution fusions is costly and fragile to noise. We introduce a resolutionaware token decoder that balances global semantics, local consistency, and boundary fidelity under imperfect supervision. Most computation occurs at a low-resolution bottleneck; a gated cross-attention injects fine-scale detail, and only a sparse, uncertainty-selected set of pixels is refined. The components are co-designed and tightly integrated: global self-attention with lightweight dilated depthwise refinement restores local coherence; a gated cross-attention integrates fine-scale features from a standard high-resolution encoder stream without amplifying noise; and a class-aware point refinement corrects residual ambiguities with negligible overhead. During training, we add a boundary-band consistency regularizer that encourages coherent predictions in a thin neighborhood around annotated edges, with no inference-time cost. Overall, the results indicate competitive performance and improved stability across transitions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Contrast-Guided Cross-Modal Distillation for Thermal Object Detection</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01435v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01435v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01435v1">http://arxiv.org/abs/2511.01435v1</a><br>Robust perception at night remains challenging for thermal-infrared detection: low contrast and weak high-frequency cues lead to duplicate, overlapping boxes, missed small objects, and class confusion. Prior remedies either translate TIR to RGB and hope pixel fidelity transfers to detection – making performance fragile to color or structure artifacts – or fuse RGB and TIR at test time, which requires extra sensors, precise calibration, and higher runtime cost. Both lines can help in favorable conditions, but do not directly shape the thermal representation used by the detector. We keep mono-modality inference and tackle the root causes during training. Specifically, we introduce training-only objectives that sharpen instance-level decision boundaries by pulling together features of the same class and pushing apart those of different classes – suppressing duplicate and confusing detections – and that inject cross-modal semantic priors by aligning the student’s multi-level pyramid features with an RGB-trained teacher, thereby strengthening texture-poor thermal features without visible input at test time. In experiments, our method outperformed prior approaches and achieved state-of-the-art performance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01449v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01449v1">http://arxiv.org/abs/2511.01449v1</a><br>To effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.   Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01450v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01450v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01450v2">http://arxiv.org/abs/2511.01450v2</a><br>Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO loss to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01458v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01458v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01458v1">http://arxiv.org/abs/2511.01458v1</a><br>Safety and reliability are essential for deploying Visual Question Answering (VQA) in surgery, where incorrect or ambiguous responses can harm the patient. Most surgical VQA research focuses on accuracy or linguistic quality while overlooking safety behaviors such as ambiguity awareness, referral to human experts, or triggering a second opinion. Inspired by Automatic Failure Detection (AFD), we study uncertainty estimation as a key enabler of safer decision making. We introduce Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question semantics into prediction confidence. It measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. We evaluate five models, including domain specific Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models degrade under mild paraphrasing, while LVLMs are more resilient. Across three LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template settings and enhances hallucination detection. The Area Under the ROC Curve (AUROC) increases by 15-38% for zero-shot models, with gains maintained under out-of-template stress. QA-SNNE offers a practical and interpretable step toward AFD in surgical VQA by linking semantic uncertainty to question context. Combining LVLM backbones with question aligned uncertainty estimation can improve safety and clinician trust. The code and model are available at <a href="https://github.com/DennisPierantozzi/QASNNE">https://github.com/DennisPierantozzi/QASNNE</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Efficiently Training A Flat Neural Network Before It has been Quantizated</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01462v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01462v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01462v1">http://arxiv.org/abs/2511.01462v1</a><br>Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error for PTQ. However, it is unclear how to efficiently train a model-agnostic neural network which is tailored for a predefined precision low-bit model. In this paper, we firstly discover that a flat full precision neural network is crucial for low-bit quantization. To achieve this, we propose a framework that proactively pre-conditions the model by measuring and disentangling the error sources. Specifically, both the Activation Quantization Error (AQE) and the Weight Quantization Error (WQE) are statistically modeled as independent Gaussian noises. We study several noise injection optimization methods to obtain a flat minimum. Experimental results attest to the effectiveness of our approach. These results open novel pathways for obtaining low-bit PTQ models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01463v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01463v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01463v1">http://arxiv.org/abs/2511.01463v1</a><br>The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
      <tag>68T45</tag>
      
      <tag>I.2.10; I.3.7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01466v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01466v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01466v1">http://arxiv.org/abs/2511.01466v1</a><br>Deep joint source-channel coding (JSCC) has emerged as a promising paradigm for semantic communication, delivering significant performance gains over conventional separate coding schemes. However, existing JSCC frameworks remain vulnerable to physical-layer adversarial threats, such as pilot spoofing and subcarrier jamming, compromising semantic fidelity. In this paper, we propose SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly enhances the security and robustness of deep JSCC under adversarial wireless environments. Different from prior diffusion-guided JSCC methods that suffer from high inference latency, SecDiff employs pseudoinverse-guided sampling and adaptive guidance weighting, enabling flexible step-size control and efficient semantic reconstruction. To counter jamming attacks, we introduce a power-based subcarrier masking strategy and recast recovery as a masked inpainting problem, solved via diffusion guidance. For pilot spoofing, we formulate channel estimation as a blind inverse problem and develop an expectation-minimization (EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and a channel operator. Notably, our method alternates between pilot recovery and channel estimation, enabling joint refinement of both variables throughout the diffusion process. Extensive experiments over orthogonal frequency-division multiplexing (OFDM) channels under adversarial conditions show that SecDiff outperforms existing secure and generative JSCC baselines by achieving a favorable trade-off between reconstruction quality and computational cost. This balance makes SecDiff a promising step toward practical, low-latency, and attack-resilient semantic communications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01498v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01498v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01498v1">http://arxiv.org/abs/2511.01498v1</a><br>Person re-identification (ReID) plays a pivotal role in computer vision, particularly in surveillance and security applications within IoT-enabled smart environments. This study introduces the Enhanced Pedestrian Alignment Network (EPAN), tailored for robust ReID across diverse IoT surveillance conditions. EPAN employs a dual-branch architecture to mitigate the impact of perspective and environmental changes, extracting alignment information under varying scales and viewpoints. Here, we demonstrate EPAN’s strong feature extraction capabilities, achieving outstanding performance on the Inspection-Personnel dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of 78.82%. This highlights EPAN’s potential for real-world IoT applications, enabling effective and reliable person ReID across diverse cameras in surveillance and security systems. The code and data are available at: <a href="https://github.com/ggboy2580/EPAN">https://github.com/ggboy2580/EPAN</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01502v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01502v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01502v1">http://arxiv.org/abs/2511.01502v1</a><br>Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group&#x2F;DiMoDE upon publication.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01510v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01510v1">http://arxiv.org/abs/2511.01510v1</a><br>Low-light image enhancement (LLIE) faces persistent challenges in balancing reconstruction fidelity with cross-scenario generalization. While existing methods predominantly focus on deterministic pixel-level mappings between paired low&#x2F;normal-light images, they often neglect the continuous physical process of luminance transitions in real-world environments, leading to performance drop when normal-light references are unavailable. Inspired by empirical analysis of natural luminance dynamics revealing power-law distributed intensity transitions, this paper introduces Luminance-Aware Statistical Quantification (LASQ), a novel framework that reformulates LLIE as a statistical sampling process over hierarchical luminance distributions. Our LASQ re-conceptualizes luminance transition as a power-law distribution in intensity coordinate space that can be approximated by stratified power functions, therefore, replacing deterministic mappings with probabilistic sampling over continuous luminance layers. A diffusion forward process is designed to autonomously discover optimal transition paths between luminance layers, achieving unsupervised distribution emulation without normal-light references. In this way, it considerably improves the performance in practical situations, enabling more adaptable and versatile light restoration. This framework is also readily applicable to cases with normal-light references, where it achieves superior performance on domain-specific datasets alongside better generalization-ability across non-reference datasets.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Example-Based Feature Painting on Textures</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01513v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01513v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01513v1">http://arxiv.org/abs/2511.01513v1</a><br>In this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: <a href="https://reality.tf.fau.de/pub/ardelean2025examplebased.html">https://reality.tf.fau.de/pub/ardelean2025examplebased.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01517v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01517v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01517v1">http://arxiv.org/abs/2511.01517v1</a><br>Current text conditioned image generation methods output realistic looking images, but they fail to capture specific styles. Simply finetuning them on the target style datasets still struggles to grasp the style features. In this work, we present a novel contrastive learning framework to improve the stylization capability of large text-to-image diffusion models. Motivated by the astonishing advance in image generation models that makes synthetic data an intrinsic part of model training in various computer vision tasks, we exploit synthetic image generation in our approach. Usually, the generated synthetic data is dependent on the task, and most of the time it is used to enlarge the available real training dataset. With NSYNC, alternatively, we focus on generating negative synthetic sets to be used in a novel contrastive training scheme along with real positive images. In our proposed training setup, we forward negative data along with positive data and obtain negative and positive gradients, respectively. We then refine the positive gradient by subtracting its projection onto the negative gradient to get the orthogonal component, based on which the parameters are updated. This orthogonal component eliminates the trivial attributes that are present in both positive and negative data and directs the model towards capturing a more unique style. Experiments on various styles of painters and illustrators show that our approach improves the performance over the baseline methods both quantitatively and qualitatively. Our code is available at <a href="https://github.com/giddyyupp/NSYNC">https://github.com/giddyyupp/NSYNC</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Driving scenario generation and evaluation using a structured layer representation and foundational models</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01541v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01541v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01541v1">http://arxiv.org/abs/2511.01541v1</a><br>Rare and challenging driving scenarios are critical for autonomous vehicle development. Since they are difficult to encounter, simulating or generating them using generative models is a popular approach. Following previous efforts to structure driving scenario representations in a layer model, we propose a structured five-layer model to improve the evaluation and generation of rare scenarios. We use this model alongside large foundational models to generate new driving scenarios using a data augmentation strategy. Unlike previous representations, our structure introduces subclasses and characteristics for every agent of the scenario, allowing us to compare them using an embedding specific to our layer-model. We study and adapt two metrics to evaluate the relevance of a synthetic dataset in the context of a structured representation: the diversity score estimates how different the scenarios of a dataset are from one another, while the originality score calculates how similar a synthetic dataset is from a real reference set. This paper showcases both metrics in different generation setup, as well as a qualitative evaluation of synthetic videos generated from structured scenario descriptions. The code and extended results can be found at <a href="https://github.com/Valgiz/5LMSG">https://github.com/Valgiz/5LMSG</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PCD-ReID: Occluded Person Re-Identification for Base Station Inspection</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01546v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01546v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01546v1">http://arxiv.org/abs/2511.01546v1</a><br>Occluded pedestrian re-identification (ReID) in base station environments is a critical task in computer vision, particularly for surveillance and security applications. This task faces numerous challenges, as occlusions often obscure key body features, increasing the complexity of identification. Traditional ResNet-based ReID algorithms often fail to address occlusions effectively, necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component Discrepancy) algorithm to address these issues. The contributions of this work are as follows: To tackle the occlusion problem, we design a Transformer-based PCD network capable of extracting shared component features, such as helmets and uniforms. To mitigate overfitting on public datasets, we collected new real-world patrol surveillance images for model training, covering six months, 10,000 individuals, and over 50,000 images. Comparative experiments with existing ReID algorithms demonstrate that our model achieves a mean Average Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1 improvement over ResNet50-based methods. Experimental evaluations indicate that PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in tower inspection scenarios, highlighting its potential for practical deployment in surveillance and security applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NOA: a versatile, extensible tool for AI-based organoid analysis</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01549v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01549v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01549v1">http://arxiv.org/abs/2511.01549v1</a><br>AI tools can greatly enhance the analysis of organoid microscopy images, from detection and segmentation to feature extraction and classification. However, their limited accessibility to biologists without programming experience remains a major barrier, resulting in labor-intensive and largely manual workflows. Although a few AI models for organoid analysis have been developed, most existing tools remain narrowly focused on specific tasks. In this work, we introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user interface to simplify AI-based organoid analysis. NOA integrates modules for detection, segmentation, tracking, feature extraction, custom feature annotation and ML-based feature prediction. It interfaces multiple state-of-the-art algorithms and is implemented as an open-source napari plugin for maximal flexibility and extensibility. We demonstrate the versatility of NOA through three case studies, involving the quantification of morphological changes during organoid differentiation, assessment of phototoxicity effects, and prediction of organoid viability and differentiation state. Together, these examples illustrate how NOA enables comprehensive, AI-driven organoid image analysis within an accessible and extensible framework.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01571v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01571v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01571v1">http://arxiv.org/abs/2511.01571v1</a><br>Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01574v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01574v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01574v1">http://arxiv.org/abs/2511.01574v1</a><br>Compared to traditional methods, Deep Learning (DL) becomes a key technology for computer vision tasks. Synthetic data generation is an interesting use case for DL, especially in the field of medical imaging such as Magnetic Resonance Imaging (MRI). The need for this task since the original MRI data is limited. The generation of realistic medical images is completely difficult and challenging. Generative Adversarial Networks (GANs) are useful for creating synthetic medical images. In this paper, we propose a DL based methodology for creating synthetic MRI data using the Deep Convolutional Generative Adversarial Network (DC-GAN) to address the problem of limited data. We also employ a Convolutional Neural Network (CNN) classifier to classify the brain tumor using synthetic data and real MRI data. CNN is used to evaluate the quality and utility of the synthetic images. The classification result demonstrates comparable performance on real and synthetic images, which validates the effectiveness of GAN-generated images for downstream tasks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01588v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01588v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01588v1">http://arxiv.org/abs/2511.01588v1</a><br>Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01593v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01593v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01593v1">http://arxiv.org/abs/2511.01593v1</a><br>The unification of understanding and generation within a single multi-modal large model (MLLM) remains one significant challenge, largely due to the dichotomy between continuous and discrete visual tokenizations. Continuous tokenizer (CT) achieves strong performance by bridging multiple independently-trained understanding modules and generation modules, but suffers from complex multi-stage pipelines and substantial engineering overhead. Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by quantizing each image into a primitive, but inevitably leading to information loss and performance degradation. To resolve this tension, we question the binary choice between CT and DT, inspired by the wave-particle duality of light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT). We treat visual data as a flexible composition of image primitives derived from quantized codebooks, with the crucial insight that the primitive number assigned to each visual sample is adaptively determined according to its complexity: simple instances use a few primitives, emulating discrete tokenization, while complex instances use many, approximating continuous tokenization. Two core components are designed: Diverse Quantitative Primitives, which encourage primitives orthogonality to better populate information space, and Dynamic Primitive Allocator, which assesses sample complexity to determine the optimal set of primitives. Extensive experiments on reconstruction, retrieval and classification show that CDD-VT achieves superior performance over to specialized CT and DT, effectively getting strong result within a concise and scalable MLLM.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01594v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01594v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01594v1">http://arxiv.org/abs/2511.01594v1</a><br>Multimodal large language models (MLLMs) have shown remarkable capabilities in cross-modal understanding and reasoning, offering new opportunities for intelligent assistive systems, yet existing systems still struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic System powered by MLLMs for assistive intelligence and designed for smart home robots supporting people with disabilities. The system integrates four agents: a visual perception agent for extracting semantic and spatial features from environment images, a risk assessment agent for identifying and prioritizing hazards, a planning agent for generating executable action sequences, and an evaluation agent for iterative optimization. By combining multimodal perception with hierarchical multi-agent decision-making, the framework enables adaptive, risk-aware, and personalized assistance in dynamic indoor environments. Experiments on multiple datasets demonstrate the superior overall performance of the proposed system in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models. The proposed approach also highlights the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>I.2.9; I.2.11; I.2.6; I.4.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01600v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01600v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01600v1">http://arxiv.org/abs/2511.01600v1</a><br>Accurate tumor size measurement is a cornerstone of evaluating cancer treatment response. The most widely adopted standard for this purpose is the Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on measuring the longest tumor diameter in a single plane. However, volumetric measurements have been shown to provide a more reliable assessment of treatment effect. Their clinical adoption has been limited, though, due to the labor-intensive nature of manual volumetric annotation. In this paper, we present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed for efficient volumetric tumor segmentation from CT scans annotated with RECIST annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1: Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of 63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an average inference time of 14.4 s on CPU on the public validation dataset.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmark-Ready 3D Anatomical Shape Classification</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01613v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01613v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01613v1">http://arxiv.org/abs/2511.01613v1</a><br>Progress in anatomical 3D shape classification is limited by the complexity of mesh data and the lack of standardized benchmarks, highlighting the need for robust learning methods and reproducible evaluation. We introduce two key steps toward clinically and benchmark-ready anatomical shape classification via self-supervised graph autoencoding. We propose Precomputed Structural Pooling (PSPooling), a non-learnable mesh pooling operator designed for efficient and structure-preserving graph coarsening in 3D anatomical shape analysis. PSPooling precomputes node correspondence sets based on geometric proximity, enabling parallelizable and reversible pooling and unpooling operations with guaranteed support structure. This design avoids the sparsity and reconstruction issues of selection-based methods and the sequential overhead of edge contraction approaches, making it particularly suitable for high-resolution medical meshes. To demonstrate its effectiveness, we integrate PSPooling into a self-supervised graph autoencoder that learns anatomy-aware representations from unlabeled surface meshes. We evaluate the downstream benefits on MedShapeNet19, a new curated benchmark dataset we derive from MedShapeNet, consisting of 19 anatomical classes with standardized training, validation, and test splits. Experiments show that PSPooling significantly improves reconstruction fidelity and classification accuracy in low-label regimes, establishing a strong baseline for medical 3D shape learning. We hope that MedShapeNet19 will serve as a widely adopted benchmark for anatomical shape classification and further research in medical 3D shape analysis. Access the complete codebase, model weights, and dataset information here: <a href="https://github.com/TomasKrsicka/MedShapeNet19-PSPooling">https://github.com/TomasKrsicka/MedShapeNet19-PSPooling</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01610v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01610v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01610v1">http://arxiv.org/abs/2511.01610v1</a><br>Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01617v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01617v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01617v1">http://arxiv.org/abs/2511.01617v1</a><br>In the retrieval domain, candidates’ fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates’ representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM’s prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) &#x2F; 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: <a href="https://github.com/mohammad2012191/ViC">https://github.com/mohammad2012191/ViC</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01618v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01618v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01618v1">http://arxiv.org/abs/2511.01618v1</a><br>Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01645v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01645v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01645v1">http://arxiv.org/abs/2511.01645v1</a><br>Reinforcement Learning (RL) has recently been incorporated into diffusion models, e.g., tasks such as text-to-image. However, directly applying existing RL methods to diffusion-based image restoration models is suboptimal, as the objective of restoration fundamentally differs from that of pure generation: it places greater emphasis on fidelity. In this paper, we investigate how to effectively integrate RL into diffusion-based restoration models. First, through extensive experiments with various reward functions, we find that an effective reward can be derived from an Image Quality Assessment (IQA) model, instead of intuitive ground-truth-based supervision, which has already been optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover, our strategy focuses on using RL for challenging samples that are significantly distant from the ground truth, and our RL approach is innovatively implemented using MLLM-based IQA models to align distributions with high-quality images initially. As the samples approach the ground truth’s distribution, RL is adaptively combined with SFT for more fine-grained alignment. This dynamic process is facilitated through an automatic weighting strategy that adjusts based on the relative difficulty of the training samples. Our strategy is plug-and-play that can be seamlessly applied to diffusion-based restoration models, boosting its performance across various restoration tasks. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed RL framework.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01678v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01678v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01678v1">http://arxiv.org/abs/2511.01678v1</a><br>Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at <a href="https://github.com/alibaba-damo-academy/Lumos-Custom">https://github.com/alibaba-damo-academy/Lumos-Custom</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01501v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01501v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01501v1">http://arxiv.org/abs/2511.01501v1</a><br>Object pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Progressive Translation of H&amp;E to IHC with Enhanced Structural Fidelity</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01698v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01698v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01698v1">http://arxiv.org/abs/2511.01698v1</a><br>Compared to hematoxylin-eosin (H&amp;E) staining, immunohistochemistry (IHC) not only maintains the structural features of tissue samples, but also provides high-resolution protein localization, which is essential for aiding in pathology diagnosis. Despite its diagnostic value, IHC remains a costly and labor-intensive technique. Its limited scalability and constraints in multiplexing further hinder widespread adoption, especially in resource-limited settings. Consequently, researchers are increasingly exploring computational stain translation techniques to synthesize IHC-equivalent images from H&amp;E-stained slides, aiming to extract protein-level information more efficiently and cost-effectively. However, most existing stain translation techniques rely on a linearly weighted summation of multiple loss terms within a single objective function, strategy that often overlooks the interdepedence among these components-resulting in suboptimal image quality and an inability to simultaneously preserve structural authenticity and color fidelity. To address this limitation, we propose a novel network architecture that follows a progressive structure, incorporating color and cell border generation logic, which enables each visual aspect to be optimized in a stage-wise and decoupled manner. To validate the effectiveness of our proposed network architecture, we build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We introduce additional loss functions based on 3,3’-diaminobenzidine (DAB) chromogen concentration and image gradient, enhancing color fidelity and cell boundary clarity in the generated IHC images. By reconstructing the generation pipeline using our structure-color-cell boundary progressive mechanism, experiments on HER2 and ER datasets demonstrated that the model significantly improved visual quality and achieved finer structural details.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01704v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01704v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01704v1">http://arxiv.org/abs/2511.01704v1</a><br>Under-display ToF imaging aims to achieve accurate depth sensing through a ToF camera placed beneath a screen panel. However, transparent OLED (TOLED) layers introduce severe degradations-such as signal attenuation, multi-path interference (MPI), and temporal noise-that significantly compromise depth quality. To alleviate this drawback, we propose Learnable Fractional Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the expressive power of neural networks with the interpretability of physical modeling. Specifically, we implement a time-fractional reaction-diffusion module that enables iterative depth refinement with dynamically generated differential orders, capturing long-term dependencies. In addition, we introduce an efficient continuous convolution operator via coefficient prediction and repeated differentiation to further improve restoration quality. Experiments on four benchmark datasets demonstrate the effectiveness of our approach. The code is publicly available at <a href="https://github.com/wudiqx106/LFRD2">https://github.com/wudiqx106/LFRD2</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01718v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01718v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01718v1">http://arxiv.org/abs/2511.01718v1</a><br>Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act – reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at <a href="https://irpn-eai.github.io/UD-VLA.github.io/">https://irpn-eai.github.io/UD-VLA.github.io/</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Probabilistic Robustness for Free? Revisiting Training via a Benchmark</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01724v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01724v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01724v1">http://arxiv.org/abs/2511.01724v1</a><br>Deep learning models are notoriously vulnerable to imperceptible perturbations. Most existing research centers on adversarial robustness (AR), which evaluates models under worst-case scenarios by examining the existence of deterministic adversarial examples (AEs). In contrast, probabilistic robustness (PR) adopts a statistical perspective, measuring the probability that predictions remain correct under stochastic perturbations. While PR is widely regarded as a practical complement to AR, dedicated training methods for improving PR are still relatively underexplored, albeit with emerging progress. Among the few PR-targeted training methods, we identify three limitations: i non-comparable evaluation protocols; ii limited comparisons to strong AT baselines despite anecdotal PR gains from AT; and iii no unified framework to compare the generalization of these methods. Thus, we introduce PRBench, the first benchmark dedicated to evaluating improvements in PR achieved by different robustness training methods. PRBench empirically compares most common AT and PR-targeted training methods using a comprehensive set of metrics, including clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). We also provide theoretical analysis on the GE of PR performance across different training methods. Main findings revealed by PRBench include: AT methods are more versatile than PR-targeted training methods in terms of improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower GE and higher clean accuracy. A leaderboard comprising 222 trained models across 7 datasets and 10 model architectures is publicly available at <a href="https://tmpspace.github.io/PRBenchLeaderboard/">https://tmpspace.github.io/PRBenchLeaderboard/</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toward Strategy Identification and Subtask Decomposition In Task Exploration</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01728v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01728v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01728v1">http://arxiv.org/abs/2511.01728v1</a><br>This research builds on work in anticipatory human-machine interaction, a subfield of human-machine interaction where machines can facilitate advantageous interactions by anticipating a user’s future state. The aim of this research is to further a machine’s understanding of user knowledge, skill, and behavior in pursuit of implicit coordination. A task explorer pipeline was developed that uses clustering techniques, paired with factor analysis and string edit distance, to automatically identify key global and local strategies that are used to complete tasks. Global strategies identify generalized sets of actions used to complete tasks, while local strategies identify sequences that used those sets of actions in a similar composition. Additionally, meaningful subtasks of various lengths are identified within the tasks. The task explorer pipeline was able to automatically identify key strategies used to complete tasks and encode user runs with hierarchical subtask structures. In addition, a Task Explorer application was developed to easily review pipeline results. The task explorer pipeline can be easily modified to any action-based time-series data and the identified strategies and subtasks help to inform humans and machines on user knowledge, skill, and behavior.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01730v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01730v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01730v2">http://arxiv.org/abs/2511.01730v2</a><br>Pneumonia remains a leading cause of morbidity and mortality worldwide, necessitating accurate and efficient automated detection systems. While recent transformer-based detectors like RT-DETR have shown promise in object detection tasks, their application to medical imaging, particularly pneumonia detection in chest X-rays, remains underexplored. This paper presents CGF-DETR, an enhanced real-time detection transformer specifically designed for pneumonia detection. We introduce XFABlock in the backbone to improve multi-scale feature extraction through convolutional attention mechanisms integrated with CSP architecture. To achieve efficient feature aggregation, we propose SPGA module that replaces standard multi-head attention with dynamic gating mechanisms and single-head self-attention. Additionally, GCFC3 is designed for the neck to enhance feature representation through multi-path convolution fusion while maintaining real-time performance via structural re-parameterization. Extensive experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR achieves 82.2% <a href="mailto:&#x6d;&#x41;&#80;&#x40;&#48;&#x2e;&#x35;">mAP@0.5</a>, outperforming the baseline RT-DETR-l by 3.7% while maintaining comparable inference speed at 48.1 FPS. Our ablation studies confirm that each proposed module contributes meaningfully to the overall performance improvement, with the complete model achieving 50.4% mAP@[0.5:0.95]</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3EED: Ground Everything Everywhere in 3D</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01755v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01755v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01755v1">http://arxiv.org/abs/2511.01755v1</a><br>Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes – 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01756v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01756v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01756v1">http://arxiv.org/abs/2511.01756v1</a><br>2D-to-3D human pose lifting is a fundamental challenge for 3D human pose estimation in monocular video, where graph convolutional networks (GCNs) and attention mechanisms have proven to be inherently suitable for encoding the spatial-temporal correlations of skeletal joints. However, depth ambiguity and errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous studies have attempted to restrict jitters in the time domain, for instance, by constraining the differences between adjacent frames while neglecting the global spatial-temporal correlations of skeletal joint motion. To tackle this problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid feature aggregation and 3D trajectory consistency in the frequency domain. Specifically, we propose a hop-hybrid graph attention (HGA) module and a Transformer encoder to model global joint spatial-temporal correlations. The HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group to enlarge the receptive field and applies the attention mechanism to discover the latent correlations of these groups globally. We then exploit global temporal correlations by constraining trajectory consistency in the frequency domain. To provide 3D information for depth inference across frames and maintain coherence over time, a preliminary network is applied to estimate the 3D pose. Extensive experiments were conducted on two standard benchmark datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional accuracy and temporal consistency.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01767v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01767v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01767v1">http://arxiv.org/abs/2511.01767v1</a><br>In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at <a href="https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus">https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01768v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01768v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01768v1">http://arxiv.org/abs/2511.01768v1</a><br>Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at <a href="https://github.com/happinesslz/UniLION">https://github.com/happinesslz/UniLION</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01775v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01775v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01775v1">http://arxiv.org/abs/2511.01775v1</a><br>Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct “plausibility gap”: while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fractional Diffusion Bridge Models</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01795v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01795v1">http://arxiv.org/abs/2511.01795v1</a><br>We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr&quot;{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$ atomic positions in protein structure prediction and lower Fr&#39;echet Inception Distance (FID) in unpaired image translation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01802v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01802v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01802v1">http://arxiv.org/abs/2511.01802v1</a><br>Retrieval-Augmented Generation (RAG) has become a robust framework for enhancing Large Language Models (LLMs) with external knowledge. Recent advances in RAG have investigated graph based retrieval for intricate reasoning; however, the influence of prompt design on enhancing the retrieval and reasoning process is still considerably under-examined. In this paper, we present a prompt-driven GraphRAG framework that underscores the significance of prompt formulation in facilitating entity extraction, fact selection, and passage reranking for multi-hop question answering. Our approach creates a symbolic knowledge graph from text data by encoding entities and factual relationships as structured facts triples. We use LLMs selectively during online retrieval to perform semantic filtering and answer generation. We also use entity-guided graph traversal through Personalized PageRank (PPR) to support efficient, scalable retrieval based on the knowledge graph we built. Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA, with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%, respectively. These results show that prompt design is an important part of improving retrieval accuracy and response quality. This research lays the groundwork for more efficient and comprehensible multi-hop question-answering systems, highlighting the importance of prompt-aware graph reasoning.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01817v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01817v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01817v1">http://arxiv.org/abs/2511.01817v1</a><br>The ability to connect visual patterns with the processes that form them represents one of the deepest forms of visual understanding. Textures of clouds and waves, the growth of cities and forests, or the formation of materials and landscapes are all examples of patterns emerging from underlying mechanisms. We present the Scitextures dataset, a large-scale collection of textures and visual patterns from all domains of science, tech, and art, along with the models and code that generate these images. Covering over 1,200 different models and 100,000 images of patterns and textures from physics, chemistry, biology, sociology, technology, mathematics, and art, this dataset offers a way to explore the connection between the visual patterns that shape our world and the mechanisms that produce them. Created by an agentic AI pipeline that autonomously collects and implements models in standardized form, we use SciTextures to evaluate the ability of leading AI models to link visual patterns to the models and code that generate them, and to identify different patterns that emerged from the same process. We also test AIs ability to infer and recreate the mechanisms behind visual patterns by providing a natural image of a real-world pattern and asking the AI to identify, model, and code the mechanism that formed the pattern, then run this code to generate a simulated image that is compared to the real image. These benchmarks show that vision-language models (VLMs) can understand and simulate the physical system beyond a visual pattern. The dataset and code are available at: <a href="https://zenodo.org/records/17485502">https://zenodo.org/records/17485502</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01833v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01833v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01833v2">http://arxiv.org/abs/2511.01833v2</a><br>The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01990v2/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01990v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01990v2">http://arxiv.org/abs/2511.01990v2</a><br>Geo-Foundational Models (GFMs) enable fast and reliable extraction of spatiotemporal information from satellite imagery, improving flood inundation mapping by leveraging location and time embeddings. Despite their potential, it remains unclear whether GFMs outperform traditional models like U-Net. A systematic comparison across sensors and data availability scenarios is still lacking, which is an essential step to guide end-users in model selection. To address this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (a Prithvi variant), against TransNorm, U-Net, and Attention U-Net using PlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performance among all GFMs, with only 2-5% variation between the best and worst models across sensors. Clay outperforms others on PlanetScope (0.79 mIoU) and Sentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). In leave-one-region-out cross-validation across five regions, Clay shows slightly better performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07), 0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA (0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, and Sentinel-1, respectively. Across all 19 sites, leave-one-region-out cross-validation reveals a 4% improvement by Clay compared to U-Net. Visual inspection highlights Clay’s superior ability to retain fine details. Few-shot experiments show Clay achieves 0.64 mIoU on PlanetScope with just five training images, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computational time, Clay is a better choice due to its smaller model size (26M parameters), making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M). Contrary to previous findings, our results suggest GFMs offer small to moderate improvements in flood mapping accuracy at lower computational cost and labeling effort compared to traditional U-Net.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Locally-Supervised Global Image Restoration</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_01998v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_01998v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01998v1">http://arxiv.org/abs/2511.01998v1</a><br>We address the problem of image reconstruction from incomplete measurements, encompassing both upsampling and inpainting, within a learning-based framework. Conventional supervised approaches require fully sampled ground truth data, while self-supervised methods allow incomplete ground truth but typically rely on random sampling that, in expectation, covers the entire image. In contrast, we consider fixed, deterministic sampling patterns with inherently incomplete coverage, even in expectation. To overcome this limitation, we exploit multiple invariances of the underlying image distribution, which theoretically allows us to achieve the same reconstruction performance as fully supervised approaches. We validate our method on optical-resolution image upsampling in photoacoustic microscopy (PAM), demonstrating competitive or superior results while requiring substantially less ground truth data.</p>]]></content>
    
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_02014v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_02014v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02014v1">http://arxiv.org/abs/2511.02014v1</a><br>The detection of Protected Health Information (PHI) in medical imaging is critical for safeguarding patient privacy and ensuring compliance with regulatory frameworks. Traditional detection methodologies predominantly utilize Optical Character Recognition (OCR) models in conjunction with named entity recognition. However, recent advancements in Large Multimodal Model (LMM) present new opportunities for enhanced text extraction and semantic analysis. In this study, we systematically benchmark three prominent closed and open-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing two distinct pipeline configurations: one dedicated to text analysis alone and another integrating both OCR and semantic analysis. Our results indicate that LMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to conventional models like EasyOCR. However, this improvement in OCR performance does not consistently correlate with enhanced overall PHI detection accuracy. The strongest performance gains are observed on test cases with complex imprint patterns. In scenarios where text regions are well readable with sufficient contrast, and strong LMMs are employed for text analysis after OCR, different pipeline configurations yield similar results. Furthermore, we provide empirically grounded recommendations for LMM selection tailored to specific operational constraints and propose a deployment strategy that leverages scalable and modular infrastructure.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_02046v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_02046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02046v1">http://arxiv.org/abs/2511.02046v1</a><br>Creation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_02065v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_02065v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02065v1">http://arxiv.org/abs/2511.02065v1</a><br>Opto-electronic neural networks integrate optical front-ends with electronic back-ends to enable fast and energy-efficient vision. However, conventional end-to-end optimization of both the optical and electronic modules is limited by costly simulations and large parameter spaces. We introduce a two-stage strategy for designing opto-electronic convolutional neural networks (CNNs): first, train a standard electronic CNN, then realize the optical front-end implemented as a metasurface array through direct kernel optimization of its first convolutional layer. This approach reduces computational and memory demands by hundreds of times and improves training stability compared to end-to-end optimization. On monocular depth estimation, the proposed two-stage design achieves twice the accuracy of end-to-end training under the same training time and resource constraints.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study</title>
    <link href="/2025/11/03/highlights/2025-11-03-2511_02086v1/"/>
    <url>/2025/11/03/highlights/2025-11-03-2511_02086v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02086v1">http://arxiv.org/abs/2511.02086v1</a><br>Purpose: In this paper, we develop and clinically evaluate a depth-only, markerless augmented reality (AR) registration pipeline on a head-mounted display, and assess accuracy across small or low-curvature anatomies in real-life operative settings. Methods: On HoloLens 2, we align Articulated HAnd Tracking (AHAT) depth to Computed Tomography (CT)-derived skin meshes via (i) depth-bias correction, (ii) brief human-in-the-loop initialization, (iii) global and local registration. We validated the surface-tracing error metric by comparing “skin-to-bone” relative distances to CT ground truth on leg and foot models, using an AR-tracked tool. We then performed seven intraoperative target trials (feet x2, ear x3, leg x2) during the initial stage of fibula free-flap harvest and mandibular reconstruction surgery, and collected 500+ data per trial. Results: Preclinical validation showed tight agreement between AR-traced and CT distances (leg: median |Delta d| 0.78 mm, RMSE 0.97 mm; feet: 0.80 mm, 1.20 mm). Clinically, per-point error had a median of 3.9 mm. Median errors by anatomy were 3.2 mm (feet), 4.3 mm (ear), and 5.3 mm (lower leg), with 5 mm coverage 92-95%, 84-90%, and 72-86%, respectively. Feet vs. lower leg differed significantly (Delta median ~1.1 mm; p &lt; 0.001). Conclusion: A depth-only, markerless AR pipeline on HMDs achieved ~3-4 mm median error across feet, ear, and lower leg in live surgical settings without fiducials, approaching typical clinical error thresholds for moderate-risk tasks. Human-guided initialization plus global-to-local registration enabled accurate alignment on small or low-curvature targets, improving the clinical readiness of markerless AR guidance.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00785v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00785v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00785v1">http://arxiv.org/abs/2511.00785v1</a><br>3D instance segmentation is an important task for real-world applications. To avoid costly manual annotations, existing methods have explored generating pseudo labels by transferring 2D masks from foundation models to 3D. However, this approach is often suboptimal since the video frames are processed independently. This causes inconsistent segmentation granularity and conflicting 3D pseudo labels, which degrades the accuracy of final segmentation. To address this, we introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences across frames, eliminating conflicting pseudo labels. Combined with a three-stage curriculum learning framework, our approach progressively trains from fragmented single-view data to unified multi-view annotations, ultimately globally coherent full-scene supervision. This structured learning pipeline enables the model to progressively expose to pseudo-labels of increasing consistency. Thus, we can robustly distill a consistent 3D representation from initially fragmented and contradictory 2D priors. Experimental results demonstrated that our method effectively generated consistent and accurate 3D segmentations. Furthermore, the proposed method achieved state-of-the-art results on standard benchmarks and open-vocabulary ability.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00795v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00795v1">http://arxiv.org/abs/2511.00795v1</a><br>Federated Learning (FL) allows multiple institutions to cooperatively train machine learning models while retaining sensitive data at the source, which has great utility in privacy-sensitive environments. However, FL systems remain vulnerable to membership-inference attacks and data heterogeneity. This paper presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using synthetic oncologic CT scans with tumor annotations. It evaluates segmentation performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD. Results show a distinct trade-off between privacy and utility: FedAvg is high performance (Dice around 0.85) with more privacy leakage (attack AUC about 0.72), while DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx and FedBN offer balanced performance under heterogeneous data, especially with non-identical distributed client data. FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00810v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00810v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00810v1">http://arxiv.org/abs/2511.00810v1</a><br>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: <a href="https://github.com/sjz5202/GUI-AIMA">https://github.com/sjz5202/GUI-AIMA</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00831v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00831v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00831v1">http://arxiv.org/abs/2511.00831v1</a><br>Visual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00833v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00833v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00833v1">http://arxiv.org/abs/2511.00833v1</a><br>Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n &lt;&lt; N. VCA first distils each head’s dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at <a href="https://github.com/LeapLabTHU/LinearDiff">https://github.com/LeapLabTHU/LinearDiff</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Parameter Interpolation Adversarial Training for Robust Image Classification</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00836v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00836v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00836v1">http://arxiv.org/abs/2511.00836v1</a><br>Though deep neural networks exhibit superior performance on various tasks, they are still plagued by adversarial examples. Adversarial training has been demonstrated to be the most effective method to defend against adversarial attacks. However, existing adversarial training methods show that the model robustness has apparent oscillations and overfitting issues in the training process, degrading the defense efficacy. To address these issues, we propose a novel framework called Parameter Interpolation Adversarial Training (PIAT). PIAT tunes the model parameters between each epoch by interpolating the parameters of the previous and current epochs. It makes the decision boundary of model change more moderate and alleviates the overfitting issue, helping the model converge better and achieving higher model robustness. In addition, we suggest using the Normalized Mean Square Error (NMSE) to further improve the robustness by aligning the relative magnitude of logits between clean and adversarial examples rather than the absolute magnitude. Extensive experiments conducted on several benchmark datasets demonstrate that our framework could prominently improve the robustness of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00846v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00846v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00846v1">http://arxiv.org/abs/2511.00846v1</a><br>Brain imaging analysis is vital for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly assisting in that analysis. However, current brain-oriented visual question-answering (VQA) benchmarks either cover a few imaging modalities or are limited to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs throughout the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis.OmniBrainBench consists of 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluation of 24 state-of-the-art models, including open-source, medical, and proprietary MLLMs, highlights the substantial challenges posed by OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5) beat open-source and medical models but lag physicians; (2) medical MLLMs vary widely in performance; (3) open-source MLLMs trail overall but excel in specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks, revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new standard for evaluating and advancing MLLMs in brain imaging analysis, highlighting gaps compared to expert clinical reasoning. We release it at benchmark &amp; code.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00858v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00858v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00858v1">http://arxiv.org/abs/2511.00858v1</a><br>Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model’s ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Representations for Human Activity Recognition</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_00900v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_00900v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00900v1">http://arxiv.org/abs/2511.00900v1</a><br>Human activity recognition is challenging because sensor signals shift with context, motion, and environment; effective models must therefore remain stable as the world around them changes. We introduce a categorical symmetry-aware learning framework that captures how signals vary over time, scale, and sensor hierarchy. We build these factors into the structure of feature representations, yielding models that automatically preserve the relationships between sensors and remain stable under realistic distortions such as time shifts, amplitude drift, and device orientation changes. On the UCI Human Activity Recognition benchmark, this categorical symmetry-driven design improves out-of-distribution accuracy by approx. 46 percentage points (approx. 3.6x over the baseline), demonstrating that abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks via category-equivariant representation theory.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_01082v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_01082v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01082v1">http://arxiv.org/abs/2511.01082v1</a><br>Image geolocalization, the task of determining an image’s geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at <a href="https://github.com/NNargesNN/GeoToken">https://github.com/NNargesNN/GeoToken</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_01087v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_01087v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01087v1">http://arxiv.org/abs/2511.01087v1</a><br>The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</title>
    <link href="/2025/11/02/cs.AI/2025-11-02-2511_01932v1/"/>
    <url>/2025/11/02/cs.AI/2025-11-02-2511_01932v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01932v1">http://arxiv.org/abs/2511.01932v1</a><br>Image generation models are usually personalized in practical uses in order to better meet the individual users’ heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56%, when different personalization scenarios are applied to multiple types of image generation models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Erasing &#39;Ugly&#39; from the Internet: Propagation of the Beauty Myth in Text-Image Models</title>
    <link href="/2025/11/02/cs.CL/2025-11-02-2511_00749v2/"/>
    <url>/2025/11/02/cs.CL/2025-11-02-2511_00749v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00749v2">http://arxiv.org/abs/2511.00749v2</a><br>Social media has exacerbated the promotion of Western beauty norms, leading to negative self-image, particularly in women and girls, and causing harm such as body dysmorphia. Increasingly content on the internet has been artificially generated, leading to concerns that these norms are being exaggerated. The aim of this work is to study how generative AI models may encode ‘beauty’ and erase ‘ugliness’, and discuss the implications of this for society. To investigate these aims, we create two image generation pipelines: a text-to-image model and a text-to-language model-to image model. We develop a structured beauty taxonomy which we use to prompt three language models (LMs) and two text-to-image models to cumulatively generate 5984 images using our two pipelines. We then recruit women and non-binary social media users to evaluate 1200 of the images through a Likert-scale within-subjects study. Participants show high agreement in their ratings. Our results show that 86.5% of generated images depicted people with lighter skin tones, 22% contained explicit content despite Safe for Work (SFW) training, and 74% were rated as being in a younger age demographic. In particular, the images of non-binary individuals were rated as both younger and more hypersexualised, indicating troubling intersectional effects. Notably, prompts encoded with ‘negative’ or ‘ugly’ beauty traits (such as “a wide nose”) consistently produced higher Not SFW (NSFW) ratings regardless of gender. This work sheds light on the pervasive demographic biases related to beauty standards present in generative AI models – biases that are actively perpetuated by model developers, such as via negative prompting. We conclude by discussing the implications of this on society, which include pollution of the data streams and active erasure of features that do not fall inside the stereotype of what is considered beautiful by developers.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
    <link href="/2025/11/02/cs.CL/2025-11-02-2511_00810v1/"/>
    <url>/2025/11/02/cs.CL/2025-11-02-2511_00810v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00810v1">http://arxiv.org/abs/2511.00810v1</a><br>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: <a href="https://github.com/sjz5202/GUI-AIMA">https://github.com/sjz5202/GUI-AIMA</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</title>
    <link href="/2025/11/02/cs.GR/2025-11-02-2511_00908v1/"/>
    <url>/2025/11/02/cs.GR/2025-11-02-2511_00908v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00908v1">http://arxiv.org/abs/2511.00908v1</a><br>Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
    <link href="/2025/11/02/cs.HC/2025-11-02-2511_00810v1/"/>
    <url>/2025/11/02/cs.HC/2025-11-02-2511_00810v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00810v1">http://arxiv.org/abs/2511.00810v1</a><br>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: <a href="https://github.com/sjz5202/GUI-AIMA">https://github.com/sjz5202/GUI-AIMA</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Representations for Human Activity Recognition</title>
    <link href="/2025/11/02/cs.HC/2025-11-02-2511_00900v1/"/>
    <url>/2025/11/02/cs.HC/2025-11-02-2511_00900v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00900v1">http://arxiv.org/abs/2511.00900v1</a><br>Human activity recognition is challenging because sensor signals shift with context, motion, and environment; effective models must therefore remain stable as the world around them changes. We introduce a categorical symmetry-aware learning framework that captures how signals vary over time, scale, and sensor hierarchy. We build these factors into the structure of feature representations, yielding models that automatically preserve the relationships between sensors and remain stable under realistic distortions such as time shifts, amplitude drift, and device orientation changes. On the UCI Human Activity Recognition benchmark, this categorical symmetry-driven design improves out-of-distribution accuracy by approx. 46 percentage points (approx. 3.6x over the baseline), demonstrating that abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks via category-equivariant representation theory.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_00804v2/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_00804v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00804v2">http://arxiv.org/abs/2511.00804v2</a><br>Erasing harmful or proprietary concepts from powerful text to image generators is an emerging safety requirement, yet current “concept erasure” techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with GFlowNets equipped with the trajectory balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model’s prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade off between performance and prior preservation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_00812v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_00812v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00812v1">http://arxiv.org/abs/2511.00812v1</a><br>Vision Transformers have been tremendously successful in computer vision tasks. However, their large computational, memory, and energy demands are a challenge for edge inference on FPGAs – a field that has seen a recent surge in demand. We recognize the benefits of recent works on logic and Look Up Table (LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in offering models that simultaneously reduce both the memory and compute footprints. However, these models natively do not perform well on common vision tasks, such as CIFAR-10&#x2F;100. In this work, we propose LL-ViT, a novel edge optimized vision transformer design that integrates layers of LUT neurons within the transformer architecture. Based on our characterization that reveals that a majority of model weights and computations are from the channel mixer (MLP layer), we design an alternate LUT-based channel mixer, and simultaneously develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to replace each multiplication with a table lookup, our architecture utilizes a neural learning approach which natively learns the LUT functions. This approach allows for reduced model sizes, and a computational and energy-efficient inference solution for vision transformer models. Evaluating on edge-suitable workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and 60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT eliminates over 60% of the model weights and 50% of the multiplications in the model, and achieves 1.9x energy efficiency and 1.3x lower latency over an integer quantized ViT accelerator, while also offering superior throughput against prior works at a 10.9W power budget.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_00810v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_00810v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00810v1">http://arxiv.org/abs/2511.00810v1</a><br>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: <a href="https://github.com/sjz5202/GUI-AIMA">https://github.com/sjz5202/GUI-AIMA</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_00858v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_00858v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00858v1">http://arxiv.org/abs/2511.00858v1</a><br>Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model’s ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Representations for Human Activity Recognition</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_00900v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_00900v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00900v1">http://arxiv.org/abs/2511.00900v1</a><br>Human activity recognition is challenging because sensor signals shift with context, motion, and environment; effective models must therefore remain stable as the world around them changes. We introduce a categorical symmetry-aware learning framework that captures how signals vary over time, scale, and sensor hierarchy. We build these factors into the structure of feature representations, yielding models that automatically preserve the relationships between sensors and remain stable under realistic distortions such as time shifts, amplitude drift, and device orientation changes. On the UCI Human Activity Recognition benchmark, this categorical symmetry-driven design improves out-of-distribution accuracy by approx. 46 percentage points (approx. 3.6x over the baseline), demonstrating that abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks via category-equivariant representation theory.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_01082v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_01082v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01082v1">http://arxiv.org/abs/2511.01082v1</a><br>Image geolocalization, the task of determining an image’s geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at <a href="https://github.com/NNargesNN/GeoToken">https://github.com/NNargesNN/GeoToken</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_01087v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_01087v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01087v1">http://arxiv.org/abs/2511.01087v1</a><br>The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_01932v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_01932v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01932v1">http://arxiv.org/abs/2511.01932v1</a><br>Image generation models are usually personalized in practical uses in order to better meet the individual users’ heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56%, when different personalization scenarios are applied to multiple types of image generation models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya</title>
    <link href="/2025/11/02/cs.LG/2025-11-02-2511_01000v1/"/>
    <url>/2025/11/02/cs.LG/2025-11-02-2511_01000v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01000v1">http://arxiv.org/abs/2511.01000v1</a><br>Art authentication of Francisco Goya’s works presents complex computational challenges due to his heterogeneous stylistic evolution and extensive historical patterns of forgery. We introduce a novel multimodal machine learning framework that applies identical feature extraction techniques to both visual and X-ray radiographic images of Goya paintings. The unified feature extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors, Local Binary Patterns, entropy measures, energy calculations, and colour distribution analysis applied consistently across both imaging modalities. The extracted features from both visual and X-ray images are processed through an optimised One-Class Support Vector Machine with hyperparameter tuning. Using a dataset of 24 authenticated Goya paintings with corresponding X-ray images, split into an 80&#x2F;20 train-test configuration with 10-fold cross-validation, the framework achieves 97.8% classification accuracy with a 0.022 false positive rate. Case study analysis of &#96;&#96;Un Gigante’’ demonstrates the practical efficacy of our pipeline, achieving 92.3% authentication confidence through unified multimodal feature analysis. Our results indicate substantial performance improvement over single-modal approaches, establishing the effectiveness of applying identical computational methods to both visual and radiographic imagery in art authentication applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing</title>
    <link href="/2025/11/02/cs.MM/2025-11-02-2511_00801v3/"/>
    <url>/2025/11/02/cs.MM/2025-11-02-2511_00801v3/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00801v3">http://arxiv.org/abs/2511.00801v3</a><br>Medical image editing has emerged as a pivotal technology with broad applications in data augmentation, model interpretability, medical education, and treatment simulation. However, the lack of large-scale, high-quality, and openly accessible datasets tailored for medical contexts with strict anatomical and clinical constraints has significantly hindered progress in this domain. To bridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over 50k medically curated image edits spanning chest X-ray, brain MRI, and fundus photography across 23 diseases. Each sample supports bidirectional lesion editing (addition and removal) and is constructed using Gemini-2.5-Flash-Image based on real clinical images. A key differentiator of our dataset is the medically grounded quality control protocol: we employ an LLM-as-Judge evaluation framework with criteria such as instruction compliance, structural plausibility, image realism, and fidelity preservation, alongside iterative refinement over up to five rounds. Additionally, Med-Banana-50K includes around 37,000 failed editing attempts with full evaluation logs to support preference learning and alignment research. By offering a large-scale, medically rigorous, and fully documented resource, Med-Banana-50K establishes a critical foundation for developing and evaluating reliable medical image editing systems. Our dataset and code are publicly available. [<a href="https://github.com/richardChenzhihui/med-banana-50k]">https://github.com/richardChenzhihui/med-banana-50k]</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</title>
    <link href="/2025/11/02/cs.MM/2025-11-02-2511_01932v1/"/>
    <url>/2025/11/02/cs.MM/2025-11-02-2511_01932v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01932v1">http://arxiv.org/abs/2511.01932v1</a><br>Image generation models are usually personalized in practical uses in order to better meet the individual users’ heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56%, when different personalization scenarios are applied to multiple types of image generation models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.MM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression</title>
    <link href="/2025/11/02/cs.NA/2025-11-02-2511_01079v1/"/>
    <url>/2025/11/02/cs.NA/2025-11-02-2511_01079v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01079v1">http://arxiv.org/abs/2511.01079v1</a><br>Neural image compression (NIC) has become the state-of-the-art for rate-distortion performance, yet its security vulnerabilities remain significantly less understood than those of classifiers. Existing adversarial attacks on NICs are often naive adaptations of pixel-space methods, overlooking the unique, structured nature of the compression pipeline. In this work, we propose a more advanced class of vulnerabilities by introducing T-MLA, the first targeted multiscale log–exponential attack framework. Our approach crafts adversarial perturbations in the wavelet domain by directly targeting the quality of the attacked and reconstructed images. This allows for a principled, offline attack where perturbations are strategically confined to specific wavelet subbands, maximizing distortion while ensuring perceptual stealth. Extensive evaluation across multiple state-of-the-art NIC architectures on standard image compression benchmarks reveals a large drop in reconstruction quality while the perturbations remain visually imperceptible. Our findings reveal a critical security flaw at the core of generative and content delivery pipelines.</p>]]></content>
    
    
    <categories>
      
      <category>cs.NA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</title>
    <link href="/2025/11/02/cs.RO/2025-11-02-2511_00933v1/"/>
    <url>/2025/11/02/cs.RO/2025-11-02-2511_00933v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00933v1">http://arxiv.org/abs/2511.00933v1</a><br>Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Erasing &#39;Ugly&#39; from the Internet: Propagation of the Beauty Myth in Text-Image Models</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00749v2/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00749v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00749v2">http://arxiv.org/abs/2511.00749v2</a><br>Social media has exacerbated the promotion of Western beauty norms, leading to negative self-image, particularly in women and girls, and causing harm such as body dysmorphia. Increasingly content on the internet has been artificially generated, leading to concerns that these norms are being exaggerated. The aim of this work is to study how generative AI models may encode ‘beauty’ and erase ‘ugliness’, and discuss the implications of this for society. To investigate these aims, we create two image generation pipelines: a text-to-image model and a text-to-language model-to image model. We develop a structured beauty taxonomy which we use to prompt three language models (LMs) and two text-to-image models to cumulatively generate 5984 images using our two pipelines. We then recruit women and non-binary social media users to evaluate 1200 of the images through a Likert-scale within-subjects study. Participants show high agreement in their ratings. Our results show that 86.5% of generated images depicted people with lighter skin tones, 22% contained explicit content despite Safe for Work (SFW) training, and 74% were rated as being in a younger age demographic. In particular, the images of non-binary individuals were rated as both younger and more hypersexualised, indicating troubling intersectional effects. Notably, prompts encoded with ‘negative’ or ‘ugly’ beauty traits (such as “a wide nose”) consistently produced higher Not SFW (NSFW) ratings regardless of gender. This work sheds light on the pervasive demographic biases related to beauty standards present in generative AI models – biases that are actively perpetuated by model developers, such as via negative prompting. We conclude by discussing the implications of this on society, which include pollution of the data streams and active erasure of features that do not fall inside the stereotype of what is considered beautiful by developers.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00777v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00777v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00777v1">http://arxiv.org/abs/2511.00777v1</a><br>Durian plantation suffers from animal intrusions that cause crop damage and financial loss. The traditional farming practices prove ineffective due to the unavailability of monitoring without human intervention. The fast growth of machine learning and Internet of Things (IoT) technology has led to new ways to detect animals. However, current systems are limited by dependence on single object detection algorithms, less accessible notification platforms, and limited deterrent mechanisms. This research suggests an IoT-enabled animal detection system for durian crops. The system integrates YOLOv5 and SSD object detection algorithms to improve detection accuracy. The system provides real-time monitoring, with detected intrusions automatically reported to farmers via Telegram notifications for rapid response. An automated sound mechanism (e.g., tiger roar) is triggered once the animal is detected. The YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%, 85% and 70%, respectively. The system shows the highest accuracy in daytime and decreases at night, regardless of whether the image is still or a video. Overall, this study contributes a comprehensive and practical framework that combines detection, notification, and deterrence, paving the way for future innovations in automated farming solutions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00785v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00785v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00785v1">http://arxiv.org/abs/2511.00785v1</a><br>3D instance segmentation is an important task for real-world applications. To avoid costly manual annotations, existing methods have explored generating pseudo labels by transferring 2D masks from foundation models to 3D. However, this approach is often suboptimal since the video frames are processed independently. This causes inconsistent segmentation granularity and conflicting 3D pseudo labels, which degrades the accuracy of final segmentation. To address this, we introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences across frames, eliminating conflicting pseudo labels. Combined with a three-stage curriculum learning framework, our approach progressively trains from fragmented single-view data to unified multi-view annotations, ultimately globally coherent full-scene supervision. This structured learning pipeline enables the model to progressively expose to pseudo-labels of increasing consistency. Thus, we can robustly distill a consistent 3D representation from initially fragmented and contradictory 2D priors. Experimental results demonstrated that our method effectively generated consistent and accurate 3D segmentations. Furthermore, the proposed method achieved state-of-the-art results on standard benchmarks and open-vocabulary ability.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00795v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00795v1">http://arxiv.org/abs/2511.00795v1</a><br>Federated Learning (FL) allows multiple institutions to cooperatively train machine learning models while retaining sensitive data at the source, which has great utility in privacy-sensitive environments. However, FL systems remain vulnerable to membership-inference attacks and data heterogeneity. This paper presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using synthetic oncologic CT scans with tumor annotations. It evaluates segmentation performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD. Results show a distinct trade-off between privacy and utility: FedAvg is high performance (Dice around 0.85) with more privacy leakage (attack AUC about 0.72), while DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx and FedBN offer balanced performance under heterogeneous data, especially with non-identical distributed client data. FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00801v3/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00801v3/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00801v3">http://arxiv.org/abs/2511.00801v3</a><br>Medical image editing has emerged as a pivotal technology with broad applications in data augmentation, model interpretability, medical education, and treatment simulation. However, the lack of large-scale, high-quality, and openly accessible datasets tailored for medical contexts with strict anatomical and clinical constraints has significantly hindered progress in this domain. To bridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over 50k medically curated image edits spanning chest X-ray, brain MRI, and fundus photography across 23 diseases. Each sample supports bidirectional lesion editing (addition and removal) and is constructed using Gemini-2.5-Flash-Image based on real clinical images. A key differentiator of our dataset is the medically grounded quality control protocol: we employ an LLM-as-Judge evaluation framework with criteria such as instruction compliance, structural plausibility, image realism, and fidelity preservation, alongside iterative refinement over up to five rounds. Additionally, Med-Banana-50K includes around 37,000 failed editing attempts with full evaluation logs to support preference learning and alignment research. By offering a large-scale, medically rigorous, and fully documented resource, Med-Banana-50K establishes a critical foundation for developing and evaluating reliable medical image editing systems. Our dataset and code are publicly available. [<a href="https://github.com/richardChenzhihui/med-banana-50k]">https://github.com/richardChenzhihui/med-banana-50k]</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00812v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00812v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00812v1">http://arxiv.org/abs/2511.00812v1</a><br>Vision Transformers have been tremendously successful in computer vision tasks. However, their large computational, memory, and energy demands are a challenge for edge inference on FPGAs – a field that has seen a recent surge in demand. We recognize the benefits of recent works on logic and Look Up Table (LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in offering models that simultaneously reduce both the memory and compute footprints. However, these models natively do not perform well on common vision tasks, such as CIFAR-10&#x2F;100. In this work, we propose LL-ViT, a novel edge optimized vision transformer design that integrates layers of LUT neurons within the transformer architecture. Based on our characterization that reveals that a majority of model weights and computations are from the channel mixer (MLP layer), we design an alternate LUT-based channel mixer, and simultaneously develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to replace each multiplication with a table lookup, our architecture utilizes a neural learning approach which natively learns the LUT functions. This approach allows for reduced model sizes, and a computational and energy-efficient inference solution for vision transformer models. Evaluating on edge-suitable workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and 60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT eliminates over 60% of the model weights and 50% of the multiplications in the model, and achieves 1.9x energy efficiency and 1.3x lower latency over an integer quantized ViT accelerator, while also offering superior throughput against prior works at a 10.9W power budget.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00804v2/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00804v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00804v2">http://arxiv.org/abs/2511.00804v2</a><br>Erasing harmful or proprietary concepts from powerful text to image generators is an emerging safety requirement, yet current “concept erasure” techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with GFlowNets equipped with the trajectory balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model’s prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade off between performance and prior preservation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00810v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00810v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00810v1">http://arxiv.org/abs/2511.00810v1</a><br>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: <a href="https://github.com/sjz5202/GUI-AIMA">https://github.com/sjz5202/GUI-AIMA</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00821v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00821v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00821v1">http://arxiv.org/abs/2511.00821v1</a><br>Vision-Language Models (VLMs) have demonstrated strong performance across various multimodal tasks, where position encoding plays a vital role in modeling both the sequential structure of textual information and the spatial structure of visual information. However, current VLMs commonly adopt modality-unified 1D or 2D positional indexing strategies, which treat textual and visual tokens uniformly without accounting for their distinct structural properties and sequential continuity for text and spatial coherence for vision. To address this limitation, we propose OMEGA, a novel position encoding framework that employs Modality-Specific Position Encoding (MSPE) to assign positional indices while preserving the inherent structures of each modality across separate coordinate dimensions. Additionally, to align the information density of multimodal data in the positional index space, OMEGA introduces Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the position encoding step size of visual tokens based on the embedding entropy of both modalities. Experimental results demonstrate that OMEGA consistently enhances VLM performance across diverse architectures and VQA benchmarks. On visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00831v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00831v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00831v1">http://arxiv.org/abs/2511.00831v1</a><br>Visual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00833v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00833v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00833v1">http://arxiv.org/abs/2511.00833v1</a><br>Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n &lt;&lt; N. VCA first distils each head’s dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at <a href="https://github.com/LeapLabTHU/LinearDiff">https://github.com/LeapLabTHU/LinearDiff</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Parameter Interpolation Adversarial Training for Robust Image Classification</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00836v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00836v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00836v1">http://arxiv.org/abs/2511.00836v1</a><br>Though deep neural networks exhibit superior performance on various tasks, they are still plagued by adversarial examples. Adversarial training has been demonstrated to be the most effective method to defend against adversarial attacks. However, existing adversarial training methods show that the model robustness has apparent oscillations and overfitting issues in the training process, degrading the defense efficacy. To address these issues, we propose a novel framework called Parameter Interpolation Adversarial Training (PIAT). PIAT tunes the model parameters between each epoch by interpolating the parameters of the previous and current epochs. It makes the decision boundary of model change more moderate and alleviates the overfitting issue, helping the model converge better and achieving higher model robustness. In addition, we suggest using the Normalized Mean Square Error (NMSE) to further improve the robustness by aligning the relative magnitude of logits between clean and adversarial examples rather than the absolute magnitude. Extensive experiments conducted on several benchmark datasets demonstrate that our framework could prominently improve the robustness of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00846v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00846v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00846v1">http://arxiv.org/abs/2511.00846v1</a><br>Brain imaging analysis is vital for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly assisting in that analysis. However, current brain-oriented visual question-answering (VQA) benchmarks either cover a few imaging modalities or are limited to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs throughout the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis.OmniBrainBench consists of 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluation of 24 state-of-the-art models, including open-source, medical, and proprietary MLLMs, highlights the substantial challenges posed by OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5) beat open-source and medical models but lag physicians; (2) medical MLLMs vary widely in performance; (3) open-source MLLMs trail overall but excel in specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks, revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new standard for evaluating and advancing MLLMs in brain imaging analysis, highlighting gaps compared to expert clinical reasoning. We release it at benchmark &amp; code.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00858v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00858v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00858v1">http://arxiv.org/abs/2511.00858v1</a><br>Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model’s ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00859v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00859v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00859v1">http://arxiv.org/abs/2511.00859v1</a><br>In autonomous driving, transparency in the decision-making of perception models is critical, as even a single misperception can be catastrophic. Yet with multi-sensor inputs, it is difficult to determine how each modality contributes to a prediction because sensor information becomes entangled within the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a post-hoc, model-agnostic interpretability method that disentangles modality-specific information across all layers of a pretrained fusion model. To our knowledge, LMD is the first approach to attribute the predictions of a perception model to individual input modalities in a sensor-fusion system for autonomous driving. We evaluate LMD on pretrained fusion models under camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous driving. Its effectiveness is validated using structured perturbation-based metrics and modality-wise visual decompositions, demonstrating practical applicability to interpreting high-capacity multimodal architectures. Code is available at <a href="https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition">https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Representations for Human Activity Recognition</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00900v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00900v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00900v1">http://arxiv.org/abs/2511.00900v1</a><br>Human activity recognition is challenging because sensor signals shift with context, motion, and environment; effective models must therefore remain stable as the world around them changes. We introduce a categorical symmetry-aware learning framework that captures how signals vary over time, scale, and sensor hierarchy. We build these factors into the structure of feature representations, yielding models that automatically preserve the relationships between sensors and remain stable under realistic distortions such as time shifts, amplitude drift, and device orientation changes. On the UCI Human Activity Recognition benchmark, this categorical symmetry-driven design improves out-of-distribution accuracy by approx. 46 percentage points (approx. 3.6x over the baseline), demonstrating that abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks via category-equivariant representation theory.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00815v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00815v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00815v1">http://arxiv.org/abs/2511.00815v1</a><br>Pancreas segmentation in medical image processing is a persistent challenge due to its small size, low contrast against adjacent tissues, and significant topological variations. Traditional level set methods drive boundary evolution using gradient flows, often ignoring pointwise topological effects. Conversely, deep learning-based segmentation networks extract rich semantic features but frequently sacrifice structural details. To bridge this gap, we propose a novel model named TA-LSDiff, which combined topology-aware diffusion probabilistic model and level set energy, achieving segmentation without explicit geometric evolution. This energy function guides implicit curve evolution by integrating the input image and deep features through four complementary terms. To further enhance boundary precision, we introduce a pixel-adaptive refinement module that locally modulates the energy function using affinity weighting from neighboring evidence. Ablation studies systematically quantify the contribution of each proposed component. Evaluations on four public pancreas datasets demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming existing methods. These results establish TA-LSDiff as a practical and accurate solution for pancreas segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00916v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00916v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00916v1">http://arxiv.org/abs/2511.00916v1</a><br>Multimodal Large Language Models (MLLMs) have demonstrated remarkable effectiveness in various general-domain scenarios, such as visual question answering and image captioning. Recently, researchers have increasingly focused on empowering MLLMs with medical conversational abilities, which hold significant promise for clinical applications. However, medical data presents unique challenges due to its heterogeneous nature – encompassing diverse modalities including 2D images, 3D volumetric scans, and temporal video sequences. The substantial domain gap and data format inconsistencies across these modalities have hindered the development of unified medical MLLMs. To address these challenges, we propose Fleming-VL, a unified end-to-end framework for comprehensive medical visual understanding across heterogeneous modalities. Fleming-VL tackles this problem from a data-centric perspective through three key strategies: (1) scaling up pretraining by integrating long-context data from both natural and medical-specific domains; (2) complementing fine-tuning with rare medical data, including holistic video analysis and underrepresented 2D modalities such as ultrasound and dermoscopy images; (3) extending existing evaluation frameworks to incorporate 3D volumetric and video understanding benchmarks. Through supervised fine-tuning (SFT) and group relative policy optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive experiments demonstrate that Fleming-VL achieves state-of-the-art performance across multiple benchmarks, including medical VQA, video QA, and 3D medical image understanding. We publicly release Fleming-VL to promote transparent, reproducible, and auditable progress in medical AI.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00925v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00925v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00925v1">http://arxiv.org/abs/2511.00925v1</a><br>The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved increasing attention due to its wide applications, e.g. e-commerce. Despite progress made in this field, previous works suffer from using imbalanced samples of modalities and inconsistent low-quality information during training, resulting in sub-optimal performance. Therefore, in this paper, we introduce an approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It consists of three components: (i) a Uni-modal Feature Extraction Module that includes a CLIP text encoder and a ViT for extracting textual and visual tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an alignment weight list by the local and global aggregation blocks to measure the aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss Module aiming to improve the balance of domains in the triplet loss. Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, show our method delivers superior performances over the state-of-the-art ZS-SBIR methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00933v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00933v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00933v1">http://arxiv.org/abs/2511.00933v1</a><br>Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00908v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00908v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00908v1">http://arxiv.org/abs/2511.00908v1</a><br>Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00956v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00956v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00956v1">http://arxiv.org/abs/2511.00956v1</a><br>We propose EVTAR, an End-to-End Virtual Try-on model with Additional Reference, that directly fits the target garment onto the person image while incorporating reference images to enhance try-on accuracy. Most existing virtual try-on approaches rely on complex inputs such as agnostic person images, human pose, densepose, or body keypoints, making them labor-intensive and impractical for real-world applications. In contrast, EVTAR adopts a two-stage training strategy, enabling simple inference with only the source image and the target garment inputs. Our model generates try-on results without masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details better. This mechanism is analogous to how humans consider reference models when choosing outfits, thereby simulating a more realistic and high-quality dressing effect. We enrich the training data with supplementary references and unpaired person images to support these capabilities. We evaluate EVTAR on two widely used benchmarks and diverse tasks, and the results consistently validate the effectiveness of our approach.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00962v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00962v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00962v1">http://arxiv.org/abs/2511.00962v1</a><br>Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner. Project Page: <a href="https://rathgrith.github.io/Unified_Frame_VAA/">https://rathgrith.github.io/Unified_Frame_VAA/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00981v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00981v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00981v1">http://arxiv.org/abs/2511.00981v1</a><br>Accurate vessel segmentation is critical for clinical applications such as disease diagnosis and surgical planning, yet remains challenging due to thin, branching structures and low texture contrast. While foundation models like the Segment Anything Model (SAM) have shown promise in generic segmentation, they perform sub-optimally on vascular structures. In this work, we present VesSAM, a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM integrates (1) a convolutional adapter to enhance local texture features, (2) a multi-prompt encoder that fuses anatomical prompts, including skeletons, bifurcation points, and segment midpoints, via hierarchical cross-attention, and (3) a lightweight mask decoder to reduce jagged artifacts. We also introduce an automated pipeline to generate structured multi-prompt annotations, and curate a diverse benchmark dataset spanning 8 datasets across 5 imaging modalities. Experimental results demonstrate that VesSAM consistently outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13% IoU, and achieves competitive performance compared to fully fine-tuned methods, with significantly fewer parameters. VesSAM also generalizes well to out-of-distribution (OoD) settings, outperforming all baselines in average OoD Dice and IoU.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MID: A Self-supervised Multimodal Iterative Denoising Framework</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_00997v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_00997v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00997v1">http://arxiv.org/abs/2511.00997v1</a><br>Data denoising is a persistent challenge across scientific and engineering domains. Real-world data is frequently corrupted by complex, non-linear noise, rendering traditional rule-based denoising methods inadequate. To overcome these obstacles, we propose a novel self-supervised multimodal iterative denoising (MID) framework. MID models the collected noisy data as a state within a continuous process of non-linear noise accumulation. By iteratively introducing further noise, MID learns two neural networks: one to estimate the current noise step and another to predict and subtract the corresponding noise increment. For complex non-linear contamination, MID employs a first-order Taylor expansion to locally linearize the noise process, enabling effective iterative removal. Crucially, MID does not require paired clean-noisy datasets, as it learns noise characteristics directly from the noisy inputs. Experiments across four classic computer vision tasks demonstrate MID’s robustness, adaptability, and consistent state-of-the-art performance. Moreover, MID exhibits strong performance and adaptability in tasks within the biomedical and bioinformatics domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01000v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01000v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01000v1">http://arxiv.org/abs/2511.01000v1</a><br>Art authentication of Francisco Goya’s works presents complex computational challenges due to his heterogeneous stylistic evolution and extensive historical patterns of forgery. We introduce a novel multimodal machine learning framework that applies identical feature extraction techniques to both visual and X-ray radiographic images of Goya paintings. The unified feature extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors, Local Binary Patterns, entropy measures, energy calculations, and colour distribution analysis applied consistently across both imaging modalities. The extracted features from both visual and X-ray images are processed through an optimised One-Class Support Vector Machine with hyperparameter tuning. Using a dataset of 24 authenticated Goya paintings with corresponding X-ray images, split into an 80&#x2F;20 train-test configuration with 10-fold cross-validation, the framework achieves 97.8% classification accuracy with a 0.022 false positive rate. Case study analysis of &#96;&#96;Un Gigante’’ demonstrates the practical efficacy of our pipeline, achieving 92.3% authentication confidence through unified multimodal feature analysis. Our results indicate substantial performance improvement over single-modal approaches, establishing the effectiveness of applying identical computational methods to both visual and radiographic imagery in art authentication applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01013v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01013v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01013v1">http://arxiv.org/abs/2511.01013v1</a><br>B-mode ultrasound for breast cancer diagnosis faces challenges: speckle, operator dependency, and indistinct boundaries. Existing deep learning suffers from single-task learning, architectural constraints (CNNs lack global context, Transformers local features), and black-box decision-making. These gaps hinder clinical adoption.   We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous segmentation and classification with intrinsic interpretability. Its dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via multi-scale hierarchical fusion blocks. An attention-gated decoder provides precision and explainability. We introduce dual-pipeline interpretability: (1) intrinsic attention validation with quantitative IoU verification (mean: 0.86), and (2) Grad-CAM for classification reasoning.   On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +&#x2F;- 0.072 and accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant Recall of 92.1 +&#x2F;- 2.2% ensures minimal false negatives. Ensemble modeling yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant Recall, eliminating false negatives. Ablation studies confirm multi-scale fusion contributes +16.8% Dice and attention gates add +5.9%.   Crucially, we conduct the first cross-dataset generalization study for hybrid CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058), confirming domain shift. However, progressive fine-tuning with only 10% target-domain data (68 images) recovers 92.5% performance. With 50% data, our model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and demonstrating true generalization.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01079v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01079v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01079v1">http://arxiv.org/abs/2511.01079v1</a><br>Neural image compression (NIC) has become the state-of-the-art for rate-distortion performance, yet its security vulnerabilities remain significantly less understood than those of classifiers. Existing adversarial attacks on NICs are often naive adaptations of pixel-space methods, overlooking the unique, structured nature of the compression pipeline. In this work, we propose a more advanced class of vulnerabilities by introducing T-MLA, the first targeted multiscale log–exponential attack framework. Our approach crafts adversarial perturbations in the wavelet domain by directly targeting the quality of the attacked and reconstructed images. This allows for a principled, offline attack where perturbations are strategically confined to specific wavelet subbands, maximizing distortion while ensuring perceptual stealth. Extensive evaluation across multiple state-of-the-art NIC architectures on standard image compression benchmarks reveals a large drop in reconstruction quality while the perturbations remain visually imperceptible. Our findings reveal a critical security flaw at the core of generative and content delivery pipelines.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01026v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01026v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01026v1">http://arxiv.org/abs/2511.01026v1</a><br>We present FastBoost, a parameter-efficient neural architecture that achieves state-of-the-art performance on CIFAR benchmarks through a novel Dynamically Scaled Progressive Attention (DSPA) mechanism. Our design establishes new efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and 93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and 74.85% (0.44M parameters) The breakthrough stems from three fundamental innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention blending with dynamic weights. (2) Phase Scaling: Training-stage-aware intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The architecture features dual attention pathways with real-time weight adjustment, cascaded refinement layers (increasing gradient flow by 12.7%), and a hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic attention and efficient convolution operations demonstrates unprecedented parameter-accuracy trade-offs, enabling deployment in resource-constrained edge devices without accuracy degradation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01082v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01082v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01082v1">http://arxiv.org/abs/2511.01082v1</a><br>Image geolocalization, the task of determining an image’s geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at <a href="https://github.com/NNargesNN/GeoToken">https://github.com/NNargesNN/GeoToken</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01087v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01087v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01087v1">http://arxiv.org/abs/2511.01087v1</a><br>The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01098v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01098v1">http://arxiv.org/abs/2511.01098v1</a><br>This study presents a novel method for diagnosing respiratory diseases using image data. It combines Epanechnikov’s non-parametric kernel density estimation (EKDE) with a bimodal logistic regression classifier in a statistical-model-based learning scheme. EKDE’s flexibility in modeling data distributions without assuming specific shapes and its adaptability to pixel intensity variations make it valuable for extracting key features from medical images. The method was tested on 13808 randomly selected chest X-rays from the COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of 59.26%, and a specificity of 74.18%, demonstrating moderate performance in detecting respiratory disease while showing room for improvement in sensitivity. While clinical expertise remains essential for further refining the model, this study highlights the potential of EKDE-based approaches to enhance diagnostic accuracy and reliability in medical imaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anatomically Constrained Transformers for Echocardiogram Analysis</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01109v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01109v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01109v1">http://arxiv.org/abs/2511.01109v1</a><br>Video transformers have recently demonstrated strong potential for echocardiogram (echo) analysis, leveraging self-supervised pre-training and flexible adaptation across diverse tasks. However, like other models operating on videos, they are prone to learning spurious correlations from non-diagnostic regions such as image backgrounds. To overcome this limitation, we propose the Video Anatomically Constrained Transformer (ViACT), a novel framework that integrates anatomical priors directly into the transformer architecture. ViACT represents a deforming anatomical structure as a point set and encodes both its spatial geometry and corresponding image patches into transformer tokens. During pre-training, ViACT follows a masked autoencoding strategy that masks and reconstructs only anatomical patches, enforcing that representation learning is focused on the anatomical region. The pre-trained model can then be fine-tuned for tasks localized to this region. In this work we focus on the myocardium, demonstrating the framework on echo analysis tasks such as left ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA) detection. The anatomical constraint focuses transformer attention within the myocardium, yielding interpretable attention maps aligned with regions of known CA pathology. Moreover, ViACT generalizes to myocardium point tracking without requiring task-specific components such as correlation volumes used in specialized tracking networks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</title>
    <link href="/2025/11/02/cs.CV/2025-11-02-2511_01932v1/"/>
    <url>/2025/11/02/cs.CV/2025-11-02-2511_01932v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01932v1">http://arxiv.org/abs/2511.01932v1</a><br>Image generation models are usually personalized in practical uses in order to better meet the individual users’ heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56%, when different personalization scenarios are applied to multiple types of image generation models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Erasing &#39;Ugly&#39; from the Internet: Propagation of the Beauty Myth in Text-Image Models</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00749v2/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00749v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00749v2">http://arxiv.org/abs/2511.00749v2</a><br>Social media has exacerbated the promotion of Western beauty norms, leading to negative self-image, particularly in women and girls, and causing harm such as body dysmorphia. Increasingly content on the internet has been artificially generated, leading to concerns that these norms are being exaggerated. The aim of this work is to study how generative AI models may encode ‘beauty’ and erase ‘ugliness’, and discuss the implications of this for society. To investigate these aims, we create two image generation pipelines: a text-to-image model and a text-to-language model-to image model. We develop a structured beauty taxonomy which we use to prompt three language models (LMs) and two text-to-image models to cumulatively generate 5984 images using our two pipelines. We then recruit women and non-binary social media users to evaluate 1200 of the images through a Likert-scale within-subjects study. Participants show high agreement in their ratings. Our results show that 86.5% of generated images depicted people with lighter skin tones, 22% contained explicit content despite Safe for Work (SFW) training, and 74% were rated as being in a younger age demographic. In particular, the images of non-binary individuals were rated as both younger and more hypersexualised, indicating troubling intersectional effects. Notably, prompts encoded with ‘negative’ or ‘ugly’ beauty traits (such as “a wide nose”) consistently produced higher Not SFW (NSFW) ratings regardless of gender. This work sheds light on the pervasive demographic biases related to beauty standards present in generative AI models – biases that are actively perpetuated by model developers, such as via negative prompting. We conclude by discussing the implications of this on society, which include pollution of the data streams and active erasure of features that do not fall inside the stereotype of what is considered beautiful by developers.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00785v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00785v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00785v1">http://arxiv.org/abs/2511.00785v1</a><br>3D instance segmentation is an important task for real-world applications. To avoid costly manual annotations, existing methods have explored generating pseudo labels by transferring 2D masks from foundation models to 3D. However, this approach is often suboptimal since the video frames are processed independently. This causes inconsistent segmentation granularity and conflicting 3D pseudo labels, which degrades the accuracy of final segmentation. To address this, we introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences across frames, eliminating conflicting pseudo labels. Combined with a three-stage curriculum learning framework, our approach progressively trains from fragmented single-view data to unified multi-view annotations, ultimately globally coherent full-scene supervision. This structured learning pipeline enables the model to progressively expose to pseudo-labels of increasing consistency. Thus, we can robustly distill a consistent 3D representation from initially fragmented and contradictory 2D priors. Experimental results demonstrated that our method effectively generated consistent and accurate 3D segmentations. Furthermore, the proposed method achieved state-of-the-art results on standard benchmarks and open-vocabulary ability.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00777v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00777v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00777v1">http://arxiv.org/abs/2511.00777v1</a><br>Durian plantation suffers from animal intrusions that cause crop damage and financial loss. The traditional farming practices prove ineffective due to the unavailability of monitoring without human intervention. The fast growth of machine learning and Internet of Things (IoT) technology has led to new ways to detect animals. However, current systems are limited by dependence on single object detection algorithms, less accessible notification platforms, and limited deterrent mechanisms. This research suggests an IoT-enabled animal detection system for durian crops. The system integrates YOLOv5 and SSD object detection algorithms to improve detection accuracy. The system provides real-time monitoring, with detected intrusions automatically reported to farmers via Telegram notifications for rapid response. An automated sound mechanism (e.g., tiger roar) is triggered once the animal is detected. The YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%, 85% and 70%, respectively. The system shows the highest accuracy in daytime and decreases at night, regardless of whether the image is still or a video. Overall, this study contributes a comprehensive and practical framework that combines detection, notification, and deterrence, paving the way for future innovations in automated farming solutions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00795v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00795v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00795v1">http://arxiv.org/abs/2511.00795v1</a><br>Federated Learning (FL) allows multiple institutions to cooperatively train machine learning models while retaining sensitive data at the source, which has great utility in privacy-sensitive environments. However, FL systems remain vulnerable to membership-inference attacks and data heterogeneity. This paper presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using synthetic oncologic CT scans with tumor annotations. It evaluates segmentation performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD. Results show a distinct trade-off between privacy and utility: FedAvg is high performance (Dice around 0.85) with more privacy leakage (attack AUC about 0.72), while DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx and FedBN offer balanced performance under heterogeneous data, especially with non-identical distributed client data. FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00801v3/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00801v3/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00801v3">http://arxiv.org/abs/2511.00801v3</a><br>Medical image editing has emerged as a pivotal technology with broad applications in data augmentation, model interpretability, medical education, and treatment simulation. However, the lack of large-scale, high-quality, and openly accessible datasets tailored for medical contexts with strict anatomical and clinical constraints has significantly hindered progress in this domain. To bridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over 50k medically curated image edits spanning chest X-ray, brain MRI, and fundus photography across 23 diseases. Each sample supports bidirectional lesion editing (addition and removal) and is constructed using Gemini-2.5-Flash-Image based on real clinical images. A key differentiator of our dataset is the medically grounded quality control protocol: we employ an LLM-as-Judge evaluation framework with criteria such as instruction compliance, structural plausibility, image realism, and fidelity preservation, alongside iterative refinement over up to five rounds. Additionally, Med-Banana-50K includes around 37,000 failed editing attempts with full evaluation logs to support preference learning and alignment research. By offering a large-scale, medically rigorous, and fully documented resource, Med-Banana-50K establishes a critical foundation for developing and evaluating reliable medical image editing systems. Our dataset and code are publicly available. [<a href="https://github.com/richardChenzhihui/med-banana-50k]">https://github.com/richardChenzhihui/med-banana-50k]</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00804v2/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00804v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00804v2">http://arxiv.org/abs/2511.00804v2</a><br>Erasing harmful or proprietary concepts from powerful text to image generators is an emerging safety requirement, yet current “concept erasure” techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with GFlowNets equipped with the trajectory balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model’s prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade off between performance and prior preservation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00810v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00810v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00810v1">http://arxiv.org/abs/2511.00810v1</a><br>Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: <a href="https://github.com/sjz5202/GUI-AIMA">https://github.com/sjz5202/GUI-AIMA</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00812v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00812v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00812v1">http://arxiv.org/abs/2511.00812v1</a><br>Vision Transformers have been tremendously successful in computer vision tasks. However, their large computational, memory, and energy demands are a challenge for edge inference on FPGAs – a field that has seen a recent surge in demand. We recognize the benefits of recent works on logic and Look Up Table (LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in offering models that simultaneously reduce both the memory and compute footprints. However, these models natively do not perform well on common vision tasks, such as CIFAR-10&#x2F;100. In this work, we propose LL-ViT, a novel edge optimized vision transformer design that integrates layers of LUT neurons within the transformer architecture. Based on our characterization that reveals that a majority of model weights and computations are from the channel mixer (MLP layer), we design an alternate LUT-based channel mixer, and simultaneously develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to replace each multiplication with a table lookup, our architecture utilizes a neural learning approach which natively learns the LUT functions. This approach allows for reduced model sizes, and a computational and energy-efficient inference solution for vision transformer models. Evaluating on edge-suitable workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and 60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT eliminates over 60% of the model weights and 50% of the multiplications in the model, and achieves 1.9x energy efficiency and 1.3x lower latency over an integer quantized ViT accelerator, while also offering superior throughput against prior works at a 10.9W power budget.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00815v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00815v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00815v1">http://arxiv.org/abs/2511.00815v1</a><br>Pancreas segmentation in medical image processing is a persistent challenge due to its small size, low contrast against adjacent tissues, and significant topological variations. Traditional level set methods drive boundary evolution using gradient flows, often ignoring pointwise topological effects. Conversely, deep learning-based segmentation networks extract rich semantic features but frequently sacrifice structural details. To bridge this gap, we propose a novel model named TA-LSDiff, which combined topology-aware diffusion probabilistic model and level set energy, achieving segmentation without explicit geometric evolution. This energy function guides implicit curve evolution by integrating the input image and deep features through four complementary terms. To further enhance boundary precision, we introduce a pixel-adaptive refinement module that locally modulates the energy function using affinity weighting from neighboring evidence. Ablation studies systematically quantify the contribution of each proposed component. Evaluations on four public pancreas datasets demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming existing methods. These results establish TA-LSDiff as a practical and accurate solution for pancreas segmentation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00821v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00821v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00821v1">http://arxiv.org/abs/2511.00821v1</a><br>Vision-Language Models (VLMs) have demonstrated strong performance across various multimodal tasks, where position encoding plays a vital role in modeling both the sequential structure of textual information and the spatial structure of visual information. However, current VLMs commonly adopt modality-unified 1D or 2D positional indexing strategies, which treat textual and visual tokens uniformly without accounting for their distinct structural properties and sequential continuity for text and spatial coherence for vision. To address this limitation, we propose OMEGA, a novel position encoding framework that employs Modality-Specific Position Encoding (MSPE) to assign positional indices while preserving the inherent structures of each modality across separate coordinate dimensions. Additionally, to align the information density of multimodal data in the positional index space, OMEGA introduces Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the position encoding step size of visual tokens based on the embedding entropy of both modalities. Experimental results demonstrate that OMEGA consistently enhances VLM performance across diverse architectures and VQA benchmarks. On visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00831v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00831v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00831v1">http://arxiv.org/abs/2511.00831v1</a><br>Visual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00833v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00833v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00833v1">http://arxiv.org/abs/2511.00833v1</a><br>Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n &lt;&lt; N. VCA first distils each head’s dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at <a href="https://github.com/LeapLabTHU/LinearDiff">https://github.com/LeapLabTHU/LinearDiff</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Parameter Interpolation Adversarial Training for Robust Image Classification</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00836v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00836v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00836v1">http://arxiv.org/abs/2511.00836v1</a><br>Though deep neural networks exhibit superior performance on various tasks, they are still plagued by adversarial examples. Adversarial training has been demonstrated to be the most effective method to defend against adversarial attacks. However, existing adversarial training methods show that the model robustness has apparent oscillations and overfitting issues in the training process, degrading the defense efficacy. To address these issues, we propose a novel framework called Parameter Interpolation Adversarial Training (PIAT). PIAT tunes the model parameters between each epoch by interpolating the parameters of the previous and current epochs. It makes the decision boundary of model change more moderate and alleviates the overfitting issue, helping the model converge better and achieving higher model robustness. In addition, we suggest using the Normalized Mean Square Error (NMSE) to further improve the robustness by aligning the relative magnitude of logits between clean and adversarial examples rather than the absolute magnitude. Extensive experiments conducted on several benchmark datasets demonstrate that our framework could prominently improve the robustness of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00846v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00846v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00846v1">http://arxiv.org/abs/2511.00846v1</a><br>Brain imaging analysis is vital for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly assisting in that analysis. However, current brain-oriented visual question-answering (VQA) benchmarks either cover a few imaging modalities or are limited to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs throughout the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis.OmniBrainBench consists of 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluation of 24 state-of-the-art models, including open-source, medical, and proprietary MLLMs, highlights the substantial challenges posed by OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5) beat open-source and medical models but lag physicians; (2) medical MLLMs vary widely in performance; (3) open-source MLLMs trail overall but excel in specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks, revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new standard for evaluating and advancing MLLMs in brain imaging analysis, highlighting gaps compared to expert clinical reasoning. We release it at benchmark &amp; code.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00858v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00858v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00858v1">http://arxiv.org/abs/2511.00858v1</a><br>Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model’s ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00859v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00859v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00859v1">http://arxiv.org/abs/2511.00859v1</a><br>In autonomous driving, transparency in the decision-making of perception models is critical, as even a single misperception can be catastrophic. Yet with multi-sensor inputs, it is difficult to determine how each modality contributes to a prediction because sensor information becomes entangled within the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a post-hoc, model-agnostic interpretability method that disentangles modality-specific information across all layers of a pretrained fusion model. To our knowledge, LMD is the first approach to attribute the predictions of a perception model to individual input modalities in a sensor-fusion system for autonomous driving. We evaluate LMD on pretrained fusion models under camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous driving. Its effectiveness is validated using structured perturbation-based metrics and modality-wise visual decompositions, demonstrating practical applicability to interpreting high-capacity multimodal architectures. Code is available at <a href="https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition">https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning with Category-Equivariant Representations for Human Activity Recognition</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00900v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00900v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00900v1">http://arxiv.org/abs/2511.00900v1</a><br>Human activity recognition is challenging because sensor signals shift with context, motion, and environment; effective models must therefore remain stable as the world around them changes. We introduce a categorical symmetry-aware learning framework that captures how signals vary over time, scale, and sensor hierarchy. We build these factors into the structure of feature representations, yielding models that automatically preserve the relationships between sensors and remain stable under realistic distortions such as time shifts, amplitude drift, and device orientation changes. On the UCI Human Activity Recognition benchmark, this categorical symmetry-driven design improves out-of-distribution accuracy by approx. 46 percentage points (approx. 3.6x over the baseline), demonstrating that abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks via category-equivariant representation theory.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00908v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00908v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00908v1">http://arxiv.org/abs/2511.00908v1</a><br>Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00916v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00916v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00916v1">http://arxiv.org/abs/2511.00916v1</a><br>Multimodal Large Language Models (MLLMs) have demonstrated remarkable effectiveness in various general-domain scenarios, such as visual question answering and image captioning. Recently, researchers have increasingly focused on empowering MLLMs with medical conversational abilities, which hold significant promise for clinical applications. However, medical data presents unique challenges due to its heterogeneous nature – encompassing diverse modalities including 2D images, 3D volumetric scans, and temporal video sequences. The substantial domain gap and data format inconsistencies across these modalities have hindered the development of unified medical MLLMs. To address these challenges, we propose Fleming-VL, a unified end-to-end framework for comprehensive medical visual understanding across heterogeneous modalities. Fleming-VL tackles this problem from a data-centric perspective through three key strategies: (1) scaling up pretraining by integrating long-context data from both natural and medical-specific domains; (2) complementing fine-tuning with rare medical data, including holistic video analysis and underrepresented 2D modalities such as ultrasound and dermoscopy images; (3) extending existing evaluation frameworks to incorporate 3D volumetric and video understanding benchmarks. Through supervised fine-tuning (SFT) and group relative policy optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive experiments demonstrate that Fleming-VL achieves state-of-the-art performance across multiple benchmarks, including medical VQA, video QA, and 3D medical image understanding. We publicly release Fleming-VL to promote transparent, reproducible, and auditable progress in medical AI.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00925v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00925v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00925v1">http://arxiv.org/abs/2511.00925v1</a><br>The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved increasing attention due to its wide applications, e.g. e-commerce. Despite progress made in this field, previous works suffer from using imbalanced samples of modalities and inconsistent low-quality information during training, resulting in sub-optimal performance. Therefore, in this paper, we introduce an approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It consists of three components: (i) a Uni-modal Feature Extraction Module that includes a CLIP text encoder and a ViT for extracting textual and visual tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an alignment weight list by the local and global aggregation blocks to measure the aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss Module aiming to improve the balance of domains in the triplet loss. Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, show our method delivers superior performances over the state-of-the-art ZS-SBIR methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00956v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00956v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00956v1">http://arxiv.org/abs/2511.00956v1</a><br>We propose EVTAR, an End-to-End Virtual Try-on model with Additional Reference, that directly fits the target garment onto the person image while incorporating reference images to enhance try-on accuracy. Most existing virtual try-on approaches rely on complex inputs such as agnostic person images, human pose, densepose, or body keypoints, making them labor-intensive and impractical for real-world applications. In contrast, EVTAR adopts a two-stage training strategy, enabling simple inference with only the source image and the target garment inputs. Our model generates try-on results without masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details better. This mechanism is analogous to how humans consider reference models when choosing outfits, thereby simulating a more realistic and high-quality dressing effect. We enrich the training data with supplementary references and unpaired person images to support these capabilities. We evaluate EVTAR on two widely used benchmarks and diverse tasks, and the results consistently validate the effectiveness of our approach.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00933v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00933v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00933v1">http://arxiv.org/abs/2511.00933v1</a><br>Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MID: A Self-supervised Multimodal Iterative Denoising Framework</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00997v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00997v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00997v1">http://arxiv.org/abs/2511.00997v1</a><br>Data denoising is a persistent challenge across scientific and engineering domains. Real-world data is frequently corrupted by complex, non-linear noise, rendering traditional rule-based denoising methods inadequate. To overcome these obstacles, we propose a novel self-supervised multimodal iterative denoising (MID) framework. MID models the collected noisy data as a state within a continuous process of non-linear noise accumulation. By iteratively introducing further noise, MID learns two neural networks: one to estimate the current noise step and another to predict and subtract the corresponding noise increment. For complex non-linear contamination, MID employs a first-order Taylor expansion to locally linearize the noise process, enabling effective iterative removal. Crucially, MID does not require paired clean-noisy datasets, as it learns noise characteristics directly from the noisy inputs. Experiments across four classic computer vision tasks demonstrate MID’s robustness, adaptability, and consistent state-of-the-art performance. Moreover, MID exhibits strong performance and adaptability in tasks within the biomedical and bioinformatics domains.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00962v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00962v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00962v1">http://arxiv.org/abs/2511.00962v1</a><br>Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner. Project Page: <a href="https://rathgrith.github.io/Unified_Frame_VAA/">https://rathgrith.github.io/Unified_Frame_VAA/</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01000v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01000v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01000v1">http://arxiv.org/abs/2511.01000v1</a><br>Art authentication of Francisco Goya’s works presents complex computational challenges due to his heterogeneous stylistic evolution and extensive historical patterns of forgery. We introduce a novel multimodal machine learning framework that applies identical feature extraction techniques to both visual and X-ray radiographic images of Goya paintings. The unified feature extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors, Local Binary Patterns, entropy measures, energy calculations, and colour distribution analysis applied consistently across both imaging modalities. The extracted features from both visual and X-ray images are processed through an optimised One-Class Support Vector Machine with hyperparameter tuning. Using a dataset of 24 authenticated Goya paintings with corresponding X-ray images, split into an 80&#x2F;20 train-test configuration with 10-fold cross-validation, the framework achieves 97.8% classification accuracy with a 0.022 false positive rate. Case study analysis of &#96;&#96;Un Gigante’’ demonstrates the practical efficacy of our pipeline, achieving 92.3% authentication confidence through unified multimodal feature analysis. Our results indicate substantial performance improvement over single-modal approaches, establishing the effectiveness of applying identical computational methods to both visual and radiographic imagery in art authentication applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01013v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01013v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01013v1">http://arxiv.org/abs/2511.01013v1</a><br>B-mode ultrasound for breast cancer diagnosis faces challenges: speckle, operator dependency, and indistinct boundaries. Existing deep learning suffers from single-task learning, architectural constraints (CNNs lack global context, Transformers local features), and black-box decision-making. These gaps hinder clinical adoption.   We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous segmentation and classification with intrinsic interpretability. Its dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via multi-scale hierarchical fusion blocks. An attention-gated decoder provides precision and explainability. We introduce dual-pipeline interpretability: (1) intrinsic attention validation with quantitative IoU verification (mean: 0.86), and (2) Grad-CAM for classification reasoning.   On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +&#x2F;- 0.072 and accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant Recall of 92.1 +&#x2F;- 2.2% ensures minimal false negatives. Ensemble modeling yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant Recall, eliminating false negatives. Ablation studies confirm multi-scale fusion contributes +16.8% Dice and attention gates add +5.9%.   Crucially, we conduct the first cross-dataset generalization study for hybrid CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058), confirming domain shift. However, progressive fine-tuning with only 10% target-domain data (68 images) recovers 92.5% performance. With 50% data, our model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and demonstrating true generalization.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01079v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01079v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01079v1">http://arxiv.org/abs/2511.01079v1</a><br>Neural image compression (NIC) has become the state-of-the-art for rate-distortion performance, yet its security vulnerabilities remain significantly less understood than those of classifiers. Existing adversarial attacks on NICs are often naive adaptations of pixel-space methods, overlooking the unique, structured nature of the compression pipeline. In this work, we propose a more advanced class of vulnerabilities by introducing T-MLA, the first targeted multiscale log–exponential attack framework. Our approach crafts adversarial perturbations in the wavelet domain by directly targeting the quality of the attacked and reconstructed images. This allows for a principled, offline attack where perturbations are strategically confined to specific wavelet subbands, maximizing distortion while ensuring perceptual stealth. Extensive evaluation across multiple state-of-the-art NIC architectures on standard image compression benchmarks reveals a large drop in reconstruction quality while the perturbations remain visually imperceptible. Our findings reveal a critical security flaw at the core of generative and content delivery pipelines.</p>]]></content>
    
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01026v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01026v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01026v1">http://arxiv.org/abs/2511.01026v1</a><br>We present FastBoost, a parameter-efficient neural architecture that achieves state-of-the-art performance on CIFAR benchmarks through a novel Dynamically Scaled Progressive Attention (DSPA) mechanism. Our design establishes new efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and 93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and 74.85% (0.44M parameters) The breakthrough stems from three fundamental innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention blending with dynamic weights. (2) Phase Scaling: Training-stage-aware intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The architecture features dual attention pathways with real-time weight adjustment, cascaded refinement layers (increasing gradient flow by 12.7%), and a hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic attention and efficient convolution operations demonstrates unprecedented parameter-accuracy trade-offs, enabling deployment in resource-constrained edge devices without accuracy degradation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01082v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01082v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01082v1">http://arxiv.org/abs/2511.01082v1</a><br>Image geolocalization, the task of determining an image’s geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at <a href="https://github.com/NNargesNN/GeoToken">https://github.com/NNargesNN/GeoToken</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_00981v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_00981v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00981v1">http://arxiv.org/abs/2511.00981v1</a><br>Accurate vessel segmentation is critical for clinical applications such as disease diagnosis and surgical planning, yet remains challenging due to thin, branching structures and low texture contrast. While foundation models like the Segment Anything Model (SAM) have shown promise in generic segmentation, they perform sub-optimally on vascular structures. In this work, we present VesSAM, a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM integrates (1) a convolutional adapter to enhance local texture features, (2) a multi-prompt encoder that fuses anatomical prompts, including skeletons, bifurcation points, and segment midpoints, via hierarchical cross-attention, and (3) a lightweight mask decoder to reduce jagged artifacts. We also introduce an automated pipeline to generate structured multi-prompt annotations, and curate a diverse benchmark dataset spanning 8 datasets across 5 imaging modalities. Experimental results demonstrate that VesSAM consistently outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13% IoU, and achieves competitive performance compared to fully fine-tuned methods, with significantly fewer parameters. VesSAM also generalizes well to out-of-distribution (OoD) settings, outperforming all baselines in average OoD Dice and IoU.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01087v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01087v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01087v1">http://arxiv.org/abs/2511.01087v1</a><br>The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01098v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01098v1">http://arxiv.org/abs/2511.01098v1</a><br>This study presents a novel method for diagnosing respiratory diseases using image data. It combines Epanechnikov’s non-parametric kernel density estimation (EKDE) with a bimodal logistic regression classifier in a statistical-model-based learning scheme. EKDE’s flexibility in modeling data distributions without assuming specific shapes and its adaptability to pixel intensity variations make it valuable for extracting key features from medical images. The method was tested on 13808 randomly selected chest X-rays from the COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of 59.26%, and a specificity of 74.18%, demonstrating moderate performance in detecting respiratory disease while showing room for improvement in sensitivity. While clinical expertise remains essential for further refining the model, this study highlights the potential of EKDE-based approaches to enhance diagnostic accuracy and reliability in medical imaging.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anatomically Constrained Transformers for Echocardiogram Analysis</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01109v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01109v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01109v1">http://arxiv.org/abs/2511.01109v1</a><br>Video transformers have recently demonstrated strong potential for echocardiogram (echo) analysis, leveraging self-supervised pre-training and flexible adaptation across diverse tasks. However, like other models operating on videos, they are prone to learning spurious correlations from non-diagnostic regions such as image backgrounds. To overcome this limitation, we propose the Video Anatomically Constrained Transformer (ViACT), a novel framework that integrates anatomical priors directly into the transformer architecture. ViACT represents a deforming anatomical structure as a point set and encodes both its spatial geometry and corresponding image patches into transformer tokens. During pre-training, ViACT follows a masked autoencoding strategy that masks and reconstructs only anatomical patches, enforcing that representation learning is focused on the anatomical region. The pre-trained model can then be fine-tuned for tasks localized to this region. In this work we focus on the myocardium, demonstrating the framework on echo analysis tasks such as left ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA) detection. The anatomical constraint focuses transformer attention within the myocardium, yielding interpretable attention maps aligned with regions of known CA pathology. Moreover, ViACT generalizes to myocardium point tracking without requiring task-specific components such as correlation volumes used in specialized tracking networks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</title>
    <link href="/2025/11/02/highlights/2025-11-02-2511_01932v1/"/>
    <url>/2025/11/02/highlights/2025-11-02-2511_01932v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01932v1">http://arxiv.org/abs/2511.01932v1</a><br>Image generation models are usually personalized in practical uses in order to better meet the individual users’ heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56%, when different personalization scenarios are applied to multiple types of image generation models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.MM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach</title>
    <link href="/2025/11/01/cs.CG/2025-11-01-2511_00508v1/"/>
    <url>/2025/11/01/cs.CG/2025-11-01-2511_00508v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00508v1">http://arxiv.org/abs/2511.00508v1</a><br>Reconstruction of an object from points cloud is essential in prosthetics, medical imaging, computer vision, etc. We present an effective algorithm for an Allen–Cahn-type model of reconstruction, employing the Lagrange multiplier approach. Utilizing scattered data points from an object, we reconstruct a narrow shell by solving the governing equation enhanced with an edge detection function derived from the unsigned distance function. The specifically designed edge detection function ensures the energy stability. By reformulating the governing equation through the Lagrange multiplier technique and implementing a Crank–Nicolson time discretization, we can update the solutions in a stable and decoupled manner. The spatial operations are approximated using the finite difference method, and we analytically demonstrate the unconditional stability of the fully discrete scheme. Comprehensive numerical experiments, including reconstructions of complex 3D volumes such as characters from \textit{Star Wars}, validate the algorithm’s accuracy, stability, and effectiveness. Additionally, we analyze how specific parameter selections influence the level of detail and refinement in the reconstructed volumes. To facilitate the interested readers to understand our algorithm, we share the computational codes and data in <a href="https://github.com/cfdyang521/C-3PO/tree/main">https://github.com/cfdyang521/C-3PO/tree/main</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CG</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>65M06, 65M12, 35K57, 65D18</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Automated Petrography</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00328v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00328v1">http://arxiv.org/abs/2511.00328v1</a><br>Petrography is a branch of geology that analyzes the mineralogical composition of rocks from microscopical thin section samples. It is essential for understanding rock properties across geology, archaeology, engineering, mineral exploration, and the oil industry. However, petrography is a labor-intensive task requiring experts to conduct detailed visual examinations of thin section samples through optical polarization microscopes, thus hampering scalability and highlighting the need for automated techniques. To address this challenge, we introduce the Large-scale Imaging and Thin section Optical-polarization Set (LITHOS), the largest and most diverse publicly available experimental framework for automated petrography. LITHOS includes 211,604 high-resolution RGB patches of polarized light and 105,802 expert-annotated grains across 25 mineral categories. Each annotation consists of the mineral class, spatial coordinates, and expert-defined major and minor axes represented as intersecting vector paths, capturing grain geometry and orientation. We evaluate multiple deep learning techniques for mineral classification in LITHOS and propose a dual-encoder transformer architecture that integrates both polarization modalities as a strong baseline for future reference. Our method consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification. We have made the LITHOS Benchmark publicly available, comprising our dataset, code, and pretrained models, to foster reproducibility and further research in automated petrographic analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00352v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00352v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00352v1">http://arxiv.org/abs/2511.00352v1</a><br>The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00362v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00362v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00362v1">http://arxiv.org/abs/2511.00362v1</a><br>Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh’s architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00370v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00370v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00370v1">http://arxiv.org/abs/2511.00370v1</a><br>Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment’s boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents’ localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00392v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00392v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00392v1">http://arxiv.org/abs/2511.00392v1</a><br>Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00411v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00411v1">http://arxiv.org/abs/2511.00411v1</a><br>Adversarial attacks present a critical challenge to deep neural networks’ robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling’s magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at <a href="https://github.com/anuin-cat/GGS">https://github.com/anuin-cat/GGS</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LGCA: Enhancing Semantic Representation via Progressive Expansion</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00419v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00419v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00419v1">http://arxiv.org/abs/2511.00419v1</a><br>Recent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00427v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00427v1">http://arxiv.org/abs/2511.00427v1</a><br>With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP’s space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00429v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00429v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00429v1">http://arxiv.org/abs/2511.00429v1</a><br>Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00443v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00443v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00443v1">http://arxiv.org/abs/2511.00443v1</a><br>The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00477v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00477v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00477v1">http://arxiv.org/abs/2511.00477v1</a><br>Algorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model’s actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00472v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00472v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00472v1">http://arxiv.org/abs/2511.00472v1</a><br>Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (<a href="https://doi.org/10.7937/bq0z-xa62">https://doi.org/10.7937/bq0z-xa62</a>).</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00580v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00580v1">http://arxiv.org/abs/2511.00580v1</a><br>Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4% AUC on UCF-Crime and 83.67% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07, 68T45, 68U10</tag>
      
      <tag>I.2.10; I.5.4; I.4.8; C.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evolve to Inspire: Novelty Search for Diverse Image Generation</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00686v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00686v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00686v1">http://arxiv.org/abs/2511.00686v1</a><br>Text-to-image diffusion models, while proficient at generating high-fidelity images, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iFlyBot-VLA Technical Report</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_01914v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_01914v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01914v1">http://arxiv.org/abs/2511.01914v1</a><br>We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control</title>
    <link href="/2025/11/01/cs.AI/2025-11-01-2511_00681v1/"/>
    <url>/2025/11/01/cs.AI/2025-11-01-2511_00681v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00681v1">http://arxiv.org/abs/2511.00681v1</a><br>Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training</title>
    <link href="/2025/11/01/cs.CR/2025-11-01-2511_00446v1/"/>
    <url>/2025/11/01/cs.CR/2025-11-01-2511_00446v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00446v1">http://arxiv.org/abs/2511.00446v1</a><br>The Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP’s training, remains underexplored. In this work, we introduce ToxicTextCLIP, a framework for generating high-quality adversarial texts that target CLIP during the pre-training phase. The framework addresses two key challenges: semantic misalignment caused by background inconsistency with the target class, and the scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively applies: 1) a background-aware selector that prioritizes texts with background content aligned to the target class, and 2) a background-driven augmenter that generates semantically coherent and diverse poisoned samples. Extensive experiments on classification and retrieval tasks show that ToxicTextCLIP achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be accessed via <a href="https://github.com/xinyaocse/ToxicTextCLIP/">https://github.com/xinyaocse/ToxicTextCLIP/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding</title>
    <link href="/2025/11/01/cs.CL/2025-11-01-2511_04699v1/"/>
    <url>/2025/11/01/cs.CL/2025-11-01-2511_04699v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04699v1">http://arxiv.org/abs/2511.04699v1</a><br>Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset comprises over 2.5 million of samples, including 1.5 million textual data, 270K fully annotated tables, and hundred thousands of real data based charts. Our pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents. In addition to text, the corpus includes variety of rendered styles for charts and tables. Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities. SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</title>
    <link href="/2025/11/01/cs.GR/2025-11-01-2511_00362v1/"/>
    <url>/2025/11/01/cs.GR/2025-11-01-2511_00362v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00362v1">http://arxiv.org/abs/2511.00362v1</a><br>Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh’s architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images</title>
    <link href="/2025/11/01/cs.GR/2025-11-01-2511_00702v1/"/>
    <url>/2025/11/01/cs.GR/2025-11-01-2511_00702v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00702v1">http://arxiv.org/abs/2511.00702v1</a><br>Doctors and researchers routinely use diffusion tensor imaging (DTI) and tractography to visualize the fibrous structure of tissues in the human body. This paper explores the connection of these techniques to the painterly rendering of images. Using a tractography algorithm the presented method can place brush strokes that mimic the painting process of human artists, analogously to how fibres are tracked in DTI. The analogue to the diffusion tensor for image orientation is the structural tensor, which can provide better local orientation information than the gradient alone. I demonstrate this technique in portraits and general images, and discuss the parallels between fibre tracking and brush stroke placement, and frame it in the language of tractography. This work presents an exploratory investigation into the cross-domain application of diffusion tensor imaging techniques to painterly rendering of images. All the code is available at <a href="https://github.com/tito21/st-python">https://github.com/tito21/st-python</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-based ground distance detection for crop-residue-covered soil</title>
    <link href="/2025/11/01/cs.GR/2025-11-01-2511_00548v1/"/>
    <url>/2025/11/01/cs.GR/2025-11-01-2511_00548v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00548v1">http://arxiv.org/abs/2511.00548v1</a><br>Conservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>eess.IV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VisionCAD: An Integration-Free Radiology Copilot Framework</title>
    <link href="/2025/11/01/cs.HC/2025-11-01-2511_00381v1/"/>
    <url>/2025/11/01/cs.HC/2025-11-01-2511_00381v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00381v1">http://arxiv.org/abs/2511.00381v1</a><br>Widespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2% across classification tasks, while natural language generation metrics for automated reports remain within 1% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00335v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00335v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00335v1">http://arxiv.org/abs/2511.00335v1</a><br>Lightweight vision classification models such as MobileNet, ShuffleNet, and EfficientNet are increasingly deployed in mobile and embedded systems, yet their performance has been predominantly benchmarked on ImageNet. This raises critical questions: Do models that excel on ImageNet also generalize across other domains? How can cross-dataset robustness be systematically quantified? And which architectural elements consistently drive generalization under tight resource constraints? Here, we present the first systematic evaluation of 11 lightweight vision models (2.5M parameters), trained under a fixed 100-epoch schedule across 7 diverse datasets. We introduce the Cross-Dataset Score (xScore), a unified metric that quantifies the consistency and robustness of model performance across diverse visual domains. Our results show that (1) ImageNet accuracy does not reliably predict performance on fine-grained or medical datasets, (2) xScore provides a scalable predictor of mobile model performance that can be estimated from just four datasets, and (3) certain architectural components–such as isotropic convolutions with higher spatial resolution and channel-wise attention–promote broader generalization, while Transformer-based blocks yield little additional benefit, despite incurring higher parameter overhead. This study provides a reproducible framework for evaluating lightweight vision models beyond ImageNet, highlights key design principles for mobile-friendly architectures, and guides the development of future models that generalize robustly across diverse application domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00411v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00411v1">http://arxiv.org/abs/2511.00411v1</a><br>Adversarial attacks present a critical challenge to deep neural networks’ robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling’s magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at <a href="https://github.com/anuin-cat/GGS">https://github.com/anuin-cat/GGS</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00345v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00345v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00345v1">http://arxiv.org/abs/2511.00345v1</a><br>Accurate and up-to-date geospatial data are essential for urban planning, infrastructure monitoring, and environmental management. Yet, automating urban monitoring remains difficult because curated datasets of specific urban features and their changes are scarce. We introduce OSMGen, a generative framework that creates realistic satellite imagery directly from raw OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen uses the full richness of OSM JSON, including vector geometries, semantic tags, location, and time, giving fine-grained control over how scenes are generated. A central feature of the framework is the ability to produce consistent before-after image pairs: user edits to OSM inputs translate into targeted visual changes, while the rest of the scene is preserved. This makes it possible to generate training data that addresses scarcity and class imbalance, and to give planners a simple way to preview proposed interventions by editing map data. More broadly, OSMGen produces paired (JSON, image) data for both static and changed states, paving the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates. Source code is available at <a href="https://github.com/amir-zsh/OSMGen">https://github.com/amir-zsh/OSMGen</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00443v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00443v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00443v1">http://arxiv.org/abs/2511.00443v1</a><br>The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00446v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00446v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00446v1">http://arxiv.org/abs/2511.00446v1</a><br>The Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP’s training, remains underexplored. In this work, we introduce ToxicTextCLIP, a framework for generating high-quality adversarial texts that target CLIP during the pre-training phase. The framework addresses two key challenges: semantic misalignment caused by background inconsistency with the target class, and the scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively applies: 1) a background-aware selector that prioritizes texts with background content aligned to the target class, and 2) a background-driven augmenter that generates semantically coherent and diverse poisoned samples. Extensive experiments on classification and retrieval tasks show that ToxicTextCLIP achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be accessed via <a href="https://github.com/xinyaocse/ToxicTextCLIP/">https://github.com/xinyaocse/ToxicTextCLIP/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00449v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00449v1">http://arxiv.org/abs/2511.00449v1</a><br>Accurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT).</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00480v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00480v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00480v1">http://arxiv.org/abs/2511.00480v1</a><br>In this paper, we introduce FedMGP, a new paradigm for personalized federated prompt learning in vision-language models. FedMGP equips each client with multiple groups of paired textual and visual prompts, enabling the model to capture diverse, fine-grained semantic and instance-level cues. A diversity loss is introduced to drive each prompt group to specialize in distinct and complementary semantic aspects, ensuring that the groups collectively cover a broader range of local characteristics. During communication, FedMGP employs a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling: each client computes the cosine similarity between its prompt groups and the global prompts from the previous round, then samples s groups via a softmax-weighted distribution. This soft selection mechanism preferentially aggregates semantically aligned knowledge while still enabling exploration of underrepresented patterns effectively balancing the preservation of common knowledge with client-specific features. Notably, FedMGP maintains parameter efficiency by redistributing a fixed prompt capacity across multiple groups, achieving state-of-the-art performance with the lowest communication parameters among all federated prompt learning methods. Theoretical analysis shows that our dynamic aggregation strategy promotes robust global representation learning by reinforcing shared semantics while suppressing client-specific noise. Extensive experiments demonstrate that FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks. The code will be released on <a href="https://github.com/weihao-bo/FedMGP.git">https://github.com/weihao-bo/FedMGP.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00543v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00543v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00543v1">http://arxiv.org/abs/2511.00543v1</a><br>Recent advances in generative modeling enable neural networks to generate weights without relying on gradient-based optimization. However, current methods are limited by issues of over-coupling and long-horizon. The former tightly binds weight generation with task-specific objectives, thereby limiting the flexibility of the learned optimizer. The latter leads to inefficiency and low accuracy during inference, caused by the lack of local constraints. In this paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that enhances flexibility through learning various optimization policies. It adopts a hybrid-policy sub-trajectory balance objective, which integrates on-policy and off-policy learning to capture local optimization policies. Theoretically, we demonstrate that learning solely local optimization policies can address the long-horizon issue while enhancing the generation of global optimal weights. In addition, we validate Lo-Hp’s superior accuracy and inference efficiency in tasks that require frequent weight updates, such as transfer learning, few-shot learning, domain generalization, and large language model adaptation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control</title>
    <link href="/2025/11/01/cs.LG/2025-11-01-2511_00681v1/"/>
    <url>/2025/11/01/cs.LG/2025-11-01-2511_00681v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00681v1">http://arxiv.org/abs/2511.00681v1</a><br>Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach</title>
    <link href="/2025/11/01/cs.NA/2025-11-01-2511_00508v1/"/>
    <url>/2025/11/01/cs.NA/2025-11-01-2511_00508v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00508v1">http://arxiv.org/abs/2511.00508v1</a><br>Reconstruction of an object from points cloud is essential in prosthetics, medical imaging, computer vision, etc. We present an effective algorithm for an Allen–Cahn-type model of reconstruction, employing the Lagrange multiplier approach. Utilizing scattered data points from an object, we reconstruct a narrow shell by solving the governing equation enhanced with an edge detection function derived from the unsigned distance function. The specifically designed edge detection function ensures the energy stability. By reformulating the governing equation through the Lagrange multiplier technique and implementing a Crank–Nicolson time discretization, we can update the solutions in a stable and decoupled manner. The spatial operations are approximated using the finite difference method, and we analytically demonstrate the unconditional stability of the fully discrete scheme. Comprehensive numerical experiments, including reconstructions of complex 3D volumes such as characters from \textit{Star Wars}, validate the algorithm’s accuracy, stability, and effectiveness. Additionally, we analyze how specific parameter selections influence the level of detail and refinement in the reconstructed volumes. To facilitate the interested readers to understand our algorithm, we share the computational codes and data in <a href="https://github.com/cfdyang521/C-3PO/tree/main">https://github.com/cfdyang521/C-3PO/tree/main</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.NA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CG</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>65M06, 65M12, 35K57, 65D18</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping</title>
    <link href="/2025/11/01/cs.RO/2025-11-01-2511_00392v1/"/>
    <url>/2025/11/01/cs.RO/2025-11-01-2511_00392v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00392v1">http://arxiv.org/abs/2511.00392v1</a><br>Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</title>
    <link href="/2025/11/01/cs.RO/2025-11-01-2511_00510v1/"/>
    <url>/2025/11/01/cs.RO/2025-11-01-2511_00510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00510v1">http://arxiv.org/abs/2511.00510v1</a><br>This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360{\deg} Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at <a href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iFlyBot-VLA Technical Report</title>
    <link href="/2025/11/01/cs.RO/2025-11-01-2511_01914v1/"/>
    <url>/2025/11/01/cs.RO/2025-11-01-2511_01914v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01914v1">http://arxiv.org/abs/2511.01914v1</a><br>We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-based ground distance detection for crop-residue-covered soil</title>
    <link href="/2025/11/01/cs.SY/2025-11-01-2511_00548v1/"/>
    <url>/2025/11/01/cs.SY/2025-11-01-2511_00548v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00548v1">http://arxiv.org/abs/2511.00548v1</a><br>Conservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.</p>]]></content>
    
    
    <categories>
      
      <category>cs.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>eess.IV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Automated Petrography</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00328v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00328v1">http://arxiv.org/abs/2511.00328v1</a><br>Petrography is a branch of geology that analyzes the mineralogical composition of rocks from microscopical thin section samples. It is essential for understanding rock properties across geology, archaeology, engineering, mineral exploration, and the oil industry. However, petrography is a labor-intensive task requiring experts to conduct detailed visual examinations of thin section samples through optical polarization microscopes, thus hampering scalability and highlighting the need for automated techniques. To address this challenge, we introduce the Large-scale Imaging and Thin section Optical-polarization Set (LITHOS), the largest and most diverse publicly available experimental framework for automated petrography. LITHOS includes 211,604 high-resolution RGB patches of polarized light and 105,802 expert-annotated grains across 25 mineral categories. Each annotation consists of the mineral class, spatial coordinates, and expert-defined major and minor axes represented as intersecting vector paths, capturing grain geometry and orientation. We evaluate multiple deep learning techniques for mineral classification in LITHOS and propose a dual-encoder transformer architecture that integrates both polarization modalities as a strong baseline for future reference. Our method consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification. We have made the LITHOS Benchmark publicly available, comprising our dataset, code, and pretrained models, to foster reproducibility and further research in automated petrographic analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00338v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00338v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00338v1">http://arxiv.org/abs/2511.00338v1</a><br>This work presents a novel hybrid approach that integrates Deep Operator Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex inverse problem. The method effectively addresses tasks such as source localization governed by the Navier-Stokes equations and image reconstruction, overcoming challenges related to nonlinearity, sparsity, and noisy data. By incorporating physics-informed constraints and task-specific regularization into the loss function, the framework ensures solutions that are both physically consistent and accurate. Validation on diverse synthetic and real datasets demonstrates its robustness, scalability, and precision, showcasing its broad potential applications in computational physics and imaging sciences.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00335v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00335v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00335v1">http://arxiv.org/abs/2511.00335v1</a><br>Lightweight vision classification models such as MobileNet, ShuffleNet, and EfficientNet are increasingly deployed in mobile and embedded systems, yet their performance has been predominantly benchmarked on ImageNet. This raises critical questions: Do models that excel on ImageNet also generalize across other domains? How can cross-dataset robustness be systematically quantified? And which architectural elements consistently drive generalization under tight resource constraints? Here, we present the first systematic evaluation of 11 lightweight vision models (2.5M parameters), trained under a fixed 100-epoch schedule across 7 diverse datasets. We introduce the Cross-Dataset Score (xScore), a unified metric that quantifies the consistency and robustness of model performance across diverse visual domains. Our results show that (1) ImageNet accuracy does not reliably predict performance on fine-grained or medical datasets, (2) xScore provides a scalable predictor of mobile model performance that can be estimated from just four datasets, and (3) certain architectural components–such as isotropic convolutions with higher spatial resolution and channel-wise attention–promote broader generalization, while Transformer-based blocks yield little additional benefit, despite incurring higher parameter overhead. This study provides a reproducible framework for evaluating lightweight vision models beyond ImageNet, highlights key design principles for mobile-friendly architectures, and guides the development of future models that generalize robustly across diverse application domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00344v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00344v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00344v1">http://arxiv.org/abs/2511.00344v1</a><br>Multimodal Emotion Recognition in Conversations (MERC) enhances emotional understanding through the fusion of multimodal signals. However, unpredictable modality absence in real-world scenarios significantly degrades the performance of existing methods. Conventional missing-modality recovery approaches, which depend on training with complete multimodal data, often suffer from semantic distortion under extreme data distributions, such as fixed-modality absence. To address this, we propose the Federated Dialogue-guided and Semantic-Consistent Diffusion (FedDISC) framework, pioneering the integration of federated learning into missing-modality recovery. By federated aggregation of modality-specific diffusion models trained on clients and broadcasting them to clients missing corresponding modalities, FedDISC overcomes single-client reliance on modality completeness. Additionally, the DISC-Diffusion module ensures consistency in context, speaker identity, and semantics between recovered and available modalities, using a Dialogue Graph Network to capture conversational dependencies and a Semantic Conditioning Network to enforce semantic alignment. We further introduce a novel Alternating Frozen Aggregation strategy, which cyclically freezes recovery and classifier modules to facilitate collaborative optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI datasets demonstrate that FedDISC achieves superior emotion classification performance across diverse missing modality patterns, outperforming existing approaches.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00352v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00352v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00352v1">http://arxiv.org/abs/2511.00352v1</a><br>The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00357v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00357v1">http://arxiv.org/abs/2511.00357v1</a><br>Onboard cloud segmentation is a critical yet underexplored task in thermal Earth observation (EO), particularly for CubeSat missions constrained by limited hardware and spectral information. CubeSats often rely on a single thermal band and lack sufficient labeled data, making conventional cloud masking techniques infeasible. This work addresses these challenges by applying transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using a UNet with a lightweight MobileNet encoder. We pretrain the model on the public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small set of mission-specific samples in a joint-training setup, improving the macro F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a TensorRT engine and demonstrate full-image inference in under 5 seconds on an NVIDIA Jetson Nano. These results show that leveraging public datasets and lightweight architectures can enable accurate, efficient thermal-only cloud masking on-orbit, supporting real-time decision-making in data-limited EO missions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00345v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00345v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00345v1">http://arxiv.org/abs/2511.00345v1</a><br>Accurate and up-to-date geospatial data are essential for urban planning, infrastructure monitoring, and environmental management. Yet, automating urban monitoring remains difficult because curated datasets of specific urban features and their changes are scarce. We introduce OSMGen, a generative framework that creates realistic satellite imagery directly from raw OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen uses the full richness of OSM JSON, including vector geometries, semantic tags, location, and time, giving fine-grained control over how scenes are generated. A central feature of the framework is the ability to produce consistent before-after image pairs: user edits to OSM inputs translate into targeted visual changes, while the rest of the scene is preserved. This makes it possible to generate training data that addresses scarcity and class imbalance, and to give planners a simple way to preview proposed interventions by editing map data. More broadly, OSMGen produces paired (JSON, image) data for both static and changed states, paving the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates. Source code is available at <a href="https://github.com/amir-zsh/OSMGen">https://github.com/amir-zsh/OSMGen</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00362v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00362v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00362v1">http://arxiv.org/abs/2511.00362v1</a><br>Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh’s architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00370v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00370v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00370v1">http://arxiv.org/abs/2511.00370v1</a><br>Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment’s boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents’ localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VisionCAD: An Integration-Free Radiology Copilot Framework</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00381v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00381v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00381v1">http://arxiv.org/abs/2511.00381v1</a><br>Widespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2% across classification tasks, while natural language generation metrics for automated reports remain within 1% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00389v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00389v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00389v1">http://arxiv.org/abs/2511.00389v1</a><br>Multimodal Large Language Models (MLLMs) have revolutionized numerous research fields, including computer vision and affective computing. As a pivotal challenge in this interdisciplinary domain, facial expression recognition (FER) has evolved from separate, domain-specific models to more unified approaches. One promising avenue to unify FER tasks is converting conventional FER datasets into visual question-answering (VQA) formats, enabling the direct application of powerful generalist MLLMs for inference. However, despite the success of cutting-edge MLLMs in various tasks, their performance on FER tasks remains largely unexplored. To address this gap, we provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art MLLMs across four widely used FER datasets. Our results reveal that, while MLLMs exhibit good classification performance, they still face significant limitations in reasoning and interpretability. To this end, we introduce post-training strategies aimed at enhancing the facial expression reasoning capabilities of MLLMs. Specifically, we curate two high-quality and large-scale datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards (RLVR), respectively. Building upon them, we develop a unified and interpretable FER foundation model termed UniFER-7B, which outperforms many open-sourced and closed-source generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00391v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00391v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00391v1">http://arxiv.org/abs/2511.00391v1</a><br>Multimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like Chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode \textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on various multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, underscoring the effectiveness of our coarse-to-fine ViRL strategy. The code and model will be available at <a href="https://github.com/DocTron-hub/VinciCoder">https://github.com/DocTron-hub/VinciCoder</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00392v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00392v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00392v1">http://arxiv.org/abs/2511.00392v1</a><br>Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00396v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00396v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00396v1">http://arxiv.org/abs/2511.00396v1</a><br>We present the first unified framework that jointly handles three operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model (VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a lightweight single-sample algorithm that leverages the discrepancy between reward and model confidence as a per-sample advantage signal. This design naturally focuses updates on informative responses while eliminating group sampling, thereby addressing GRPO’s key limitations: confidence-agnostic learning, signal dilution, and prohibitive computational overhead. We also introduce an “output-to-reasoning” strategy to construct high-fidelity SFT data that ensures logical consistency with ground-truth masks. Experiments show our model matches or outperforms specialized SOTA methods and strong closed-source VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for CoSOD, surpassing the prior best by 8.0 percentage points, despite using far less training data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00411v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00411v1">http://arxiv.org/abs/2511.00411v1</a><br>Adversarial attacks present a critical challenge to deep neural networks’ robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling’s magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at <a href="https://github.com/anuin-cat/GGS">https://github.com/anuin-cat/GGS</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LGCA: Enhancing Semantic Representation via Progressive Expansion</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00419v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00419v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00419v1">http://arxiv.org/abs/2511.00419v1</a><br>Recent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00427v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00427v1">http://arxiv.org/abs/2511.00427v1</a><br>With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP’s space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00429v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00429v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00429v1">http://arxiv.org/abs/2511.00429v1</a><br>Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00443v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00443v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00443v1">http://arxiv.org/abs/2511.00443v1</a><br>The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00446v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00446v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00446v1">http://arxiv.org/abs/2511.00446v1</a><br>The Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP’s training, remains underexplored. In this work, we introduce ToxicTextCLIP, a framework for generating high-quality adversarial texts that target CLIP during the pre-training phase. The framework addresses two key challenges: semantic misalignment caused by background inconsistency with the target class, and the scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively applies: 1) a background-aware selector that prioritizes texts with background content aligned to the target class, and 2) a background-driven augmenter that generates semantically coherent and diverse poisoned samples. Extensive experiments on classification and retrieval tasks show that ToxicTextCLIP achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be accessed via <a href="https://github.com/xinyaocse/ToxicTextCLIP/">https://github.com/xinyaocse/ToxicTextCLIP/</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00449v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00449v1">http://arxiv.org/abs/2511.00449v1</a><br>Accurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT).</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00456v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00456v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00456v1">http://arxiv.org/abs/2511.00456v1</a><br>This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach utilizes image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven ImageNet-pretrained architectures ResNet-18&#x2F;50, DenseNet-121, EfficientNet-B0, MobileNet-V2&#x2F;V3, and ViT-B16 under identical training conditions with focal loss and patient-wise splits to prevent data leakage. Experimental results on the Kermany CXR dataset demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test accuracy of 98%, ROC-AUC &#x3D; 0.997, and F1 &#x3D; 0.987, while MobileNet-V2 provides an optimal trade-off between accuracy and computational cost. Grad-CAM visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised explainable models that enhance pneumonia screening transparency, and clinical trust in AI-assisted medical imaging.   <a href="https://github.com/kiranshahi/pneumonia-analysis">https://github.com/kiranshahi/pneumonia-analysis</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00468v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00468v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00468v1">http://arxiv.org/abs/2511.00468v1</a><br>Recent advances in generative models have achieved high-fidelity in 3D human reconstruction, yet their utility for specific tasks (e.g., human 3D segmentation) remains constrained. We propose HumanCrafter, a unified framework that enables the joint modeling of appearance and human-part semantics from a single image in a feed-forward manner. Specifically, we integrate human geometric priors in the reconstruction stage and self-supervised semantic priors in the segmentation stage. To address labeled 3D human datasets scarcity, we further develop an interactive annotation procedure for generating high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task synergy, while the multi-task objective simultaneously optimizes texture modeling fidelity and semantic consistency. Extensive experiments demonstrate that HumanCrafter surpasses existing state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from a single image.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00480v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00480v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00480v1">http://arxiv.org/abs/2511.00480v1</a><br>In this paper, we introduce FedMGP, a new paradigm for personalized federated prompt learning in vision-language models. FedMGP equips each client with multiple groups of paired textual and visual prompts, enabling the model to capture diverse, fine-grained semantic and instance-level cues. A diversity loss is introduced to drive each prompt group to specialize in distinct and complementary semantic aspects, ensuring that the groups collectively cover a broader range of local characteristics. During communication, FedMGP employs a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling: each client computes the cosine similarity between its prompt groups and the global prompts from the previous round, then samples s groups via a softmax-weighted distribution. This soft selection mechanism preferentially aggregates semantically aligned knowledge while still enabling exploration of underrepresented patterns effectively balancing the preservation of common knowledge with client-specific features. Notably, FedMGP maintains parameter efficiency by redistributing a fixed prompt capacity across multiple groups, achieving state-of-the-art performance with the lowest communication parameters among all federated prompt learning methods. Theoretical analysis shows that our dynamic aggregation strategy promotes robust global representation learning by reinforcing shared semantics while suppressing client-specific noise. Extensive experiments demonstrate that FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks. The code will be released on <a href="https://github.com/weihao-bo/FedMGP.git">https://github.com/weihao-bo/FedMGP.git</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00477v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00477v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00477v1">http://arxiv.org/abs/2511.00477v1</a><br>Algorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model’s actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00472v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00472v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00472v1">http://arxiv.org/abs/2511.00472v1</a><br>Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (<a href="https://doi.org/10.7937/bq0z-xa62">https://doi.org/10.7937/bq0z-xa62</a>).</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00503v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00503v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00503v1">http://arxiv.org/abs/2511.00503v1</a><br>We introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00508v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00508v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00508v1">http://arxiv.org/abs/2511.00508v1</a><br>Reconstruction of an object from points cloud is essential in prosthetics, medical imaging, computer vision, etc. We present an effective algorithm for an Allen–Cahn-type model of reconstruction, employing the Lagrange multiplier approach. Utilizing scattered data points from an object, we reconstruct a narrow shell by solving the governing equation enhanced with an edge detection function derived from the unsigned distance function. The specifically designed edge detection function ensures the energy stability. By reformulating the governing equation through the Lagrange multiplier technique and implementing a Crank–Nicolson time discretization, we can update the solutions in a stable and decoupled manner. The spatial operations are approximated using the finite difference method, and we analytically demonstrate the unconditional stability of the fully discrete scheme. Comprehensive numerical experiments, including reconstructions of complex 3D volumes such as characters from \textit{Star Wars}, validate the algorithm’s accuracy, stability, and effectiveness. Additionally, we analyze how specific parameter selections influence the level of detail and refinement in the reconstructed volumes. To facilitate the interested readers to understand our algorithm, we share the computational codes and data in <a href="https://github.com/cfdyang521/C-3PO/tree/main">https://github.com/cfdyang521/C-3PO/tree/main</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CG</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>65M06, 65M12, 35K57, 65D18</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00510v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00510v1">http://arxiv.org/abs/2511.00510v1</a><br>This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360{\deg} Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at <a href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00511v2/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00511v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00511v2">http://arxiv.org/abs/2511.00511v2</a><br>Video generative models pretrained on large-scale datasets can produce high-quality videos, but are often conditioned on text or a single image, limiting controllability and applicability. We introduce ID-Composer, a novel framework that addresses this gap by tackling multi-subject video generation from a text prompt and reference images. This task is challenging as it requires preserving subject identities, integrating semantics across subjects and modalities, and maintaining temporal consistency. To faithfully preserve the subject consistency and textual information in synthesized videos, ID-Composer designs a hierarchical identity-preserving attention mechanism, which effectively aggregates features within and across subjects and modalities. To effectively allow for the semantic following of user intention, we introduce semantic understanding via pretrained vision-language model (VLM), leveraging VLM’s superior semantic understanding to provide fine-grained guidance and capture complex interactions between multiple subjects. Considering that standard diffusion loss often fails in aligning the critical concepts like subject ID, we employ an online reinforcement learning phase to drive the overall training objective of ID-Composer into RLVR. Extensive experiments demonstrate that our model surpasses existing methods in identity preservation, temporal consistency, and video quality.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00523v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00523v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00523v1">http://arxiv.org/abs/2511.00523v1</a><br>Vision language models such as CLIP have shown remarkable performance in zero shot classification, but remain susceptible to spurious correlations, where irrelevant visual features influence predictions. Existing debiasing methods often require access to training data and explicit group labels to perform fine-tuning or adjust embeddings, which limits their practicality in real-world settings. Test-time methods attempt to avoid this constraint, but many still depend on prior knowledge of dataset specific biases, limiting their generalizability in open set settings. In this work, we propose a test-time debiasing method for ViT based CLIP models that requires no additional training or assumptions of bias annotations. Our approach uses a pretrained segmentation model to isolate the target visual attribute, then adjusts the non target regions so that their embeddings are uniformly similar to all class specific text prompts. This procedure removes unintended bias signals from confounding visual regions while preserving the target attribute. Experiments on Waterbirds and CelebA show that our method outperforms existing test-time debiasing approaches in both group robustness metrics and Attention IoU. These results demonstrate the effectiveness of segmentation guided interventions for scalable and annotation free bias mitigation in vision language models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text-guided Fine-Grained Video Anomaly Detection</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00524v2/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00524v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00524v2">http://arxiv.org/abs/2511.00524v2</a><br>Video Anomaly Detection (VAD) aims to identify anomalous events within video segments. In scenarios such as surveillance or industrial process monitoring, anomaly detection is of critical importance. While existing approaches are semi-automated, requiring human assessment for anomaly detection, traditional VADs offer limited output as either normal or anomalous. We propose Text-guided Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD) that performs pixel-wise visual-textual feature alignment to generate fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly Encoder (RAE) that transforms the heatmaps into learnable textual embeddings, guiding the LVLM to accurately identify and localize anomalous events in videos. This significantly enhances both the granularity and interactivity of anomaly detection. The proposed method achieving SOTA performance by demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and 67.8%&#x2F;76.7% accuracy in anomaly heatmaps (RBDC&#x2F;TBDC) on the UBnormal dataset, and subjectively verified more preferable textual description on the ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories; Yes&#x2F;No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for targets, 78.10 for trajectories; Yes&#x2F;No accuracy: 89.73%).</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00540v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00540v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00540v1">http://arxiv.org/abs/2511.00540v1</a><br>Industrial Anomaly Detection (IAD) is critical for enhancing operational safety, ensuring product quality, and optimizing manufacturing efficiency across global industries. However, the IAD algorithms are severely constrained by the limitations of existing public benchmarks. Current datasets exhibit restricted category diversity and insufficient scale, frequently resulting in metric saturation and limited model transferability to real-world scenarios. To address this gap, we introduce Real-IAD Variety, the largest and most diverse IAD benchmark, comprising 198,960 high-resolution images across 160 distinct object categories. Its diversity is ensured through comprehensive coverage of 28 industries, 24 material types, and 22 color variations. Our comprehensive experimental analysis validates the benchmark’s substantial challenge: state-of-the-art multi-class unsupervised anomaly detection methods experience significant performance degradation when scaled from 30 to 160 categories. Crucially, we demonstrate that vision-language models exhibit remarkable robustness to category scale-up, with minimal performance variation across different category counts, significantly enhancing generalization capabilities in diverse industrial contexts. The unprecedented scale and complexity of Real-IAD Variety position it as an essential resource for training and evaluating next-generation foundation models for anomaly detection. By providing this comprehensive benchmark with rigorous evaluation protocols across multi-class unsupervised, multi-view, and zero-&#x2F;few-shot settings, we aim to accelerate research beyond domain-specific constraints, enabling the development of scalable, general-purpose anomaly detection systems. Real-IAD Variety will be made publicly available to facilitate innovation in this critical field.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIFO: Learning and Synthesizing Multi-Instance from One Image</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00542v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00542v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00542v1">http://arxiv.org/abs/2511.00542v1</a><br>This paper proposes a method for precise learning and synthesizing multi-instance semantics from a single image. The difficulty of this problem lies in the limited training data, and it becomes even more challenging when the instances to be learned have similar semantics or appearance. To address this, we propose a penalty-based attention optimization to disentangle similar semantics during the learning stage. Then, in the synthesis, we introduce and optimize box control in attention layers to further mitigate semantic leakage while precisely controlling the output layout. Experimental results demonstrate that our method achieves disentangled and high-quality semantic learning and synthesis, strikingly balancing editability and instance consistency. Our method remains robust when dealing with semantically or visually similar instances or rare-seen objects. The code is publicly available at <a href="https://github.com/Kareneveve/MIFO">https://github.com/Kareneveve/MIFO</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00543v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00543v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00543v1">http://arxiv.org/abs/2511.00543v1</a><br>Recent advances in generative modeling enable neural networks to generate weights without relying on gradient-based optimization. However, current methods are limited by issues of over-coupling and long-horizon. The former tightly binds weight generation with task-specific objectives, thereby limiting the flexibility of the learned optimizer. The latter leads to inefficiency and low accuracy during inference, caused by the lack of local constraints. In this paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that enhances flexibility through learning various optimization policies. It adopts a hybrid-policy sub-trajectory balance objective, which integrates on-policy and off-policy learning to capture local optimization policies. Theoretically, we demonstrate that learning solely local optimization policies can address the long-horizon issue while enhancing the generation of global optimal weights. In addition, we validate Lo-Hp’s superior accuracy and inference efficiency in tasks that require frequent weight updates, such as transfer learning, few-shot learning, domain generalization, and large language model adaptation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-based ground distance detection for crop-residue-covered soil</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00548v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00548v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00548v1">http://arxiv.org/abs/2511.00548v1</a><br>Conservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>eess.IV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00504v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00504v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00504v1">http://arxiv.org/abs/2511.00504v1</a><br>We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset contains 17,597 question-answer pairs across 4,394 images, each annotated with radiologist-verified bounding boxes and clinical reasoning explanations. Our question taxonomy spans six diagnostic types-Where, What, Is there, How many, Which, and Yes&#x2F;No-capturing diverse clinical intents. To improve reliability, we construct a balanced distribution of 41.7% positive and 58.3% negative samples, mitigating hallucinations in normal cases. Benchmarking with MedGemma-4B-it demonstrates improved performance (F1 &#x3D; 0.624, +11.8% over baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance reproducible and clinically grounded Med-VQA research. The dataset and evaluation tools are publicly available at huggingface.co&#x2F;datasets&#x2F;Dangindev&#x2F;VinDR-CXR-VQA.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00560v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00560v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00560v1">http://arxiv.org/abs/2511.00560v1</a><br>Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis, extending it to dynamic scenes still results in substantial memory overhead from replicating Gaussians across frames. To address this challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines voxel-based representations with neural Gaussian splatting for efficient dynamic scene modeling. Instead of generating separate Gaussian sets per timestamp, our method employs a compact set of neural voxels with learned deformation fields to model temporal dynamics. The design greatly reduces memory consumption and accelerates training while preserving high image quality. We further introduce a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization, maintaining global efficiency while enhancing rendering quality for difficult viewing angles. Experiments demonstrate that our method outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00580v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00580v1">http://arxiv.org/abs/2511.00580v1</a><br>Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4% AUC on UCF-Crime and 83.67% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07, 68T45, 68U10</tag>
      
      <tag>I.2.10; I.5.4; I.4.8; C.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00598v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00598v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00598v1">http://arxiv.org/abs/2511.00598v1</a><br>Registration of optical and synthetic aperture radar (SAR) remote sensing images serves as a critical foundation for image fusion and visual navigation tasks. This task is particularly challenging because of their modal discrepancy, primarily manifested as severe nonlinear radiometric differences (NRD), geometric distortions, and noise variations. Under large geometric transformations, existing classical template-based and sparse keypoint-based strategies struggle to achieve reliable registration results for optical-SAR image pairs. To address these limitations, we propose GDROS, a geometry-guided dense registration framework leveraging global cross-modal image interactions. First, we extract cross-modal deep features from optical and SAR images through a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D correlation volume is constructed and iteratively refined to establish pixel-wise dense correspondences. Subsequently, we implement a least squares regression (LSR) module to geometrically constrain the predicted dense optical flow field. Such geometry guidance mitigates prediction divergence by directly imposing an estimated affine transformation on the final flow predictions. Extensive experiments have been conducted on three representative datasets WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial resolutions, demonstrating robust performance of our proposed method across different imaging resolutions. Qualitative and quantitative results show that GDROS significantly outperforms current state-of-the-art methods in all metrics. Our source code will be released at: <a href="https://github.com/Zi-Xuan-Sun/GDROS">https://github.com/Zi-Xuan-Sun/GDROS</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00613v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00613v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00613v1">http://arxiv.org/abs/2511.00613v1</a><br>How far are deep models from real-world video anomaly understanding (VAU)? Current works typically emphasize on detecting unexpected occurrences deviated from normal patterns or comprehending anomalous events with interpretable descriptions. However, they exhibit only a superficial comprehension of real-world anomalies, with limited breadth in complex principles and subtle context that distinguish the anomalies from normalities, e.g., climbing cliffs with safety gear vs. without it. To this end, we introduce CueBench, the first of its kind Benchmark, devoted to Context-aware video anomalies within a Unified Evaluation framework. We comprehensively establish an event-centric hierarchical taxonomy that anchors two core event types: 14 conditional and 18 absolute anomaly events, defined by their refined semantics from diverse contexts across 174 scenes and 198 attributes. Based on this, we propose to unify and benchmark context-aware VAU with various challenging tasks across recognition, temporal grounding, detection, and anticipation. This also serves as a rigorous and fair probing evaluation suite for generative-discriminative as well as generalized-specialized vision-language models (VLMs). To address the challenges underlying CueBench, we further develop Cue-R1 based on R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative manner. Extensive results on CueBench reveal that, existing VLMs are still far from satisfactory real-world anomaly understanding, while our Cue-R1 surpasses these state-of-the-art approaches by over 24% on average.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00643v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00643v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00643v1">http://arxiv.org/abs/2511.00643v1</a><br>Understanding surgical instrument-tissue interactions requires not only identifying which instrument performs which action on which anatomical target, but also grounding these interactions spatially within the surgical scene. Existing surgical action triplet recognition methods are limited to learning from frame-level classification, failing to reliably link actions to specific instrument instances.Previous attempts at spatial grounding have primarily relied on class activation maps, which lack the precision and robustness required for detailed instrument-tissue interaction analysis.To address this gap, we propose grounding surgical action triplets with instrument instance segmentation, or triplet segmentation for short, a new unified task which produces spatially grounded &lt;instrument, verb, target&gt; outputs.We start by presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000 annotated frames, linking instrument instance masks with action verb and anatomical target annotations, and establishing the first benchmark for strongly supervised, instance-level triplet grounding and evaluation.To learn triplet segmentation, we propose TargetFusionNet, a novel architecture that extends Mask2Former with a target-aware fusion mechanism to address the challenge of accurate anatomical target prediction by fusing weak anatomy priors with instrument instance queries.Evaluated across recognition, detection, and triplet segmentation metrics, TargetFusionNet consistently improves performance over existing baselines, demonstrating that strong instance supervision combined with weak target priors significantly enhances the accuracy and robustness of surgical action understanding.Triplet segmentation establishes a unified framework for spatially grounding surgical action triplets. The proposed benchmark and architecture pave the way for more interpretable, surgical scene understanding.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00652v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00652v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00652v1">http://arxiv.org/abs/2511.00652v1</a><br>An autonomous vehicle can generate several terabytes of sensor data per day. A significant portion of this data consists of 3D point clouds produced by depth sensors such as LiDARs. This data must be transferred to cloud storage, where it is utilized for training machine learning models or conducting analyses, such as forensic investigations in the event of an accident. To reduce network and storage costs, this paper introduces DejaView. Although prior work uses interframe redundancies to compress data, DejaView searches for and uses redundancies on larger temporal scales (days and months) for more effective compression. We designed DejaView with the insight that the operating area of autonomous vehicles is limited and that vehicles mostly traverse the same routes daily. Consequently, the 3D data they collect daily is likely similar to the data they have captured in the past. To capture this, the core of DejaView is a diff operation that compactly represents point clouds as delta w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end implementation of DejaView can compress point clouds by a factor of 210 at a reconstruction error of only 15 cm.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00653v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00653v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00653v1">http://arxiv.org/abs/2511.00653v1</a><br>Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for applications such as forest inventory, carbon monitoring and biodiversity assessment. Traditionally, ITS has been achieved with unsupervised geometry-based algorithms, while more recent advances have shifted toward supervised deep learning (DL). In the past, progress in method development was hindered by the lack of large-scale benchmark datasets, and the availability of novel data formats, particularly multispectral (MS) LiDAR, remains limited to this day, despite evidence that MS reflectance can improve the accuracy of ITS. This study introduces FGI-EMIT, the first large-scale MS airborne laser scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550 nm, the dataset consists of 1,561 manually annotated trees, with a particular focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked four conventional unsupervised algorithms and four supervised DL approaches. Hyperparameters of unsupervised methods were optimized using a Bayesian approach, while DL models were trained from scratch. Among the unsupervised methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL approaches performed significantly better overall, with the best model, ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9 percentage points. An ablation study demonstrated that current DL-based approaches generally fail to leverage MS reflectance information when it is provided as additional input features, although single channel reflectance can improve accuracy marginally, especially for understory trees. A performance analysis across point densities further showed that DL methods consistently remain superior to unsupervised algorithms, even at densities as low as 10 points&#x2F;m$^2$.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00681v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00681v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00681v1">http://arxiv.org/abs/2511.00681v1</a><br>Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Outlier-Aware Post-Training Quantization for Image Super-Resolution</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00682v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00682v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00682v1">http://arxiv.org/abs/2511.00682v1</a><br>Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75 speedup.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evolve to Inspire: Novelty Search for Diverse Image Generation</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00686v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00686v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00686v1">http://arxiv.org/abs/2511.00686v1</a><br>Text-to-image diffusion models, while proficient at generating high-fidelity images, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00698v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00698v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00698v1">http://arxiv.org/abs/2511.00698v1</a><br>Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to mitigate high exposure side effects, but often suffers from noise and artifacts that affect diagnostic accuracy. To tackle this issue, deep learning models have been developed to enhance LDCT images. Various loss functions have been employed, including classical approaches such as Mean Square Error and adversarial losses, as well as customized loss functions(LFs) designed for specific architectures. Although these models achieve remarkable performance in terms of PSNR and SSIM, these metrics are limited in their ability to reflect perceptual quality, especially for medical images. In this paper, we focus on one of the most critical elements of DL-based architectures, namely the loss function. We conduct an objective analysis of the relevance of different loss functions for LDCT image quality enhancement and their consistency with image quality metrics. Our findings reveal inconsistencies between LFs and quality metrics, and highlight the need of consideration of image quality metrics when developing a new loss function for image quality enhancement.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00702v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00702v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00702v1">http://arxiv.org/abs/2511.00702v1</a><br>Doctors and researchers routinely use diffusion tensor imaging (DTI) and tractography to visualize the fibrous structure of tissues in the human body. This paper explores the connection of these techniques to the painterly rendering of images. Using a tractography algorithm the presented method can place brush strokes that mimic the painting process of human artists, analogously to how fibres are tracked in DTI. The analogue to the diffusion tensor for image orientation is the structural tensor, which can provide better local orientation information than the gradient alone. I demonstrate this technique in portraits and general images, and discuss the parallels between fibre tracking and brush stroke placement, and frame it in the language of tractography. This work presents an exploratory investigation into the cross-domain application of diffusion tensor imaging techniques to painterly rendering of images. All the code is available at <a href="https://github.com/tito21/st-python">https://github.com/tito21/st-python</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Validating Deep Models for Alzheimer&#39;s 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00728v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00728v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00728v1">http://arxiv.org/abs/2511.00728v1</a><br>Deep learning models have shown strong performance in diagnosing Alzheimer’s disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with training datasets largely composed of North American cohorts such as those in the Alzheimer’s Disease Neuroimaging Initiative (ADNI). However, their generalization to underrepresented populations remains underexplored. In this study, we benchmark convolutional and Transformer-based models on the ADNI dataset and assess their generalization performance on a novel Latin American clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show that while all models achieve high AUCs on ADNI (up to .96, .97), their performance drops substantially on FLENI (down to .82, .80, respectively), revealing a significant domain shift. The tested architectures demonstrated similar performance, calling into question the supposed advantages of transformers for this specific task. Through ablation studies, we identify per-image normalization and a correct sampling selection as key factors for generalization. Occlusion sensitivity analysis further reveals that models trained on ADNI, generally attend to canonical hypometabolic regions for the AD class, but focus becomes unclear for the other classes and for FLENI scans. These findings highlight the need for population-aware validation of diagnostic AI models and motivate future work on domain adaptation and cohort diversification.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00573v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00573v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00573v1">http://arxiv.org/abs/2511.00573v1</a><br>Generalized Category Discovery (GCD) aims to leverage labeled samples from known categories to cluster unlabeled data that may include both known and unknown categories. While existing methods have achieved impressive results under standard conditions, their performance often deteriorates in the presence of distribution shifts. In this paper, we explore a more realistic task: Domain-Shifted Generalized Category Discovery (DS_GCD), where the unlabeled data includes not only unknown categories but also samples from unknown domains. To tackle this challenge, we propose a \textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE) that enhances the model’s ability to discover categories under distributional shift by leveraging frequency-domain information. Specifically, we first propose a frequency-based domain separation strategy that partitions samples into known and unknown domains by measuring their amplitude differences. We then propose two types of frequency-domain perturbation strategies: a cross-domain strategy, which adapts to new distributions by exchanging amplitude components across domains, and an intra-domain strategy, which enhances robustness to intra-domain variations within the unknown domain. Furthermore, we extend the self-supervised contrastive objective and semantic clustering loss to better guide the training process. Finally, we introduce a clustering-difficulty-aware resampling technique to adaptively focus on harder-to-cluster categories, further enhancing model performance. Extensive experiments demonstrate that our method effectively mitigates the impact of distributional shifts across various benchmark datasets and achieves superior performance in discovering both known and unknown categories.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards classification-based representation learning for place recognition on LiDAR scans</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_00738v2/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_00738v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00738v2">http://arxiv.org/abs/2511.00738v2</a><br>Place recognition is a crucial task in autonomous driving, allowing vehicles to determine their position using sensor data. While most existing methods rely on contrastive learning, we explore an alternative approach by framing place recognition as a multi-class classification problem. Our method assigns discrete location labels to LiDAR scans and trains an encoder-decoder model to classify each scan’s position directly. We evaluate this approach on the NuScenes dataset and show that it achieves competitive performance compared to contrastive learning-based methods while offering advantages in training efficiency and stability.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iFlyBot-VLA Technical Report</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_01914v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_01914v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01914v1">http://arxiv.org/abs/2511.01914v1</a><br>We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_01915v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_01915v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01915v1">http://arxiv.org/abs/2511.01915v1</a><br>Purpose: This study provides the first comprehensive evaluation of foundation models in fetal ultrasound (US) imaging under low inter-class variability conditions. While recent vision foundation models such as DINOv3 have shown remarkable transferability across medical domains, their ability to discriminate anatomically similar structures has not been systematically investigated. We address this gap by focusing on fetal brain standard planes–transthalamic (TT), transventricular (TV), and transcerebellar (TC)–which exhibit highly overlapping anatomical features and pose a critical challenge for reliable biometric assessment.   Methods: To ensure a fair and reproducible evaluation, all publicly available fetal ultrasound datasets were curated and aggregated into a unified multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated images from heterogeneous acquisition settings. DINOv3 was pretrained in a self-supervised manner to learn ultrasound-aware representations. The learned features were then evaluated through standardized adaptation protocols, including linear probing with frozen backbone and full fine-tuning, under two initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization from natural-image DINOv3 weights.   Results: Models pretrained on fetal ultrasound data consistently outperformed those initialized on natural images, with weighted F1-score improvements of up to 20 percent. Domain-adaptive pretraining enabled the network to preserve subtle echogenic and structural cues crucial for distinguishing intermediate planes such as TV.   Conclusion: Results demonstrate that generic foundation models fail to generalize under low inter-class variability, whereas domain-specific pretraining is essential to achieve robust and clinically reliable representations in fetal brain ultrasound imaging.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding</title>
    <link href="/2025/11/01/cs.CV/2025-11-01-2511_04699v1/"/>
    <url>/2025/11/01/cs.CV/2025-11-01-2511_04699v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04699v1">http://arxiv.org/abs/2511.04699v1</a><br>Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset comprises over 2.5 million of samples, including 1.5 million textual data, 270K fully annotated tables, and hundred thousands of real data based charts. Our pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents. In addition to text, the corpus includes variety of rendered styles for charts and tables. Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities. SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements</title>
    <link href="/2025/11/01/eess.IV/2025-11-01-2511_00449v1/"/>
    <url>/2025/11/01/eess.IV/2025-11-01-2511_00449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00449v1">http://arxiv.org/abs/2511.00449v1</a><br>Accurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT).</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation</title>
    <link href="/2025/11/01/eess.IV/2025-11-01-2511_00477v1/"/>
    <url>/2025/11/01/eess.IV/2025-11-01-2511_00477v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00477v1">http://arxiv.org/abs/2511.00477v1</a><br>Algorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model’s actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</title>
    <link href="/2025/11/01/eess.IV/2025-11-01-2511_00510v1/"/>
    <url>/2025/11/01/eess.IV/2025-11-01-2511_00510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00510v1">http://arxiv.org/abs/2511.00510v1</a><br>This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360{\deg} Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at <a href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-based ground distance detection for crop-residue-covered soil</title>
    <link href="/2025/11/01/eess.IV/2025-11-01-2511_00548v1/"/>
    <url>/2025/11/01/eess.IV/2025-11-01-2511_00548v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00548v1">http://arxiv.org/abs/2511.00548v1</a><br>Conservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>eess.IV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations</title>
    <link href="/2025/11/01/eess.IV/2025-11-01-2511_00598v1/"/>
    <url>/2025/11/01/eess.IV/2025-11-01-2511_00598v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00598v1">http://arxiv.org/abs/2511.00598v1</a><br>Registration of optical and synthetic aperture radar (SAR) remote sensing images serves as a critical foundation for image fusion and visual navigation tasks. This task is particularly challenging because of their modal discrepancy, primarily manifested as severe nonlinear radiometric differences (NRD), geometric distortions, and noise variations. Under large geometric transformations, existing classical template-based and sparse keypoint-based strategies struggle to achieve reliable registration results for optical-SAR image pairs. To address these limitations, we propose GDROS, a geometry-guided dense registration framework leveraging global cross-modal image interactions. First, we extract cross-modal deep features from optical and SAR images through a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D correlation volume is constructed and iteratively refined to establish pixel-wise dense correspondences. Subsequently, we implement a least squares regression (LSR) module to geometrically constrain the predicted dense optical flow field. Such geometry guidance mitigates prediction divergence by directly imposing an estimated affine transformation on the final flow predictions. Extensive experiments have been conducted on three representative datasets WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial resolutions, demonstrating robust performance of our proposed method across different imaging resolutions. Qualitative and quantitative results show that GDROS significantly outperforms current state-of-the-art methods in all metrics. Our source code will be released at: <a href="https://github.com/Zi-Xuan-Sun/GDROS">https://github.com/Zi-Xuan-Sun/GDROS</a>.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound</title>
    <link href="/2025/11/01/eess.IV/2025-11-01-2511_01915v1/"/>
    <url>/2025/11/01/eess.IV/2025-11-01-2511_01915v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01915v1">http://arxiv.org/abs/2511.01915v1</a><br>Purpose: This study provides the first comprehensive evaluation of foundation models in fetal ultrasound (US) imaging under low inter-class variability conditions. While recent vision foundation models such as DINOv3 have shown remarkable transferability across medical domains, their ability to discriminate anatomically similar structures has not been systematically investigated. We address this gap by focusing on fetal brain standard planes–transthalamic (TT), transventricular (TV), and transcerebellar (TC)–which exhibit highly overlapping anatomical features and pose a critical challenge for reliable biometric assessment.   Methods: To ensure a fair and reproducible evaluation, all publicly available fetal ultrasound datasets were curated and aggregated into a unified multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated images from heterogeneous acquisition settings. DINOv3 was pretrained in a self-supervised manner to learn ultrasound-aware representations. The learned features were then evaluated through standardized adaptation protocols, including linear probing with frozen backbone and full fine-tuning, under two initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization from natural-image DINOv3 weights.   Results: Models pretrained on fetal ultrasound data consistently outperformed those initialized on natural images, with weighted F1-score improvements of up to 20 percent. Domain-adaptive pretraining enabled the network to preserve subtle echogenic and structural cues crucial for distinguishing intermediate planes such as TV.   Conclusion: Results demonstrate that generic foundation models fail to generalize under low inter-class variability, whereas domain-specific pretraining is essential to achieve robust and clinically reliable representations in fetal brain ultrasound imaging.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars</title>
    <link href="/2025/11/01/eess.IV/2025-11-01-2511_00652v1/"/>
    <url>/2025/11/01/eess.IV/2025-11-01-2511_00652v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00652v1">http://arxiv.org/abs/2511.00652v1</a><br>An autonomous vehicle can generate several terabytes of sensor data per day. A significant portion of this data consists of 3D point clouds produced by depth sensors such as LiDARs. This data must be transferred to cloud storage, where it is utilized for training machine learning models or conducting analyses, such as forensic investigations in the event of an accident. To reduce network and storage costs, this paper introduces DejaView. Although prior work uses interframe redundancies to compress data, DejaView searches for and uses redundancies on larger temporal scales (days and months) for more effective compression. We designed DejaView with the insight that the operating area of autonomous vehicles is limited and that vehicles mostly traverse the same routes daily. Consequently, the 3D data they collect daily is likely similar to the data they have captured in the past. To capture this, the core of DejaView is a diff operation that compactly represents point clouds as delta w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end implementation of DejaView can compress point clouds by a factor of 210 at a reconstruction error of only 15 cm.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-based ground distance detection for crop-residue-covered soil</title>
    <link href="/2025/11/01/eess.SY/2025-11-01-2511_00548v1/"/>
    <url>/2025/11/01/eess.SY/2025-11-01-2511_00548v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00548v1">http://arxiv.org/abs/2511.00548v1</a><br>Conservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>eess.IV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Automated Petrography</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00328v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00328v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00328v1">http://arxiv.org/abs/2511.00328v1</a><br>Petrography is a branch of geology that analyzes the mineralogical composition of rocks from microscopical thin section samples. It is essential for understanding rock properties across geology, archaeology, engineering, mineral exploration, and the oil industry. However, petrography is a labor-intensive task requiring experts to conduct detailed visual examinations of thin section samples through optical polarization microscopes, thus hampering scalability and highlighting the need for automated techniques. To address this challenge, we introduce the Large-scale Imaging and Thin section Optical-polarization Set (LITHOS), the largest and most diverse publicly available experimental framework for automated petrography. LITHOS includes 211,604 high-resolution RGB patches of polarized light and 105,802 expert-annotated grains across 25 mineral categories. Each annotation consists of the mineral class, spatial coordinates, and expert-defined major and minor axes represented as intersecting vector paths, capturing grain geometry and orientation. We evaluate multiple deep learning techniques for mineral classification in LITHOS and propose a dual-encoder transformer architecture that integrates both polarization modalities as a strong baseline for future reference. Our method consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification. We have made the LITHOS Benchmark publicly available, comprising our dataset, code, and pretrained models, to foster reproducibility and further research in automated petrographic analysis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00335v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00335v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00335v1">http://arxiv.org/abs/2511.00335v1</a><br>Lightweight vision classification models such as MobileNet, ShuffleNet, and EfficientNet are increasingly deployed in mobile and embedded systems, yet their performance has been predominantly benchmarked on ImageNet. This raises critical questions: Do models that excel on ImageNet also generalize across other domains? How can cross-dataset robustness be systematically quantified? And which architectural elements consistently drive generalization under tight resource constraints? Here, we present the first systematic evaluation of 11 lightweight vision models (2.5M parameters), trained under a fixed 100-epoch schedule across 7 diverse datasets. We introduce the Cross-Dataset Score (xScore), a unified metric that quantifies the consistency and robustness of model performance across diverse visual domains. Our results show that (1) ImageNet accuracy does not reliably predict performance on fine-grained or medical datasets, (2) xScore provides a scalable predictor of mobile model performance that can be estimated from just four datasets, and (3) certain architectural components–such as isotropic convolutions with higher spatial resolution and channel-wise attention–promote broader generalization, while Transformer-based blocks yield little additional benefit, despite incurring higher parameter overhead. This study provides a reproducible framework for evaluating lightweight vision models beyond ImageNet, highlights key design principles for mobile-friendly architectures, and guides the development of future models that generalize robustly across diverse application domains.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00338v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00338v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00338v1">http://arxiv.org/abs/2511.00338v1</a><br>This work presents a novel hybrid approach that integrates Deep Operator Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex inverse problem. The method effectively addresses tasks such as source localization governed by the Navier-Stokes equations and image reconstruction, overcoming challenges related to nonlinearity, sparsity, and noisy data. By incorporating physics-informed constraints and task-specific regularization into the loss function, the framework ensures solutions that are both physically consistent and accurate. Validation on diverse synthetic and real datasets demonstrates its robustness, scalability, and precision, showcasing its broad potential applications in computational physics and imaging sciences.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00345v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00345v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00345v1">http://arxiv.org/abs/2511.00345v1</a><br>Accurate and up-to-date geospatial data are essential for urban planning, infrastructure monitoring, and environmental management. Yet, automating urban monitoring remains difficult because curated datasets of specific urban features and their changes are scarce. We introduce OSMGen, a generative framework that creates realistic satellite imagery directly from raw OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen uses the full richness of OSM JSON, including vector geometries, semantic tags, location, and time, giving fine-grained control over how scenes are generated. A central feature of the framework is the ability to produce consistent before-after image pairs: user edits to OSM inputs translate into targeted visual changes, while the rest of the scene is preserved. This makes it possible to generate training data that addresses scarcity and class imbalance, and to give planners a simple way to preview proposed interventions by editing map data. More broadly, OSMGen produces paired (JSON, image) data for both static and changed states, paving the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates. Source code is available at <a href="https://github.com/amir-zsh/OSMGen">https://github.com/amir-zsh/OSMGen</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00344v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00344v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00344v1">http://arxiv.org/abs/2511.00344v1</a><br>Multimodal Emotion Recognition in Conversations (MERC) enhances emotional understanding through the fusion of multimodal signals. However, unpredictable modality absence in real-world scenarios significantly degrades the performance of existing methods. Conventional missing-modality recovery approaches, which depend on training with complete multimodal data, often suffer from semantic distortion under extreme data distributions, such as fixed-modality absence. To address this, we propose the Federated Dialogue-guided and Semantic-Consistent Diffusion (FedDISC) framework, pioneering the integration of federated learning into missing-modality recovery. By federated aggregation of modality-specific diffusion models trained on clients and broadcasting them to clients missing corresponding modalities, FedDISC overcomes single-client reliance on modality completeness. Additionally, the DISC-Diffusion module ensures consistency in context, speaker identity, and semantics between recovered and available modalities, using a Dialogue Graph Network to capture conversational dependencies and a Semantic Conditioning Network to enforce semantic alignment. We further introduce a novel Alternating Frozen Aggregation strategy, which cyclically freezes recovery and classifier modules to facilitate collaborative optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI datasets demonstrate that FedDISC achieves superior emotion classification performance across diverse missing modality patterns, outperforming existing approaches.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00352v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00352v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00352v1">http://arxiv.org/abs/2511.00352v1</a><br>The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00357v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00357v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00357v1">http://arxiv.org/abs/2511.00357v1</a><br>Onboard cloud segmentation is a critical yet underexplored task in thermal Earth observation (EO), particularly for CubeSat missions constrained by limited hardware and spectral information. CubeSats often rely on a single thermal band and lack sufficient labeled data, making conventional cloud masking techniques infeasible. This work addresses these challenges by applying transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using a UNet with a lightweight MobileNet encoder. We pretrain the model on the public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small set of mission-specific samples in a joint-training setup, improving the macro F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a TensorRT engine and demonstrate full-image inference in under 5 seconds on an NVIDIA Jetson Nano. These results show that leveraging public datasets and lightweight architectures can enable accurate, efficient thermal-only cloud masking on-orbit, supporting real-time decision-making in data-limited EO missions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00362v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00362v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00362v1">http://arxiv.org/abs/2511.00362v1</a><br>Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh’s architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00370v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00370v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00370v1">http://arxiv.org/abs/2511.00370v1</a><br>Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment’s boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents’ localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00389v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00389v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00389v1">http://arxiv.org/abs/2511.00389v1</a><br>Multimodal Large Language Models (MLLMs) have revolutionized numerous research fields, including computer vision and affective computing. As a pivotal challenge in this interdisciplinary domain, facial expression recognition (FER) has evolved from separate, domain-specific models to more unified approaches. One promising avenue to unify FER tasks is converting conventional FER datasets into visual question-answering (VQA) formats, enabling the direct application of powerful generalist MLLMs for inference. However, despite the success of cutting-edge MLLMs in various tasks, their performance on FER tasks remains largely unexplored. To address this gap, we provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art MLLMs across four widely used FER datasets. Our results reveal that, while MLLMs exhibit good classification performance, they still face significant limitations in reasoning and interpretability. To this end, we introduce post-training strategies aimed at enhancing the facial expression reasoning capabilities of MLLMs. Specifically, we curate two high-quality and large-scale datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards (RLVR), respectively. Building upon them, we develop a unified and interpretable FER foundation model termed UniFER-7B, which outperforms many open-sourced and closed-source generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VisionCAD: An Integration-Free Radiology Copilot Framework</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00381v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00381v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00381v1">http://arxiv.org/abs/2511.00381v1</a><br>Widespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2% across classification tasks, while natural language generation metrics for automated reports remain within 1% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00391v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00391v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00391v1">http://arxiv.org/abs/2511.00391v1</a><br>Multimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like Chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode \textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on various multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, underscoring the effectiveness of our coarse-to-fine ViRL strategy. The code and model will be available at <a href="https://github.com/DocTron-hub/VinciCoder">https://github.com/DocTron-hub/VinciCoder</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00396v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00396v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00396v1">http://arxiv.org/abs/2511.00396v1</a><br>We present the first unified framework that jointly handles three operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model (VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a lightweight single-sample algorithm that leverages the discrepancy between reward and model confidence as a per-sample advantage signal. This design naturally focuses updates on informative responses while eliminating group sampling, thereby addressing GRPO’s key limitations: confidence-agnostic learning, signal dilution, and prohibitive computational overhead. We also introduce an “output-to-reasoning” strategy to construct high-fidelity SFT data that ensures logical consistency with ground-truth masks. Experiments show our model matches or outperforms specialized SOTA methods and strong closed-source VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for CoSOD, surpassing the prior best by 8.0 percentage points, despite using far less training data.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00392v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00392v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00392v1">http://arxiv.org/abs/2511.00392v1</a><br>Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00411v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00411v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00411v1">http://arxiv.org/abs/2511.00411v1</a><br>Adversarial attacks present a critical challenge to deep neural networks’ robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling’s magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at <a href="https://github.com/anuin-cat/GGS">https://github.com/anuin-cat/GGS</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LGCA: Enhancing Semantic Representation via Progressive Expansion</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00419v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00419v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00419v1">http://arxiv.org/abs/2511.00419v1</a><br>Recent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00429v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00429v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00429v1">http://arxiv.org/abs/2511.00429v1</a><br>Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00427v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00427v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00427v1">http://arxiv.org/abs/2511.00427v1</a><br>With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP’s space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00443v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00443v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00443v1">http://arxiv.org/abs/2511.00443v1</a><br>The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00449v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00449v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00449v1">http://arxiv.org/abs/2511.00449v1</a><br>Accurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT).</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00456v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00456v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00456v1">http://arxiv.org/abs/2511.00456v1</a><br>This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach utilizes image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven ImageNet-pretrained architectures ResNet-18&#x2F;50, DenseNet-121, EfficientNet-B0, MobileNet-V2&#x2F;V3, and ViT-B16 under identical training conditions with focal loss and patient-wise splits to prevent data leakage. Experimental results on the Kermany CXR dataset demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test accuracy of 98%, ROC-AUC &#x3D; 0.997, and F1 &#x3D; 0.987, while MobileNet-V2 provides an optimal trade-off between accuracy and computational cost. Grad-CAM visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised explainable models that enhance pneumonia screening transparency, and clinical trust in AI-assisted medical imaging.   <a href="https://github.com/kiranshahi/pneumonia-analysis">https://github.com/kiranshahi/pneumonia-analysis</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00446v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00446v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00446v1">http://arxiv.org/abs/2511.00446v1</a><br>The Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP’s training, remains underexplored. In this work, we introduce ToxicTextCLIP, a framework for generating high-quality adversarial texts that target CLIP during the pre-training phase. The framework addresses two key challenges: semantic misalignment caused by background inconsistency with the target class, and the scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively applies: 1) a background-aware selector that prioritizes texts with background content aligned to the target class, and 2) a background-driven augmenter that generates semantically coherent and diverse poisoned samples. Extensive experiments on classification and retrieval tasks show that ToxicTextCLIP achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be accessed via <a href="https://github.com/xinyaocse/ToxicTextCLIP/">https://github.com/xinyaocse/ToxicTextCLIP/</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00468v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00468v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00468v1">http://arxiv.org/abs/2511.00468v1</a><br>Recent advances in generative models have achieved high-fidelity in 3D human reconstruction, yet their utility for specific tasks (e.g., human 3D segmentation) remains constrained. We propose HumanCrafter, a unified framework that enables the joint modeling of appearance and human-part semantics from a single image in a feed-forward manner. Specifically, we integrate human geometric priors in the reconstruction stage and self-supervised semantic priors in the segmentation stage. To address labeled 3D human datasets scarcity, we further develop an interactive annotation procedure for generating high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task synergy, while the multi-task objective simultaneously optimizes texture modeling fidelity and semantic consistency. Extensive experiments demonstrate that HumanCrafter surpasses existing state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from a single image.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00472v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00472v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00472v1">http://arxiv.org/abs/2511.00472v1</a><br>Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (<a href="https://doi.org/10.7937/bq0z-xa62">https://doi.org/10.7937/bq0z-xa62</a>).</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00477v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00477v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00477v1">http://arxiv.org/abs/2511.00477v1</a><br>Algorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model’s actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00480v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00480v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00480v1">http://arxiv.org/abs/2511.00480v1</a><br>In this paper, we introduce FedMGP, a new paradigm for personalized federated prompt learning in vision-language models. FedMGP equips each client with multiple groups of paired textual and visual prompts, enabling the model to capture diverse, fine-grained semantic and instance-level cues. A diversity loss is introduced to drive each prompt group to specialize in distinct and complementary semantic aspects, ensuring that the groups collectively cover a broader range of local characteristics. During communication, FedMGP employs a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling: each client computes the cosine similarity between its prompt groups and the global prompts from the previous round, then samples s groups via a softmax-weighted distribution. This soft selection mechanism preferentially aggregates semantically aligned knowledge while still enabling exploration of underrepresented patterns effectively balancing the preservation of common knowledge with client-specific features. Notably, FedMGP maintains parameter efficiency by redistributing a fixed prompt capacity across multiple groups, achieving state-of-the-art performance with the lowest communication parameters among all federated prompt learning methods. Theoretical analysis shows that our dynamic aggregation strategy promotes robust global representation learning by reinforcing shared semantics while suppressing client-specific noise. Extensive experiments demonstrate that FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks. The code will be released on <a href="https://github.com/weihao-bo/FedMGP.git">https://github.com/weihao-bo/FedMGP.git</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00503v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00503v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00503v1">http://arxiv.org/abs/2511.00503v1</a><br>We introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00504v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00504v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00504v1">http://arxiv.org/abs/2511.00504v1</a><br>We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset contains 17,597 question-answer pairs across 4,394 images, each annotated with radiologist-verified bounding boxes and clinical reasoning explanations. Our question taxonomy spans six diagnostic types-Where, What, Is there, How many, Which, and Yes&#x2F;No-capturing diverse clinical intents. To improve reliability, we construct a balanced distribution of 41.7% positive and 58.3% negative samples, mitigating hallucinations in normal cases. Benchmarking with MedGemma-4B-it demonstrates improved performance (F1 &#x3D; 0.624, +11.8% over baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance reproducible and clinically grounded Med-VQA research. The dataset and evaluation tools are publicly available at huggingface.co&#x2F;datasets&#x2F;Dangindev&#x2F;VinDR-CXR-VQA.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00510v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00510v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00510v1">http://arxiv.org/abs/2511.00510v1</a><br>This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360{\deg} Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at <a href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00508v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00508v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00508v1">http://arxiv.org/abs/2511.00508v1</a><br>Reconstruction of an object from points cloud is essential in prosthetics, medical imaging, computer vision, etc. We present an effective algorithm for an Allen–Cahn-type model of reconstruction, employing the Lagrange multiplier approach. Utilizing scattered data points from an object, we reconstruct a narrow shell by solving the governing equation enhanced with an edge detection function derived from the unsigned distance function. The specifically designed edge detection function ensures the energy stability. By reformulating the governing equation through the Lagrange multiplier technique and implementing a Crank–Nicolson time discretization, we can update the solutions in a stable and decoupled manner. The spatial operations are approximated using the finite difference method, and we analytically demonstrate the unconditional stability of the fully discrete scheme. Comprehensive numerical experiments, including reconstructions of complex 3D volumes such as characters from \textit{Star Wars}, validate the algorithm’s accuracy, stability, and effectiveness. Additionally, we analyze how specific parameter selections influence the level of detail and refinement in the reconstructed volumes. To facilitate the interested readers to understand our algorithm, we share the computational codes and data in <a href="https://github.com/cfdyang521/C-3PO/tree/main">https://github.com/cfdyang521/C-3PO/tree/main</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>math.NA</tag>
      
      <tag>cs.CG</tag>
      
      <tag>cs.CV</tag>
      
      <tag>cs.NA</tag>
      
      <tag>65M06, 65M12, 35K57, 65D18</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00511v2/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00511v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00511v2">http://arxiv.org/abs/2511.00511v2</a><br>Video generative models pretrained on large-scale datasets can produce high-quality videos, but are often conditioned on text or a single image, limiting controllability and applicability. We introduce ID-Composer, a novel framework that addresses this gap by tackling multi-subject video generation from a text prompt and reference images. This task is challenging as it requires preserving subject identities, integrating semantics across subjects and modalities, and maintaining temporal consistency. To faithfully preserve the subject consistency and textual information in synthesized videos, ID-Composer designs a hierarchical identity-preserving attention mechanism, which effectively aggregates features within and across subjects and modalities. To effectively allow for the semantic following of user intention, we introduce semantic understanding via pretrained vision-language model (VLM), leveraging VLM’s superior semantic understanding to provide fine-grained guidance and capture complex interactions between multiple subjects. Considering that standard diffusion loss often fails in aligning the critical concepts like subject ID, we employ an online reinforcement learning phase to drive the overall training objective of ID-Composer into RLVR. Extensive experiments demonstrate that our model surpasses existing methods in identity preservation, temporal consistency, and video quality.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00523v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00523v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00523v1">http://arxiv.org/abs/2511.00523v1</a><br>Vision language models such as CLIP have shown remarkable performance in zero shot classification, but remain susceptible to spurious correlations, where irrelevant visual features influence predictions. Existing debiasing methods often require access to training data and explicit group labels to perform fine-tuning or adjust embeddings, which limits their practicality in real-world settings. Test-time methods attempt to avoid this constraint, but many still depend on prior knowledge of dataset specific biases, limiting their generalizability in open set settings. In this work, we propose a test-time debiasing method for ViT based CLIP models that requires no additional training or assumptions of bias annotations. Our approach uses a pretrained segmentation model to isolate the target visual attribute, then adjusts the non target regions so that their embeddings are uniformly similar to all class specific text prompts. This procedure removes unintended bias signals from confounding visual regions while preserving the target attribute. Experiments on Waterbirds and CelebA show that our method outperforms existing test-time debiasing approaches in both group robustness metrics and Attention IoU. These results demonstrate the effectiveness of segmentation guided interventions for scalable and annotation free bias mitigation in vision language models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text-guided Fine-Grained Video Anomaly Detection</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00524v2/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00524v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00524v2">http://arxiv.org/abs/2511.00524v2</a><br>Video Anomaly Detection (VAD) aims to identify anomalous events within video segments. In scenarios such as surveillance or industrial process monitoring, anomaly detection is of critical importance. While existing approaches are semi-automated, requiring human assessment for anomaly detection, traditional VADs offer limited output as either normal or anomalous. We propose Text-guided Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD) that performs pixel-wise visual-textual feature alignment to generate fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly Encoder (RAE) that transforms the heatmaps into learnable textual embeddings, guiding the LVLM to accurately identify and localize anomalous events in videos. This significantly enhances both the granularity and interactivity of anomaly detection. The proposed method achieving SOTA performance by demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and 67.8%&#x2F;76.7% accuracy in anomaly heatmaps (RBDC&#x2F;TBDC) on the UBnormal dataset, and subjectively verified more preferable textual description on the ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories; Yes&#x2F;No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for targets, 78.10 for trajectories; Yes&#x2F;No accuracy: 89.73%).</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIFO: Learning and Synthesizing Multi-Instance from One Image</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00542v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00542v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00542v1">http://arxiv.org/abs/2511.00542v1</a><br>This paper proposes a method for precise learning and synthesizing multi-instance semantics from a single image. The difficulty of this problem lies in the limited training data, and it becomes even more challenging when the instances to be learned have similar semantics or appearance. To address this, we propose a penalty-based attention optimization to disentangle similar semantics during the learning stage. Then, in the synthesis, we introduce and optimize box control in attention layers to further mitigate semantic leakage while precisely controlling the output layout. Experimental results demonstrate that our method achieves disentangled and high-quality semantic learning and synthesis, strikingly balancing editability and instance consistency. Our method remains robust when dealing with semantically or visually similar instances or rare-seen objects. The code is publicly available at <a href="https://github.com/Kareneveve/MIFO">https://github.com/Kareneveve/MIFO</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00540v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00540v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00540v1">http://arxiv.org/abs/2511.00540v1</a><br>Industrial Anomaly Detection (IAD) is critical for enhancing operational safety, ensuring product quality, and optimizing manufacturing efficiency across global industries. However, the IAD algorithms are severely constrained by the limitations of existing public benchmarks. Current datasets exhibit restricted category diversity and insufficient scale, frequently resulting in metric saturation and limited model transferability to real-world scenarios. To address this gap, we introduce Real-IAD Variety, the largest and most diverse IAD benchmark, comprising 198,960 high-resolution images across 160 distinct object categories. Its diversity is ensured through comprehensive coverage of 28 industries, 24 material types, and 22 color variations. Our comprehensive experimental analysis validates the benchmark’s substantial challenge: state-of-the-art multi-class unsupervised anomaly detection methods experience significant performance degradation when scaled from 30 to 160 categories. Crucially, we demonstrate that vision-language models exhibit remarkable robustness to category scale-up, with minimal performance variation across different category counts, significantly enhancing generalization capabilities in diverse industrial contexts. The unprecedented scale and complexity of Real-IAD Variety position it as an essential resource for training and evaluating next-generation foundation models for anomaly detection. By providing this comprehensive benchmark with rigorous evaluation protocols across multi-class unsupervised, multi-view, and zero-&#x2F;few-shot settings, we aim to accelerate research beyond domain-specific constraints, enabling the development of scalable, general-purpose anomaly detection systems. Real-IAD Variety will be made publicly available to facilitate innovation in this critical field.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00543v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00543v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00543v1">http://arxiv.org/abs/2511.00543v1</a><br>Recent advances in generative modeling enable neural networks to generate weights without relying on gradient-based optimization. However, current methods are limited by issues of over-coupling and long-horizon. The former tightly binds weight generation with task-specific objectives, thereby limiting the flexibility of the learned optimizer. The latter leads to inefficiency and low accuracy during inference, caused by the lack of local constraints. In this paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that enhances flexibility through learning various optimization policies. It adopts a hybrid-policy sub-trajectory balance objective, which integrates on-policy and off-policy learning to capture local optimization policies. Theoretically, we demonstrate that learning solely local optimization policies can address the long-horizon issue while enhancing the generation of global optimal weights. In addition, we validate Lo-Hp’s superior accuracy and inference efficiency in tasks that require frequent weight updates, such as transfer learning, few-shot learning, domain generalization, and large language model adaptation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>stat.ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00573v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00573v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00573v1">http://arxiv.org/abs/2511.00573v1</a><br>Generalized Category Discovery (GCD) aims to leverage labeled samples from known categories to cluster unlabeled data that may include both known and unknown categories. While existing methods have achieved impressive results under standard conditions, their performance often deteriorates in the presence of distribution shifts. In this paper, we explore a more realistic task: Domain-Shifted Generalized Category Discovery (DS_GCD), where the unlabeled data includes not only unknown categories but also samples from unknown domains. To tackle this challenge, we propose a \textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE) that enhances the model’s ability to discover categories under distributional shift by leveraging frequency-domain information. Specifically, we first propose a frequency-based domain separation strategy that partitions samples into known and unknown domains by measuring their amplitude differences. We then propose two types of frequency-domain perturbation strategies: a cross-domain strategy, which adapts to new distributions by exchanging amplitude components across domains, and an intra-domain strategy, which enhances robustness to intra-domain variations within the unknown domain. Furthermore, we extend the self-supervised contrastive objective and semantic clustering loss to better guide the training process. Finally, we introduce a clustering-difficulty-aware resampling technique to adaptively focus on harder-to-cluster categories, further enhancing model performance. Extensive experiments demonstrate that our method effectively mitigates the impact of distributional shifts across various benchmark datasets and achieves superior performance in discovering both known and unknown categories.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image-based ground distance detection for crop-residue-covered soil</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00548v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00548v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00548v1">http://arxiv.org/abs/2511.00548v1</a><br>Conservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SY</tag>
      
      <tag>eess.IV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00560v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00560v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00560v1">http://arxiv.org/abs/2511.00560v1</a><br>Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis, extending it to dynamic scenes still results in substantial memory overhead from replicating Gaussians across frames. To address this challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines voxel-based representations with neural Gaussian splatting for efficient dynamic scene modeling. Instead of generating separate Gaussian sets per timestamp, our method employs a compact set of neural voxels with learned deformation fields to model temporal dynamics. The design greatly reduces memory consumption and accelerates training while preserving high image quality. We further introduce a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization, maintaining global efficiency while enhancing rendering quality for difficult viewing angles. Experiments demonstrate that our method outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00580v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00580v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00580v1">http://arxiv.org/abs/2511.00580v1</a><br>Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4% AUC on UCF-Crime and 83.67% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>68T07, 68T45, 68U10</tag>
      
      <tag>I.2.10; I.5.4; I.4.8; C.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00598v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00598v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00598v1">http://arxiv.org/abs/2511.00598v1</a><br>Registration of optical and synthetic aperture radar (SAR) remote sensing images serves as a critical foundation for image fusion and visual navigation tasks. This task is particularly challenging because of their modal discrepancy, primarily manifested as severe nonlinear radiometric differences (NRD), geometric distortions, and noise variations. Under large geometric transformations, existing classical template-based and sparse keypoint-based strategies struggle to achieve reliable registration results for optical-SAR image pairs. To address these limitations, we propose GDROS, a geometry-guided dense registration framework leveraging global cross-modal image interactions. First, we extract cross-modal deep features from optical and SAR images through a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D correlation volume is constructed and iteratively refined to establish pixel-wise dense correspondences. Subsequently, we implement a least squares regression (LSR) module to geometrically constrain the predicted dense optical flow field. Such geometry guidance mitigates prediction divergence by directly imposing an estimated affine transformation on the final flow predictions. Extensive experiments have been conducted on three representative datasets WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial resolutions, demonstrating robust performance of our proposed method across different imaging resolutions. Qualitative and quantitative results show that GDROS significantly outperforms current state-of-the-art methods in all metrics. Our source code will be released at: <a href="https://github.com/Zi-Xuan-Sun/GDROS">https://github.com/Zi-Xuan-Sun/GDROS</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00613v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00613v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00613v1">http://arxiv.org/abs/2511.00613v1</a><br>How far are deep models from real-world video anomaly understanding (VAU)? Current works typically emphasize on detecting unexpected occurrences deviated from normal patterns or comprehending anomalous events with interpretable descriptions. However, they exhibit only a superficial comprehension of real-world anomalies, with limited breadth in complex principles and subtle context that distinguish the anomalies from normalities, e.g., climbing cliffs with safety gear vs. without it. To this end, we introduce CueBench, the first of its kind Benchmark, devoted to Context-aware video anomalies within a Unified Evaluation framework. We comprehensively establish an event-centric hierarchical taxonomy that anchors two core event types: 14 conditional and 18 absolute anomaly events, defined by their refined semantics from diverse contexts across 174 scenes and 198 attributes. Based on this, we propose to unify and benchmark context-aware VAU with various challenging tasks across recognition, temporal grounding, detection, and anticipation. This also serves as a rigorous and fair probing evaluation suite for generative-discriminative as well as generalized-specialized vision-language models (VLMs). To address the challenges underlying CueBench, we further develop Cue-R1 based on R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative manner. Extensive results on CueBench reveal that, existing VLMs are still far from satisfactory real-world anomaly understanding, while our Cue-R1 surpasses these state-of-the-art approaches by over 24% on average.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00643v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00643v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00643v1">http://arxiv.org/abs/2511.00643v1</a><br>Understanding surgical instrument-tissue interactions requires not only identifying which instrument performs which action on which anatomical target, but also grounding these interactions spatially within the surgical scene. Existing surgical action triplet recognition methods are limited to learning from frame-level classification, failing to reliably link actions to specific instrument instances.Previous attempts at spatial grounding have primarily relied on class activation maps, which lack the precision and robustness required for detailed instrument-tissue interaction analysis.To address this gap, we propose grounding surgical action triplets with instrument instance segmentation, or triplet segmentation for short, a new unified task which produces spatially grounded &lt;instrument, verb, target&gt; outputs.We start by presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000 annotated frames, linking instrument instance masks with action verb and anatomical target annotations, and establishing the first benchmark for strongly supervised, instance-level triplet grounding and evaluation.To learn triplet segmentation, we propose TargetFusionNet, a novel architecture that extends Mask2Former with a target-aware fusion mechanism to address the challenge of accurate anatomical target prediction by fusing weak anatomy priors with instrument instance queries.Evaluated across recognition, detection, and triplet segmentation metrics, TargetFusionNet consistently improves performance over existing baselines, demonstrating that strong instance supervision combined with weak target priors significantly enhances the accuracy and robustness of surgical action understanding.Triplet segmentation establishes a unified framework for spatially grounding surgical action triplets. The proposed benchmark and architecture pave the way for more interpretable, surgical scene understanding.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00652v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00652v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00652v1">http://arxiv.org/abs/2511.00652v1</a><br>An autonomous vehicle can generate several terabytes of sensor data per day. A significant portion of this data consists of 3D point clouds produced by depth sensors such as LiDARs. This data must be transferred to cloud storage, where it is utilized for training machine learning models or conducting analyses, such as forensic investigations in the event of an accident. To reduce network and storage costs, this paper introduces DejaView. Although prior work uses interframe redundancies to compress data, DejaView searches for and uses redundancies on larger temporal scales (days and months) for more effective compression. We designed DejaView with the insight that the operating area of autonomous vehicles is limited and that vehicles mostly traverse the same routes daily. Consequently, the 3D data they collect daily is likely similar to the data they have captured in the past. To capture this, the core of DejaView is a diff operation that compactly represents point clouds as delta w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end implementation of DejaView can compress point clouds by a factor of 210 at a reconstruction error of only 15 cm.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00681v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00681v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00681v1">http://arxiv.org/abs/2511.00681v1</a><br>Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00653v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00653v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00653v1">http://arxiv.org/abs/2511.00653v1</a><br>Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for applications such as forest inventory, carbon monitoring and biodiversity assessment. Traditionally, ITS has been achieved with unsupervised geometry-based algorithms, while more recent advances have shifted toward supervised deep learning (DL). In the past, progress in method development was hindered by the lack of large-scale benchmark datasets, and the availability of novel data formats, particularly multispectral (MS) LiDAR, remains limited to this day, despite evidence that MS reflectance can improve the accuracy of ITS. This study introduces FGI-EMIT, the first large-scale MS airborne laser scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550 nm, the dataset consists of 1,561 manually annotated trees, with a particular focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked four conventional unsupervised algorithms and four supervised DL approaches. Hyperparameters of unsupervised methods were optimized using a Bayesian approach, while DL models were trained from scratch. Among the unsupervised methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL approaches performed significantly better overall, with the best model, ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9 percentage points. An ablation study demonstrated that current DL-based approaches generally fail to leverage MS reflectance information when it is provided as additional input features, although single channel reflectance can improve accuracy marginally, especially for understory trees. A performance analysis across point densities further showed that DL methods consistently remain superior to unsupervised algorithms, even at densities as low as 10 points&#x2F;m$^2$.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Outlier-Aware Post-Training Quantization for Image Super-Resolution</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00682v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00682v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00682v1">http://arxiv.org/abs/2511.00682v1</a><br>Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75 speedup.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evolve to Inspire: Novelty Search for Diverse Image Generation</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00686v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00686v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00686v1">http://arxiv.org/abs/2511.00686v1</a><br>Text-to-image diffusion models, while proficient at generating high-fidelity images, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00698v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00698v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00698v1">http://arxiv.org/abs/2511.00698v1</a><br>Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to mitigate high exposure side effects, but often suffers from noise and artifacts that affect diagnostic accuracy. To tackle this issue, deep learning models have been developed to enhance LDCT images. Various loss functions have been employed, including classical approaches such as Mean Square Error and adversarial losses, as well as customized loss functions(LFs) designed for specific architectures. Although these models achieve remarkable performance in terms of PSNR and SSIM, these metrics are limited in their ability to reflect perceptual quality, especially for medical images. In this paper, we focus on one of the most critical elements of DL-based architectures, namely the loss function. We conduct an objective analysis of the relevance of different loss functions for LDCT image quality enhancement and their consistency with image quality metrics. Our findings reveal inconsistencies between LFs and quality metrics, and highlight the need of consideration of image quality metrics when developing a new loss function for image quality enhancement.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00702v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00702v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00702v1">http://arxiv.org/abs/2511.00702v1</a><br>Doctors and researchers routinely use diffusion tensor imaging (DTI) and tractography to visualize the fibrous structure of tissues in the human body. This paper explores the connection of these techniques to the painterly rendering of images. Using a tractography algorithm the presented method can place brush strokes that mimic the painting process of human artists, analogously to how fibres are tracked in DTI. The analogue to the diffusion tensor for image orientation is the structural tensor, which can provide better local orientation information than the gradient alone. I demonstrate this technique in portraits and general images, and discuss the parallels between fibre tracking and brush stroke placement, and frame it in the language of tractography. This work presents an exploratory investigation into the cross-domain application of diffusion tensor imaging techniques to painterly rendering of images. All the code is available at <a href="https://github.com/tito21/st-python">https://github.com/tito21/st-python</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Validating Deep Models for Alzheimer&#39;s 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00728v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00728v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00728v1">http://arxiv.org/abs/2511.00728v1</a><br>Deep learning models have shown strong performance in diagnosing Alzheimer’s disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with training datasets largely composed of North American cohorts such as those in the Alzheimer’s Disease Neuroimaging Initiative (ADNI). However, their generalization to underrepresented populations remains underexplored. In this study, we benchmark convolutional and Transformer-based models on the ADNI dataset and assess their generalization performance on a novel Latin American clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show that while all models achieve high AUCs on ADNI (up to .96, .97), their performance drops substantially on FLENI (down to .82, .80, respectively), revealing a significant domain shift. The tested architectures demonstrated similar performance, calling into question the supposed advantages of transformers for this specific task. Through ablation studies, we identify per-image normalization and a correct sampling selection as key factors for generalization. Occlusion sensitivity analysis further reveals that models trained on ADNI, generally attend to canonical hypometabolic regions for the AD class, but focus becomes unclear for the other classes and for FLENI scans. These findings highlight the need for population-aware validation of diagnostic AI models and motivate future work on domain adaptation and cohort diversification.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards classification-based representation learning for place recognition on LiDAR scans</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_00738v2/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_00738v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00738v2">http://arxiv.org/abs/2511.00738v2</a><br>Place recognition is a crucial task in autonomous driving, allowing vehicles to determine their position using sensor data. While most existing methods rely on contrastive learning, we explore an alternative approach by framing place recognition as a multi-class classification problem. Our method assigns discrete location labels to LiDAR scans and trains an encoder-decoder model to classify each scan’s position directly. We evaluate this approach on the NuScenes dataset and show that it achieves competitive performance compared to contrastive learning-based methods while offering advantages in training efficiency and stability.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iFlyBot-VLA Technical Report</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_01914v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_01914v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01914v1">http://arxiv.org/abs/2511.01914v1</a><br>We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_01915v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_01915v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.01915v1">http://arxiv.org/abs/2511.01915v1</a><br>Purpose: This study provides the first comprehensive evaluation of foundation models in fetal ultrasound (US) imaging under low inter-class variability conditions. While recent vision foundation models such as DINOv3 have shown remarkable transferability across medical domains, their ability to discriminate anatomically similar structures has not been systematically investigated. We address this gap by focusing on fetal brain standard planes–transthalamic (TT), transventricular (TV), and transcerebellar (TC)–which exhibit highly overlapping anatomical features and pose a critical challenge for reliable biometric assessment.   Methods: To ensure a fair and reproducible evaluation, all publicly available fetal ultrasound datasets were curated and aggregated into a unified multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated images from heterogeneous acquisition settings. DINOv3 was pretrained in a self-supervised manner to learn ultrasound-aware representations. The learned features were then evaluated through standardized adaptation protocols, including linear probing with frozen backbone and full fine-tuning, under two initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization from natural-image DINOv3 weights.   Results: Models pretrained on fetal ultrasound data consistently outperformed those initialized on natural images, with weighted F1-score improvements of up to 20 percent. Domain-adaptive pretraining enabled the network to preserve subtle echogenic and structural cues crucial for distinguishing intermediate planes such as TV.   Conclusion: Results demonstrate that generic foundation models fail to generalize under low inter-class variability, whereas domain-specific pretraining is essential to achieve robust and clinically reliable representations in fetal brain ultrasound imaging.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding</title>
    <link href="/2025/11/01/highlights/2025-11-01-2511_04699v1/"/>
    <url>/2025/11/01/highlights/2025-11-01-2511_04699v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.04699v1">http://arxiv.org/abs/2511.04699v1</a><br>Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset comprises over 2.5 million of samples, including 1.5 million textual data, 270K fully annotated tables, and hundred thousands of real data based charts. Our pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents. In addition to text, the corpus includes variety of rendered styles for charts and tables. Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities. SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00114v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00114v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00114v1">http://arxiv.org/abs/2511.00114v1</a><br>Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00120v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00120v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00120v1">http://arxiv.org/abs/2511.00120v1</a><br>The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00141v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00141v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00141v1">http://arxiv.org/abs/2511.00141v1</a><br>Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Retrospect to Multi-prompt Learning across Vision and Language</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00191v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00191v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00191v1">http://arxiv.org/abs/2511.00191v1</a><br>The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00211v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00211v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00211v1">http://arxiv.org/abs/2511.00211v1</a><br>The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet’s essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00218v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00218v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00218v1">http://arxiv.org/abs/2511.00218v1</a><br>Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM’s simultaneous capture of complementary illumination and phase cues for robust cell segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00269v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00269v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00269v1">http://arxiv.org/abs/2511.00269v1</a><br>Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
    <link href="/2025/10/31/cs.AI/2025-10-31-2511_00270v1/"/>
    <url>/2025/10/31/cs.AI/2025-10-31-2511_00270v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00270v1">http://arxiv.org/abs/2511.00270v1</a><br>Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection</title>
    <link href="/2025/10/31/cs.CR/2025-10-31-2511_00181v1/"/>
    <url>/2025/10/31/cs.CR/2025-10-31-2511_00181v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00181v1">http://arxiv.org/abs/2511.00181v1</a><br>The rapid evolution of AI-generated images poses unprecedented challenges to information integrity and media authenticity. Existing detection approaches suffer from fundamental limitations: traditional classifiers lack interpretability and fail to generalize across evolving generative models, while vision-language models (VLMs), despite their promise, remain constrained to single-shot analysis and pixel-level reasoning. To address these challenges, we introduce AIFo (Agent-based Image Forensics), a novel training-free framework that emulates human forensic investigation through multi-agent collaboration. Unlike conventional methods, our framework employs a set of forensic tools, including reverse image search, metadata extraction, pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based agents that collect, synthesize, and reason over cross-source evidence. When evidence is conflicting or insufficient, a structured multi-agent debate mechanism allows agents to exchange arguments and reach a reliable conclusion. Furthermore, we enhance the framework with a memory-augmented reasoning module that learns from historical cases to improve future detection accuracy. Our comprehensive evaluation spans 6,000 images across both controlled laboratory settings and challenging real-world scenarios, including images from modern generative platforms and diverse online sources. AIFo achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. These results demonstrate that agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
    <link href="/2025/10/31/cs.CL/2025-10-31-2511_00270v1/"/>
    <url>/2025/10/31/cs.CL/2025-10-31-2511_00270v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00270v1">http://arxiv.org/abs/2511.00270v1</a><br>Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object-Aware 4D Human Motion Generation</title>
    <link href="/2025/10/31/cs.GR/2025-10-31-2511_00248v1/"/>
    <url>/2025/10/31/cs.GR/2025-10-31-2511_00248v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00248v1">http://arxiv.org/abs/2511.00248v1</a><br>Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.GR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spot The Ball: A Benchmark for Visual Social Inference</title>
    <link href="/2025/10/31/cs.HC/2025-10-31-2511_00261v1/"/>
    <url>/2025/10/31/cs.HC/2025-10-31-2511_00261v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00261v1">http://arxiv.org/abs/2511.00261v1</a><br>Humans excel at visual social inference, the ability to infer hidden elements of a scene from subtle behavioral cues such as other people’s gaze, pose, and orientation. This ability drives everyday social reasoning in humans and is critical for developing more human-like AI agents. We introduce Spot The Ball, a challenging benchmark for evaluating visual social inference in vision-language models (VLMs) using sports as a test domain. The task is to localize a removed sports ball from soccer, basketball, and volleyball images. We present a curated evaluation set with human baselines and a scalable pipeline for generating additional test items. We evaluate four state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting strategies, finding that humans are consistently two to three times more accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show that models rely on superficial spatial heuristics–such as guessing near the image center or nearby players–while humans leverage social cues like gaze direction and body pose. These findings reveal a persistent human-model gap in visual social reasoning and underscore the need for architectures that explicitly encode structured behavioral cues to achieve robust, human-like inference.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</title>
    <link href="/2025/10/31/cs.LG/2025-10-31-2511_00114v1/"/>
    <url>/2025/10/31/cs.LG/2025-10-31-2511_00114v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00114v1">http://arxiv.org/abs/2511.00114v1</a><br>Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Retrospect to Multi-prompt Learning across Vision and Language</title>
    <link href="/2025/10/31/cs.LG/2025-10-31-2511_00191v1/"/>
    <url>/2025/10/31/cs.LG/2025-10-31-2511_00191v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00191v1">http://arxiv.org/abs/2511.00191v1</a><br>The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation</title>
    <link href="/2025/10/31/cs.LG/2025-10-31-2511_00123v1/"/>
    <url>/2025/10/31/cs.LG/2025-10-31-2511_00123v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00123v1">http://arxiv.org/abs/2511.00123v1</a><br>Age estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs localized feature extraction capabilities and the Transformers global attention mechanisms. Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture. Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks. This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Melanoma Classification Through Deep Ensemble Learning and Explainable AI</title>
    <link href="/2025/10/31/cs.LG/2025-10-31-2511_00246v1/"/>
    <url>/2025/10/31/cs.LG/2025-10-31-2511_00246v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00246v1">http://arxiv.org/abs/2511.00246v1</a><br>Melanoma is one of the most aggressive and deadliest skin cancers, leading to mortality if not detected and treated in the early stages. Artificial intelligence techniques have recently been developed to help dermatologists in the early detection of melanoma, and systems based on deep learning (DL) have been able to detect these lesions with high accuracy. However, the entire community must overcome the explainability limit to get the maximum benefit from DL for diagnostics in the healthcare domain. Because of the black box operation’s shortcomings in DL models’ decisions, there is a lack of reliability and trust in the outcomes. However, Explainable Artificial Intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This paper proposes a machine learning model using ensemble learning of three state-of-the-art deep transfer Learning networks, along with an approach to ensure the reliability of the predictions by utilizing XAI techniques to explain the basis of the predictions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
    <link href="/2025/10/31/cs.LG/2025-10-31-2511_00270v1/"/>
    <url>/2025/10/31/cs.LG/2025-10-31-2511_00270v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00270v1">http://arxiv.org/abs/2511.00270v1</a><br>Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Step Toward World Models: A Survey on Robotic Manipulation</title>
    <link href="/2025/10/31/cs.RO/2025-10-31-2511_02097v1/"/>
    <url>/2025/10/31/cs.RO/2025-10-31-2511_02097v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02097v1">http://arxiv.org/abs/2511.02097v1</a><br>Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond purely reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and enable prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, rather than directly imposing a fixed definition and limiting our scope to methods explicitly labeled as world models, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a real world model should possess. Building on this analysis, we aim to outline a roadmap for developing generalizable and practical world models for robotics.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00114v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00114v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00114v1">http://arxiv.org/abs/2511.00114v1</a><br>Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00120v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00120v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00120v1">http://arxiv.org/abs/2511.00120v1</a><br>The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00123v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00123v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00123v1">http://arxiv.org/abs/2511.00123v1</a><br>Age estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs localized feature extraction capabilities and the Transformers global attention mechanisms. Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture. Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks. This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GeneFlow: Translation of Single-cell Gene Expression to Histopathological Images via Rectified Flow</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00119v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00119v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00119v1">http://arxiv.org/abs/2511.00119v1</a><br>Spatial transcriptomics (ST) technologies can be used to align transcriptomes with histopathological morphology, presenting exciting new opportunities for biomolecular discovery. Using ST data, we construct a novel framework, GeneFlow, to map transcriptomics onto paired cellular images. By combining an attention-based RNA encoder with a conditional UNet guided by rectified flow, we generate high-resolution images with different staining methods (e.g. H&amp;E, DAPI) to highlight various cellular&#x2F;tissue structures. Rectified flow with high-order ODE solvers creates a continuous, bijective mapping between transcriptomics and image manifolds, addressing the many-to-one relationship inherent in this problem. Our method enables the generation of realistic cellular morphology features and spatially resolved intercellular interactions from observational gene expression profiles, provides potential to incorporate genetic&#x2F;chemical perturbations, and enables disease diagnosis by revealing dysregulated patterns in imaging phenotypes. Our rectified flow-based method outperforms diffusion-based baseline method in all experiments. Code can be found at <a href="https://github.com/wangmengbo/GeneFlow">https://github.com/wangmengbo/GeneFlow</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00141v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00141v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00141v1">http://arxiv.org/abs/2511.00141v1</a><br>Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00143v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00143v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00143v1">http://arxiv.org/abs/2511.00143v1</a><br>Recent advances in text-to-image models have increased the exposure of powerful image editing techniques as a tool, raising concerns about their potential for malicious use. An emerging line of research to address such threats focuses on implanting “protective” adversarial noise into images before their public release, so future attempts to edit them using text-to-image models can be impeded. However, subsequent works have shown that these adversarial noises are often easily “reversed,” e.g., with techniques as simple as JPEG compression, casting doubt on the practicality of the approach. In this paper, we argue that adversarial noise for image protection should not only be imperceptible, as has been a primary focus of prior work, but also irreversible, viz., it should be difficult to detect as noise provided that the original image is hidden. We propose a surprisingly simple method to enhance the robustness of image protection methods against noise reversal techniques. Specifically, it applies an adaptive per-region Gaussian blur on the noise to adjust the overall frequency spectrum. Through extensive experiments, we show that our method consistently improves the per-sample worst-case protection performance of existing methods against a wide range of reversal techniques on diverse image editing scenarios, while also reducing quality degradation due to noise in terms of perceptual metrics. Code is available at <a href="https://github.com/jsu-kim/BlurGuard">https://github.com/jsu-kim/BlurGuard</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Retrospect to Multi-prompt Learning across Vision and Language</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00191v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00191v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00191v1">http://arxiv.org/abs/2511.00191v1</a><br>The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CompAgent: An Agentic Framework for Visual Compliance Verification</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00171v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00171v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00171v1">http://arxiv.org/abs/2511.00171v1</a><br>Visual compliance verification is a critical yet underexplored problem in computer vision, especially in domains such as media, entertainment, and advertising where content must adhere to complex and evolving policy rules. Existing methods often rely on task-specific deep learning models trained on manually labeled datasets, which are costly to build and limited in generalizability. While recent multi-modal large language models (MLLMs) offer broad real-world knowledge and policy understanding, they struggle to reason over fine-grained visual details and apply structured compliance rules effectively on their own. In this paper, we propose CompAgent, the first agentic framework for visual compliance verification. CompAgent augments MLLMs with a suite of visual tools - such as object detectors, face analyzers, NSFW detectors, and captioning models - and introduces a planning agent that dynamically selects appropriate tools based on the compliance policy. A verification agent then integrates image, tool outputs, and policy context to perform multi-modal reasoning. Experiments on public benchmarks show that CompAgent outperforms specialized classifiers, direct MLLM prompting, and curated routing baselines, achieving up to 76% F1 score and a 10% improvement over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate the effectiveness of agentic planning and tool-augmented reasoning for scalable, accurate, and adaptable visual compliance verification.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00181v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00181v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00181v1">http://arxiv.org/abs/2511.00181v1</a><br>The rapid evolution of AI-generated images poses unprecedented challenges to information integrity and media authenticity. Existing detection approaches suffer from fundamental limitations: traditional classifiers lack interpretability and fail to generalize across evolving generative models, while vision-language models (VLMs), despite their promise, remain constrained to single-shot analysis and pixel-level reasoning. To address these challenges, we introduce AIFo (Agent-based Image Forensics), a novel training-free framework that emulates human forensic investigation through multi-agent collaboration. Unlike conventional methods, our framework employs a set of forensic tools, including reverse image search, metadata extraction, pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based agents that collect, synthesize, and reason over cross-source evidence. When evidence is conflicting or insufficient, a structured multi-agent debate mechanism allows agents to exchange arguments and reach a reliable conclusion. Furthermore, we enhance the framework with a memory-augmented reasoning module that learns from historical cases to improve future detection accuracy. Our comprehensive evaluation spans 6,000 images across both controlled laboratory settings and challenging real-world scenarios, including images from modern generative platforms and diverse online sources. AIFo achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. These results demonstrate that agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00211v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00211v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00211v1">http://arxiv.org/abs/2511.00211v1</a><br>The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet’s essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00218v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00218v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00218v1">http://arxiv.org/abs/2511.00218v1</a><br>Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM’s simultaneous capture of complementary illumination and phase cues for robust cell segmentation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00231v2/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00231v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00231v2">http://arxiv.org/abs/2511.00231v2</a><br>Petascale electron microscopy (EM) datasets push storage, transfer, and downstream analysis toward their current limits. We present a vector-quantized variational autoencoder-based (VQ-VAE) compression framework for EM that spans 16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme compression, with an optional Transformer prior that predicts bottom tokens (without changing the compression ratio) to restore texture via feature-wise linear modulation (FiLM) and concatenation; we further introduce an ROI-driven workflow that performs selective high-resolution reconstruction from 1024x-compressed latents only where needed.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hyperbolic Optimal Transport</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00244v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00244v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00244v1">http://arxiv.org/abs/2511.00244v1</a><br>The optimal transport (OT) problem aims to find the most efficient mapping between two probability distributions under a given cost function, and has diverse applications in many fields such as machine learning, computer vision and computer graphics. However, existing methods for computing optimal transport maps are primarily developed for Euclidean spaces and the sphere. In this paper, we explore the problem of computing the optimal transport map in hyperbolic space, which naturally arises in contexts involving hierarchical data, networks, and multi-genus Riemann surfaces. We propose a novel and efficient algorithm for computing the optimal transport map in hyperbolic space using a geometric variational technique by extending methods for Euclidean and spherical geometry to the hyperbolic setting. We also perform experiments on synthetic data and multi-genus surface models to validate the efficacy of the proposed method.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object-Aware 4D Human Motion Generation</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00248v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00248v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00248v1">http://arxiv.org/abs/2511.00248v1</a><br>Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Merlin L48 Spectrogram Dataset</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00252v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00252v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00252v1">http://arxiv.org/abs/2511.00252v1</a><br>In the single-positive multi-label (SPML) setting, each image in a dataset is labeled with the presence of a single class, while the true presence of other classes remains unknown. The challenge is to narrow the performance gap between this partially-labeled setting and fully-supervised learning, which often requires a significant annotation budget. Prior SPML methods were developed and benchmarked on synthetic datasets created by randomly sampling single positive labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and CUB200. However, this synthetic approach does not reflect real-world scenarios and fails to capture the fine-grained complexities that can lead to difficult misclassifications. In this work, we introduce the L48 dataset, a fine-grained, real-world multi-label dataset derived from recordings of bird sounds. L48 provides a natural SPML setting with single-positive annotations on a challenging, fine-grained domain, as well as two extended settings in which domain priors give access to additional negative labels. We benchmark existing SPML methods on L48 and observe significant performance differences compared to synthetic datasets and analyze method weaknesses, underscoring the need for more realistic and difficult benchmarks.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00255v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00255v1">http://arxiv.org/abs/2511.00255v1</a><br>In entomology and ecology research, biologists often need to collect a large number of insects, among which beetles are the most common species. A common practice for biologists to organize beetles is to place them on trays and take a picture of each tray. Given the images of thousands of such trays, it is important to have an automated pipeline to process the large-scale data for further research. Therefore, we develop a 3-stage pipeline to detect all the beetles on each tray, sort and crop the image of each beetle, and do morphological segmentation on the cropped beetles. For detection, we design an iterative process utilizing a transformer-based open-vocabulary object detector and a vision-language model. For segmentation, we manually labeled 670 beetle images and fine-tuned two variants of a transformer-based segmentation model to achieve fine-grained segmentation of beetles with relatively high accuracy. The pipeline integrates multiple deep learning methods and is specialized for beetle image processing, which can greatly improve the efficiency to process large-scale beetle data and accelerate biological research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00260v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00260v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00260v1">http://arxiv.org/abs/2511.00260v1</a><br>Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T07 (Primary) 68T45, 92C55 (Secondary)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spot The Ball: A Benchmark for Visual Social Inference</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00261v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00261v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00261v1">http://arxiv.org/abs/2511.00261v1</a><br>Humans excel at visual social inference, the ability to infer hidden elements of a scene from subtle behavioral cues such as other people’s gaze, pose, and orientation. This ability drives everyday social reasoning in humans and is critical for developing more human-like AI agents. We introduce Spot The Ball, a challenging benchmark for evaluating visual social inference in vision-language models (VLMs) using sports as a test domain. The task is to localize a removed sports ball from soccer, basketball, and volleyball images. We present a curated evaluation set with human baselines and a scalable pipeline for generating additional test items. We evaluate four state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting strategies, finding that humans are consistently two to three times more accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show that models rely on superficial spatial heuristics–such as guessing near the image center or nearby players–while humans leverage social cues like gaze direction and body pose. These findings reveal a persistent human-model gap in visual social reasoning and underscore the need for architectures that explicitly encode structured behavioral cues to achieve robust, human-like inference.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00269v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00269v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00269v1">http://arxiv.org/abs/2511.00269v1</a><br>Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00270v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00270v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00270v1">http://arxiv.org/abs/2511.00270v1</a><br>Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-View Consistent Human Image Customization via In-Context Learning</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00293v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00293v1">http://arxiv.org/abs/2511.00293v1</a><br>Recent advances in personalized generative models demonstrate impressive results in creating identity-consistent images of the same person under diverse settings. Yet, we note that most methods cannot control the viewpoint of the generated image, nor generate consistent multiple views of the person. To address this problem, we propose a lightweight adaptation method, PersonalView, capable of enabling an existing model to acquire multi-view generation capability with as few as 100 training samples. PersonalView consists of two key components: First, we design a conditioning architecture to take advantage of the in-context learning ability of the pre-trained diffusion transformer. Second, we preserve the original generative ability of the pretrained model with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view consistency, text alignment, identity similarity, and visual quality of PersonalView and compare it to recent baselines with potential capability of multi-view customization. PersonalView significantly outperforms baselines trained on a large corpus of multi-view data with only 100 training samples.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Step Toward World Models: A Survey on Robotic Manipulation</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_02097v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_02097v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02097v1">http://arxiv.org/abs/2511.02097v1</a><br>Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond purely reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and enable prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, rather than directly imposing a fixed definition and limiting our scope to methods explicitly labeled as world models, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a real world model should possess. Building on this analysis, we aim to outline a roadmap for developing generalizable and practical world models for robotics.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Melanoma Classification Through Deep Ensemble Learning and Explainable AI</title>
    <link href="/2025/10/31/cs.CV/2025-10-31-2511_00246v1/"/>
    <url>/2025/10/31/cs.CV/2025-10-31-2511_00246v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00246v1">http://arxiv.org/abs/2511.00246v1</a><br>Melanoma is one of the most aggressive and deadliest skin cancers, leading to mortality if not detected and treated in the early stages. Artificial intelligence techniques have recently been developed to help dermatologists in the early detection of melanoma, and systems based on deep learning (DL) have been able to detect these lesions with high accuracy. However, the entire community must overcome the explainability limit to get the maximum benefit from DL for diagnostics in the healthcare domain. Because of the black box operation’s shortcomings in DL models’ decisions, there is a lack of reliability and trust in the outcomes. However, Explainable Artificial Intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This paper proposes a machine learning model using ensemble learning of three state-of-the-art deep transfer Learning networks, along with an approach to ensure the reliability of the predictions by utilizing XAI techniques to explain the basis of the predictions.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
    <link href="/2025/10/31/eess.IV/2025-10-31-2511_00211v1/"/>
    <url>/2025/10/31/eess.IV/2025-10-31-2511_00211v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00211v1">http://arxiv.org/abs/2511.00211v1</a><br>The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet’s essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00114v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00114v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00114v1">http://arxiv.org/abs/2511.00114v1</a><br>Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GeneFlow: Translation of Single-cell Gene Expression to Histopathological Images via Rectified Flow</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00119v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00119v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00119v1">http://arxiv.org/abs/2511.00119v1</a><br>Spatial transcriptomics (ST) technologies can be used to align transcriptomes with histopathological morphology, presenting exciting new opportunities for biomolecular discovery. Using ST data, we construct a novel framework, GeneFlow, to map transcriptomics onto paired cellular images. By combining an attention-based RNA encoder with a conditional UNet guided by rectified flow, we generate high-resolution images with different staining methods (e.g. H&amp;E, DAPI) to highlight various cellular&#x2F;tissue structures. Rectified flow with high-order ODE solvers creates a continuous, bijective mapping between transcriptomics and image manifolds, addressing the many-to-one relationship inherent in this problem. Our method enables the generation of realistic cellular morphology features and spatially resolved intercellular interactions from observational gene expression profiles, provides potential to incorporate genetic&#x2F;chemical perturbations, and enables disease diagnosis by revealing dysregulated patterns in imaging phenotypes. Our rectified flow-based method outperforms diffusion-based baseline method in all experiments. Code can be found at <a href="https://github.com/wangmengbo/GeneFlow">https://github.com/wangmengbo/GeneFlow</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>q-bio.QM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00123v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00123v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00123v1">http://arxiv.org/abs/2511.00123v1</a><br>Age estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs localized feature extraction capabilities and the Transformers global attention mechanisms. Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture. Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks. This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00120v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00120v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00120v1">http://arxiv.org/abs/2511.00120v1</a><br>The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00141v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00141v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00141v1">http://arxiv.org/abs/2511.00141v1</a><br>Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00143v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00143v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00143v1">http://arxiv.org/abs/2511.00143v1</a><br>Recent advances in text-to-image models have increased the exposure of powerful image editing techniques as a tool, raising concerns about their potential for malicious use. An emerging line of research to address such threats focuses on implanting “protective” adversarial noise into images before their public release, so future attempts to edit them using text-to-image models can be impeded. However, subsequent works have shown that these adversarial noises are often easily “reversed,” e.g., with techniques as simple as JPEG compression, casting doubt on the practicality of the approach. In this paper, we argue that adversarial noise for image protection should not only be imperceptible, as has been a primary focus of prior work, but also irreversible, viz., it should be difficult to detect as noise provided that the original image is hidden. We propose a surprisingly simple method to enhance the robustness of image protection methods against noise reversal techniques. Specifically, it applies an adaptive per-region Gaussian blur on the noise to adjust the overall frequency spectrum. Through extensive experiments, we show that our method consistently improves the per-sample worst-case protection performance of existing methods against a wide range of reversal techniques on diverse image editing scenarios, while also reducing quality degradation due to noise in terms of perceptual metrics. Code is available at <a href="https://github.com/jsu-kim/BlurGuard">https://github.com/jsu-kim/BlurGuard</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CompAgent: An Agentic Framework for Visual Compliance Verification</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00171v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00171v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00171v1">http://arxiv.org/abs/2511.00171v1</a><br>Visual compliance verification is a critical yet underexplored problem in computer vision, especially in domains such as media, entertainment, and advertising where content must adhere to complex and evolving policy rules. Existing methods often rely on task-specific deep learning models trained on manually labeled datasets, which are costly to build and limited in generalizability. While recent multi-modal large language models (MLLMs) offer broad real-world knowledge and policy understanding, they struggle to reason over fine-grained visual details and apply structured compliance rules effectively on their own. In this paper, we propose CompAgent, the first agentic framework for visual compliance verification. CompAgent augments MLLMs with a suite of visual tools - such as object detectors, face analyzers, NSFW detectors, and captioning models - and introduces a planning agent that dynamically selects appropriate tools based on the compliance policy. A verification agent then integrates image, tool outputs, and policy context to perform multi-modal reasoning. Experiments on public benchmarks show that CompAgent outperforms specialized classifiers, direct MLLM prompting, and curated routing baselines, achieving up to 76% F1 score and a 10% improvement over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate the effectiveness of agentic planning and tool-augmented reasoning for scalable, accurate, and adaptable visual compliance verification.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00181v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00181v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00181v1">http://arxiv.org/abs/2511.00181v1</a><br>The rapid evolution of AI-generated images poses unprecedented challenges to information integrity and media authenticity. Existing detection approaches suffer from fundamental limitations: traditional classifiers lack interpretability and fail to generalize across evolving generative models, while vision-language models (VLMs), despite their promise, remain constrained to single-shot analysis and pixel-level reasoning. To address these challenges, we introduce AIFo (Agent-based Image Forensics), a novel training-free framework that emulates human forensic investigation through multi-agent collaboration. Unlike conventional methods, our framework employs a set of forensic tools, including reverse image search, metadata extraction, pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based agents that collect, synthesize, and reason over cross-source evidence. When evidence is conflicting or insufficient, a structured multi-agent debate mechanism allows agents to exchange arguments and reach a reliable conclusion. Furthermore, we enhance the framework with a memory-augmented reasoning module that learns from historical cases to improve future detection accuracy. Our comprehensive evaluation spans 6,000 images across both controlled laboratory settings and challenging real-world scenarios, including images from modern generative platforms and diverse online sources. AIFo achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. These results demonstrate that agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.CR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Retrospect to Multi-prompt Learning across Vision and Language</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00191v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00191v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00191v1">http://arxiv.org/abs/2511.00191v1</a><br>The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00211v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00211v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00211v1">http://arxiv.org/abs/2511.00211v1</a><br>The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet’s essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00218v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00218v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00218v1">http://arxiv.org/abs/2511.00218v1</a><br>Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM’s simultaneous capture of complementary illumination and phase cues for robust cell segmentation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00231v2/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00231v2/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00231v2">http://arxiv.org/abs/2511.00231v2</a><br>Petascale electron microscopy (EM) datasets push storage, transfer, and downstream analysis toward their current limits. We present a vector-quantized variational autoencoder-based (VQ-VAE) compression framework for EM that spans 16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme compression, with an optional Transformer prior that predicts bottom tokens (without changing the compression ratio) to restore texture via feature-wise linear modulation (FiLM) and concatenation; we further introduce an ROI-driven workflow that performs selective high-resolution reconstruction from 1024x-compressed latents only where needed.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hyperbolic Optimal Transport</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00244v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00244v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00244v1">http://arxiv.org/abs/2511.00244v1</a><br>The optimal transport (OT) problem aims to find the most efficient mapping between two probability distributions under a given cost function, and has diverse applications in many fields such as machine learning, computer vision and computer graphics. However, existing methods for computing optimal transport maps are primarily developed for Euclidean spaces and the sphere. In this paper, we explore the problem of computing the optimal transport map in hyperbolic space, which naturally arises in contexts involving hierarchical data, networks, and multi-genus Riemann surfaces. We propose a novel and efficient algorithm for computing the optimal transport map in hyperbolic space using a geometric variational technique by extending methods for Euclidean and spherical geometry to the hyperbolic setting. We also perform experiments on synthetic data and multi-genus surface models to validate the efficacy of the proposed method.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Melanoma Classification Through Deep Ensemble Learning and Explainable AI</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00246v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00246v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00246v1">http://arxiv.org/abs/2511.00246v1</a><br>Melanoma is one of the most aggressive and deadliest skin cancers, leading to mortality if not detected and treated in the early stages. Artificial intelligence techniques have recently been developed to help dermatologists in the early detection of melanoma, and systems based on deep learning (DL) have been able to detect these lesions with high accuracy. However, the entire community must overcome the explainability limit to get the maximum benefit from DL for diagnostics in the healthcare domain. Because of the black box operation’s shortcomings in DL models’ decisions, there is a lack of reliability and trust in the outcomes. However, Explainable Artificial Intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This paper proposes a machine learning model using ensemble learning of three state-of-the-art deep transfer Learning networks, along with an approach to ensure the reliability of the predictions by utilizing XAI techniques to explain the basis of the predictions.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Object-Aware 4D Human Motion Generation</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00248v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00248v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00248v1">http://arxiv.org/abs/2511.00248v1</a><br>Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.GR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Merlin L48 Spectrogram Dataset</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00252v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00252v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00252v1">http://arxiv.org/abs/2511.00252v1</a><br>In the single-positive multi-label (SPML) setting, each image in a dataset is labeled with the presence of a single class, while the true presence of other classes remains unknown. The challenge is to narrow the performance gap between this partially-labeled setting and fully-supervised learning, which often requires a significant annotation budget. Prior SPML methods were developed and benchmarked on synthetic datasets created by randomly sampling single positive labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and CUB200. However, this synthetic approach does not reflect real-world scenarios and fails to capture the fine-grained complexities that can lead to difficult misclassifications. In this work, we introduce the L48 dataset, a fine-grained, real-world multi-label dataset derived from recordings of bird sounds. L48 provides a natural SPML setting with single-positive annotations on a challenging, fine-grained domain, as well as two extended settings in which domain priors give access to additional negative labels. We benchmark existing SPML methods on L48 and observe significant performance differences compared to synthetic datasets and analyze method weaknesses, underscoring the need for more realistic and difficult benchmarks.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00255v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00255v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00255v1">http://arxiv.org/abs/2511.00255v1</a><br>In entomology and ecology research, biologists often need to collect a large number of insects, among which beetles are the most common species. A common practice for biologists to organize beetles is to place them on trays and take a picture of each tray. Given the images of thousands of such trays, it is important to have an automated pipeline to process the large-scale data for further research. Therefore, we develop a 3-stage pipeline to detect all the beetles on each tray, sort and crop the image of each beetle, and do morphological segmentation on the cropped beetles. For detection, we design an iterative process utilizing a transformer-based open-vocabulary object detector and a vision-language model. For segmentation, we manually labeled 670 beetle images and fine-tuned two variants of a transformer-based segmentation model to achieve fine-grained segmentation of beetles with relatively high accuracy. The pipeline integrates multiple deep learning methods and is specialized for beetle image processing, which can greatly improve the efficiency to process large-scale beetle data and accelerate biological research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00260v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00260v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00260v1">http://arxiv.org/abs/2511.00260v1</a><br>Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68T07 (Primary) 68T45, 92C55 (Secondary)</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spot The Ball: A Benchmark for Visual Social Inference</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00261v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00261v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00261v1">http://arxiv.org/abs/2511.00261v1</a><br>Humans excel at visual social inference, the ability to infer hidden elements of a scene from subtle behavioral cues such as other people’s gaze, pose, and orientation. This ability drives everyday social reasoning in humans and is critical for developing more human-like AI agents. We introduce Spot The Ball, a challenging benchmark for evaluating visual social inference in vision-language models (VLMs) using sports as a test domain. The task is to localize a removed sports ball from soccer, basketball, and volleyball images. We present a curated evaluation set with human baselines and a scalable pipeline for generating additional test items. We evaluate four state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting strategies, finding that humans are consistently two to three times more accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show that models rely on superficial spatial heuristics–such as guessing near the image center or nearby players–while humans leverage social cues like gaze direction and body pose. These findings reveal a persistent human-model gap in visual social reasoning and underscore the need for architectures that explicitly encode structured behavioral cues to achieve robust, human-like inference.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00269v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00269v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00269v1">http://arxiv.org/abs/2511.00269v1</a><br>Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00270v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00270v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00270v1">http://arxiv.org/abs/2511.00270v1</a><br>Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multi-View Consistent Human Image Customization via In-Context Learning</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_00293v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_00293v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00293v1">http://arxiv.org/abs/2511.00293v1</a><br>Recent advances in personalized generative models demonstrate impressive results in creating identity-consistent images of the same person under diverse settings. Yet, we note that most methods cannot control the viewpoint of the generated image, nor generate consistent multiple views of the person. To address this problem, we propose a lightweight adaptation method, PersonalView, capable of enabling an existing model to acquire multi-view generation capability with as few as 100 training samples. PersonalView consists of two key components: First, we design a conditioning architecture to take advantage of the in-context learning ability of the pre-trained diffusion transformer. Second, we preserve the original generative ability of the pretrained model with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view consistency, text alignment, identity similarity, and visual quality of PersonalView and compare it to recent baselines with potential capability of multi-view customization. PersonalView significantly outperforms baselines trained on a large corpus of multi-view data with only 100 training samples.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Step Toward World Models: A Survey on Robotic Manipulation</title>
    <link href="/2025/10/31/highlights/2025-10-31-2511_02097v1/"/>
    <url>/2025/10/31/highlights/2025-10-31-2511_02097v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02097v1">http://arxiv.org/abs/2511.02097v1</a><br>Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond purely reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and enable prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, rather than directly imposing a fixed definition and limiting our scope to methods explicitly labeled as world models, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a real world model should possess. Building on this analysis, we aim to outline a roadmap for developing generalizable and practical world models for robotics.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_00090v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_00090v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00090v1">http://arxiv.org/abs/2511.00090v1</a><br>We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :<a href="https://github.com/UnicomAI/LeMiCa">https://github.com/UnicomAI/LeMiCa</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_00095v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_00095v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00095v1">http://arxiv.org/abs/2511.00095v1</a><br>The anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3% parsing accuracy and sub-800 ms response times. The software is released on <a href="https://github.com/6jm233333/spinalsam-r1">https://github.com/6jm233333/spinalsam-r1</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>92C55</tag>
      
      <tag>I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_00098v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_00098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00098v1">http://arxiv.org/abs/2511.00098v1</a><br>Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_00099v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_00099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00099v1">http://arxiv.org/abs/2511.00099v1</a><br>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_00103v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_00103v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00103v1">http://arxiv.org/abs/2511.00103v1</a><br>Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: <a href="https://azencot-group.github.io/FreeSliders/">https://azencot-group.github.io/FreeSliders/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_00107v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_00107v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00107v1">http://arxiv.org/abs/2511.00107v1</a><br>Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chain of Time: In-Context Physical Simulation with Image Generation Models</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_00110v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_00110v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00110v1">http://arxiv.org/abs/2511.00110v1</a><br>We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our &#96;&#96;Chain of Time” method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A convolutional neural network deep learning method for model class selection</title>
    <link href="/2025/10/30/cs.AI/2025-10-30-2511_03743v1/"/>
    <url>/2025/10/30/cs.AI/2025-10-30-2511_03743v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03743v1">http://arxiv.org/abs/2511.03743v1</a><br>The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
    <link href="/2025/10/30/cs.IR/2025-10-30-2511_00107v1/"/>
    <url>/2025/10/30/cs.IR/2025-10-30-2511_00107v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00107v1">http://arxiv.org/abs/2511.00107v1</a><br>Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.</p>]]></content>
    
    
    <categories>
      
      <category>cs.IR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</title>
    <link href="/2025/10/30/cs.LG/2025-10-30-2511_00098v1/"/>
    <url>/2025/10/30/cs.LG/2025-10-30-2511_00098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00098v1">http://arxiv.org/abs/2511.00098v1</a><br>Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
    <link href="/2025/10/30/cs.LG/2025-10-30-2511_00099v1/"/>
    <url>/2025/10/30/cs.LG/2025-10-30-2511_00099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00099v1">http://arxiv.org/abs/2511.00099v1</a><br>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</title>
    <link href="/2025/10/30/cs.LG/2025-10-30-2511_00100v1/"/>
    <url>/2025/10/30/cs.LG/2025-10-30-2511_00100v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00100v1">http://arxiv.org/abs/2511.00100v1</a><br>The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A convolutional neural network deep learning method for model class selection</title>
    <link href="/2025/10/30/cs.LG/2025-10-30-2511_03743v1/"/>
    <url>/2025/10/30/cs.LG/2025-10-30-2511_03743v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03743v1">http://arxiv.org/abs/2511.03743v1</a><br>The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</title>
    <link href="/2025/10/30/cs.RO/2025-10-30-2511_00091v1/"/>
    <url>/2025/10/30/cs.RO/2025-10-30-2511_00091v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00091v1">http://arxiv.org/abs/2511.00091v1</a><br>Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist’s deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
    <link href="/2025/10/30/cs.SY/2025-10-30-2511_00099v1/"/>
    <url>/2025/10/30/cs.SY/2025-10-30-2511_00099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00099v1">http://arxiv.org/abs/2511.00099v1</a><br>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</p>]]></content>
    
    
    <categories>
      
      <category>cs.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</title>
    <link href="/2025/10/30/cs.SY/2025-10-30-2511_00100v1/"/>
    <url>/2025/10/30/cs.SY/2025-10-30-2511_00100v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00100v1">http://arxiv.org/abs/2511.00100v1</a><br>The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.</p>]]></content>
    
    
    <categories>
      
      <category>cs.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A convolutional neural network deep learning method for model class selection</title>
    <link href="/2025/10/30/cs.SY/2025-10-30-2511_03743v1/"/>
    <url>/2025/10/30/cs.SY/2025-10-30-2511_03743v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03743v1">http://arxiv.org/abs/2511.03743v1</a><br>The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00090v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00090v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00090v1">http://arxiv.org/abs/2511.00090v1</a><br>We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :<a href="https://github.com/UnicomAI/LeMiCa">https://github.com/UnicomAI/LeMiCa</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00091v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00091v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00091v1">http://arxiv.org/abs/2511.00091v1</a><br>Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist’s deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00095v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00095v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00095v1">http://arxiv.org/abs/2511.00095v1</a><br>The anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3% parsing accuracy and sub-800 ms response times. The software is released on <a href="https://github.com/6jm233333/spinalsam-r1">https://github.com/6jm233333/spinalsam-r1</a>.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>92C55</tag>
      
      <tag>I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00098v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00098v1">http://arxiv.org/abs/2511.00098v1</a><br>Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00100v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00100v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00100v1">http://arxiv.org/abs/2511.00100v1</a><br>The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00099v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00099v1">http://arxiv.org/abs/2511.00099v1</a><br>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00103v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00103v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00103v1">http://arxiv.org/abs/2511.00103v1</a><br>Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: <a href="https://azencot-group.github.io/FreeSliders/">https://azencot-group.github.io/FreeSliders/</a></p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chain of Time: In-Context Physical Simulation with Image Generation Models</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00110v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00110v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00110v1">http://arxiv.org/abs/2511.00110v1</a><br>We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our &#96;&#96;Chain of Time” method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_00107v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_00107v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00107v1">http://arxiv.org/abs/2511.00107v1</a><br>Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>StrengthSense: A Dataset of IMU Signals Capturing Everyday Strength-Demanding Activities</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_02027v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_02027v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02027v1">http://arxiv.org/abs/2511.02027v1</a><br>Tracking strength-demanding activities with wearable sensors like IMUs is crucial for monitoring muscular strength, endurance, and power. However, there is a lack of comprehensive datasets capturing these activities. To fill this gap, we introduce \textit{StrengthSense}, an open dataset that encompasses IMU signals capturing 11 strength-demanding activities, such as sit-to-stand, climbing stairs, and mopping. For comparative purposes, the dataset also includes 2 non-strength demanding activities. The dataset was collected from 29 healthy subjects utilizing 10 IMUs placed on limbs and the torso, and was annotated using video recordings as references. This paper provides a comprehensive overview of the data collection, pre-processing, and technical validation. We conducted a comparative analysis between the joint angles estimated by IMUs and those directly extracted from video to verify the accuracy and reliability of the sensor data. Researchers and developers can utilize \textit{StrengthSense} to advance the development of human activity recognition algorithms, create fitness and health monitoring applications, and more.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A convolutional neural network deep learning method for model class selection</title>
    <link href="/2025/10/30/cs.CV/2025-10-30-2511_03743v1/"/>
    <url>/2025/10/30/cs.CV/2025-10-30-2511_03743v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03743v1">http://arxiv.org/abs/2511.03743v1</a><br>The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</title>
    <link href="/2025/10/30/eess.SP/2025-10-30-2511_00100v1/"/>
    <url>/2025/10/30/eess.SP/2025-10-30-2511_00100v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00100v1">http://arxiv.org/abs/2511.00100v1</a><br>The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
    <link href="/2025/10/30/eess.SP/2025-10-30-2511_00099v1/"/>
    <url>/2025/10/30/eess.SP/2025-10-30-2511_00099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00099v1">http://arxiv.org/abs/2511.00099v1</a><br>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A convolutional neural network deep learning method for model class selection</title>
    <link href="/2025/10/30/eess.SP/2025-10-30-2511_03743v1/"/>
    <url>/2025/10/30/eess.SP/2025-10-30-2511_03743v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03743v1">http://arxiv.org/abs/2511.03743v1</a><br>The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
    <link href="/2025/10/30/eess.SY/2025-10-30-2511_00099v1/"/>
    <url>/2025/10/30/eess.SY/2025-10-30-2511_00099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00099v1">http://arxiv.org/abs/2511.00099v1</a><br>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</title>
    <link href="/2025/10/30/eess.SY/2025-10-30-2511_00100v1/"/>
    <url>/2025/10/30/eess.SY/2025-10-30-2511_00100v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00100v1">http://arxiv.org/abs/2511.00100v1</a><br>The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A convolutional neural network deep learning method for model class selection</title>
    <link href="/2025/10/30/eess.SY/2025-10-30-2511_03743v1/"/>
    <url>/2025/10/30/eess.SY/2025-10-30-2511_03743v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03743v1">http://arxiv.org/abs/2511.03743v1</a><br>The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00090v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00090v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00090v1">http://arxiv.org/abs/2511.00090v1</a><br>We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :<a href="https://github.com/UnicomAI/LeMiCa">https://github.com/UnicomAI/LeMiCa</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00091v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00091v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00091v1">http://arxiv.org/abs/2511.00091v1</a><br>Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist’s deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00095v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00095v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00095v1">http://arxiv.org/abs/2511.00095v1</a><br>The anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3% parsing accuracy and sub-800 ms response times. The software is released on <a href="https://github.com/6jm233333/spinalsam-r1">https://github.com/6jm233333/spinalsam-r1</a>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>92C55</tag>
      
      <tag>I.2.10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00098v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00098v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00098v1">http://arxiv.org/abs/2511.00098v1</a><br>Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00099v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00099v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00099v1">http://arxiv.org/abs/2511.00099v1</a><br>The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00100v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00100v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00100v1">http://arxiv.org/abs/2511.00100v1</a><br>The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
      <tag>stat.AP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00103v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00103v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00103v1">http://arxiv.org/abs/2511.00103v1</a><br>Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: <a href="https://azencot-group.github.io/FreeSliders/">https://azencot-group.github.io/FreeSliders/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00107v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00107v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00107v1">http://arxiv.org/abs/2511.00107v1</a><br>Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chain of Time: In-Context Physical Simulation with Image Generation Models</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_00110v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_00110v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00110v1">http://arxiv.org/abs/2511.00110v1</a><br>We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our &#96;&#96;Chain of Time” method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>StrengthSense: A Dataset of IMU Signals Capturing Everyday Strength-Demanding Activities</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_02027v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_02027v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02027v1">http://arxiv.org/abs/2511.02027v1</a><br>Tracking strength-demanding activities with wearable sensors like IMUs is crucial for monitoring muscular strength, endurance, and power. However, there is a lack of comprehensive datasets capturing these activities. To fill this gap, we introduce \textit{StrengthSense}, an open dataset that encompasses IMU signals capturing 11 strength-demanding activities, such as sit-to-stand, climbing stairs, and mopping. For comparative purposes, the dataset also includes 2 non-strength demanding activities. The dataset was collected from 29 healthy subjects utilizing 10 IMUs placed on limbs and the torso, and was annotated using video recordings as references. This paper provides a comprehensive overview of the data collection, pre-processing, and technical validation. We conducted a comparative analysis between the joint angles estimated by IMUs and those directly extracted from video to verify the accuracy and reliability of the sensor data. Researchers and developers can utilize \textit{StrengthSense} to advance the development of human activity recognition algorithms, create fitness and health monitoring applications, and more.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A convolutional neural network deep learning method for model class selection</title>
    <link href="/2025/10/30/highlights/2025-10-30-2511_03743v1/"/>
    <url>/2025/10/30/highlights/2025-10-30-2511_03743v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.03743v1">http://arxiv.org/abs/2511.03743v1</a><br>The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.SY</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.SY</tag>
      
      <tag>68T05 (Learning and adaptive systems) 93C95 (Neural networks in
  control theory)</tag>
      
      <tag>I.2.6; I.2.8</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</title>
    <link href="/2025/10/29/cs.AI/2025-10-29-2511_00072v1/"/>
    <url>/2025/10/29/cs.AI/2025-10-29-2511_00072v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00072v1">http://arxiv.org/abs/2511.00072v1</a><br>Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3–7% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</title>
    <link href="/2025/10/29/cs.IR/2025-10-29-2511_00072v1/"/>
    <url>/2025/10/29/cs.IR/2025-10-29-2511_00072v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00072v1">http://arxiv.org/abs/2511.00072v1</a><br>Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3–7% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.IR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</title>
    <link href="/2025/10/29/cs.LG/2025-10-29-2511_00072v1/"/>
    <url>/2025/10/29/cs.LG/2025-10-29-2511_00072v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00072v1">http://arxiv.org/abs/2511.00072v1</a><br>Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3–7% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</title>
    <link href="/2025/10/29/cs.CV/2025-10-29-2511_00072v1/"/>
    <url>/2025/10/29/cs.CV/2025-10-29-2511_00072v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00072v1">http://arxiv.org/abs/2511.00072v1</a><br>Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3–7% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures</title>
    <link href="/2025/10/29/cs.CV/2025-10-29-2511_00073v1/"/>
    <url>/2025/10/29/cs.CV/2025-10-29-2511_00073v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00073v1">http://arxiv.org/abs/2511.00073v1</a><br>Rapid climate change and other disturbances in alpine ecosystems demand frequent habitat monitoring, yet manual mapping remains prohibitively expensive for the required temporal resolution. We employ deep learning for change detection using long-term alpine habitat data from Gesaeuse National Park, Austria, addressing a major gap in applying geospatial foundation models (GFMs) to complex natural environments with fuzzy class boundaries and highly imbalanced classes. We compare two paradigms: post-classification change detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the transformer ChangeViT against U-Net baselines. Using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus U-Net’s 41% for multi-class habitat change, while both reach 67% for binary change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net’s 23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy. Although overall accuracies are lower than in more homogeneous landscapes, they reflect realistic performance for complex alpine habitats. Future work will integrate object-based post-processing and physical constraints to enhance applicability.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</title>
    <link href="/2025/10/29/highlights/2025-10-29-2511_00072v1/"/>
    <url>/2025/10/29/highlights/2025-10-29-2511_00072v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00072v1">http://arxiv.org/abs/2511.00072v1</a><br>Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3–7% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.IR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures</title>
    <link href="/2025/10/29/highlights/2025-10-29-2511_00073v1/"/>
    <url>/2025/10/29/highlights/2025-10-29-2511_00073v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00073v1">http://arxiv.org/abs/2511.00073v1</a><br>Rapid climate change and other disturbances in alpine ecosystems demand frequent habitat monitoring, yet manual mapping remains prohibitively expensive for the required temporal resolution. We employ deep learning for change detection using long-term alpine habitat data from Gesaeuse National Park, Austria, addressing a major gap in applying geospatial foundation models (GFMs) to complex natural environments with fuzzy class boundaries and highly imbalanced classes. We compare two paradigms: post-classification change detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the transformer ChangeViT against U-Net baselines. Using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus U-Net’s 41% for multi-class habitat change, while both reach 67% for binary change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net’s 23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy. Although overall accuracies are lower than in more homogeneous landscapes, they reflect realistic performance for complex alpine habitats. Future work will integrate object-based post-processing and physical constraints to enhance applicability.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>World Simulation with Video Foundation Models for Physical AI</title>
    <link href="/2025/10/28/cs.AI/2025-10-28-2511_00062v1/"/>
    <url>/2025/10/28/cs.AI/2025-10-28-2511_00062v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00062v1">http://arxiv.org/abs/2511.00062v1</a><br>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a> and <a href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">https://github.com/nvidia-cosmos/cosmos-transfer2.5</a>. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>World Simulation with Video Foundation Models for Physical AI</title>
    <link href="/2025/10/28/cs.LG/2025-10-28-2511_00062v1/"/>
    <url>/2025/10/28/cs.LG/2025-10-28-2511_00062v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00062v1">http://arxiv.org/abs/2511.00062v1</a><br>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a> and <a href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">https://github.com/nvidia-cosmos/cosmos-transfer2.5</a>. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?</title>
    <link href="/2025/10/28/cs.RO/2025-10-28-2511_00060v1/"/>
    <url>/2025/10/28/cs.RO/2025-10-28-2511_00060v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00060v1">http://arxiv.org/abs/2511.00060v1</a><br>LiDAR-based roadside perception is a cornerstone of advanced Intelligent Transportation Systems (ITS). While considerable research has addressed optimal LiDAR placement for infrastructure, the profound impact of differing LiDAR scanning patterns on perceptual performance remains comparatively under-investigated. The inherent nature of various scanning modes - such as traditional repetitive (mechanical&#x2F;solid-state) versus emerging non-repetitive (e.g. prism-based) systems - leads to distinct point cloud distributions at varying distances, critically dictating the efficacy of object detection and overall environmental understanding. To systematically investigate these differences in infrastructure-based contexts, we introduce the “InfraLiDARs’ Benchmark,” a novel dataset meticulously collected in the CARLA simulation environment using concurrently operating infrastructure-based LiDARs exhibiting both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive statistical analysis of the respective LiDAR scanning abilities and evaluate the impact of these distinct patterns on the performance of various leading 3D object detection algorithms. Our findings reveal that non-repetitive scanning LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable detection performance across various scenarios. Despite non-repetitive LiDAR’s limited perception range, it’s a cost-effective option considering its low price. Ultimately, this study provides insights for setting up roadside perception system with optimal LiDAR scanning patterns and compatible algorithms for diverse roadside applications, and publicly releases the “InfraLiDARs’ Benchmark” dataset to foster further research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>World Simulation with Video Foundation Models for Physical AI</title>
    <link href="/2025/10/28/cs.RO/2025-10-28-2511_00062v1/"/>
    <url>/2025/10/28/cs.RO/2025-10-28-2511_00062v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00062v1">http://arxiv.org/abs/2511.00062v1</a><br>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a> and <a href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">https://github.com/nvidia-cosmos/cosmos-transfer2.5</a>. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>]]></content>
    
    
    <categories>
      
      <category>cs.RO</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing rice leaf images: An overview of image denoising techniques</title>
    <link href="/2025/10/28/cs.CV/2025-10-28-2511_00046v1/"/>
    <url>/2025/10/28/cs.CV/2025-10-28-2511_00046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00046v1">http://arxiv.org/abs/2511.00046v1</a><br>Digital image processing involves the systematic handling of images using advanced computer algorithms, and has gained significant attention in both academic and practical fields. Image enhancement is a crucial preprocessing stage in the image-processing chain, improving image quality and emphasizing features. This makes subsequent tasks (segmentation, feature extraction, classification) more reliable. Image enhancement is essential for rice leaf analysis, aiding in disease detection, nutrient deficiency evaluation, and growth analysis. Denoising followed by contrast enhancement are the primary steps. Image filters, generally employed for denoising, transform or enhance visual characteristics like brightness, contrast, and sharpness, playing a crucial role in improving overall image quality and enabling the extraction of useful information. This work provides an extensive comparative study of well-known image-denoising methods combined with CLAHE (Contrast Limited Adaptive Histogram Equalization) for efficient denoising of rice leaf images. The experiments were performed on a rice leaf image dataset to ensure the data is relevant and representative. Results were examined using various metrics to comprehensively test enhancement methods. This approach provides a strong basis for assessing the effectiveness of methodologies in digital image processing and reveals insights useful for future adaptation in agricultural research and other domains.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68U10, 94A08</tag>
      
      <tag>I.4.3; I.4.4; I.5.1; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?</title>
    <link href="/2025/10/28/cs.CV/2025-10-28-2511_00060v1/"/>
    <url>/2025/10/28/cs.CV/2025-10-28-2511_00060v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00060v1">http://arxiv.org/abs/2511.00060v1</a><br>LiDAR-based roadside perception is a cornerstone of advanced Intelligent Transportation Systems (ITS). While considerable research has addressed optimal LiDAR placement for infrastructure, the profound impact of differing LiDAR scanning patterns on perceptual performance remains comparatively under-investigated. The inherent nature of various scanning modes - such as traditional repetitive (mechanical&#x2F;solid-state) versus emerging non-repetitive (e.g. prism-based) systems - leads to distinct point cloud distributions at varying distances, critically dictating the efficacy of object detection and overall environmental understanding. To systematically investigate these differences in infrastructure-based contexts, we introduce the “InfraLiDARs’ Benchmark,” a novel dataset meticulously collected in the CARLA simulation environment using concurrently operating infrastructure-based LiDARs exhibiting both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive statistical analysis of the respective LiDAR scanning abilities and evaluate the impact of these distinct patterns on the performance of various leading 3D object detection algorithms. Our findings reveal that non-repetitive scanning LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable detection performance across various scenarios. Despite non-repetitive LiDAR’s limited perception range, it’s a cost-effective option considering its low price. Ultimately, this study provides insights for setting up roadside perception system with optimal LiDAR scanning patterns and compatible algorithms for diverse roadside applications, and publicly releases the “InfraLiDARs’ Benchmark” dataset to foster further research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>World Simulation with Video Foundation Models for Physical AI</title>
    <link href="/2025/10/28/cs.CV/2025-10-28-2511_00062v1/"/>
    <url>/2025/10/28/cs.CV/2025-10-28-2511_00062v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00062v1">http://arxiv.org/abs/2511.00062v1</a><br>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a> and <a href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">https://github.com/nvidia-cosmos/cosmos-transfer2.5</a>. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?</title>
    <link href="/2025/10/28/eess.IV/2025-10-28-2511_00060v1/"/>
    <url>/2025/10/28/eess.IV/2025-10-28-2511_00060v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00060v1">http://arxiv.org/abs/2511.00060v1</a><br>LiDAR-based roadside perception is a cornerstone of advanced Intelligent Transportation Systems (ITS). While considerable research has addressed optimal LiDAR placement for infrastructure, the profound impact of differing LiDAR scanning patterns on perceptual performance remains comparatively under-investigated. The inherent nature of various scanning modes - such as traditional repetitive (mechanical&#x2F;solid-state) versus emerging non-repetitive (e.g. prism-based) systems - leads to distinct point cloud distributions at varying distances, critically dictating the efficacy of object detection and overall environmental understanding. To systematically investigate these differences in infrastructure-based contexts, we introduce the “InfraLiDARs’ Benchmark,” a novel dataset meticulously collected in the CARLA simulation environment using concurrently operating infrastructure-based LiDARs exhibiting both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive statistical analysis of the respective LiDAR scanning abilities and evaluate the impact of these distinct patterns on the performance of various leading 3D object detection algorithms. Our findings reveal that non-repetitive scanning LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable detection performance across various scenarios. Despite non-repetitive LiDAR’s limited perception range, it’s a cost-effective option considering its low price. Ultimately, this study provides insights for setting up roadside perception system with optimal LiDAR scanning patterns and compatible algorithms for diverse roadside applications, and publicly releases the “InfraLiDARs’ Benchmark” dataset to foster further research.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Enhancing rice leaf images: An overview of image denoising techniques</title>
    <link href="/2025/10/28/highlights/2025-10-28-2511_00046v1/"/>
    <url>/2025/10/28/highlights/2025-10-28-2511_00046v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00046v1">http://arxiv.org/abs/2511.00046v1</a><br>Digital image processing involves the systematic handling of images using advanced computer algorithms, and has gained significant attention in both academic and practical fields. Image enhancement is a crucial preprocessing stage in the image-processing chain, improving image quality and emphasizing features. This makes subsequent tasks (segmentation, feature extraction, classification) more reliable. Image enhancement is essential for rice leaf analysis, aiding in disease detection, nutrient deficiency evaluation, and growth analysis. Denoising followed by contrast enhancement are the primary steps. Image filters, generally employed for denoising, transform or enhance visual characteristics like brightness, contrast, and sharpness, playing a crucial role in improving overall image quality and enabling the extraction of useful information. This work provides an extensive comparative study of well-known image-denoising methods combined with CLAHE (Contrast Limited Adaptive Histogram Equalization) for efficient denoising of rice leaf images. The experiments were performed on a rice leaf image dataset to ensure the data is relevant and representative. Results were examined using various metrics to comprehensively test enhancement methods. This approach provides a strong basis for assessing the effectiveness of methodologies in digital image processing and reveals insights useful for future adaptation in agricultural research and other domains.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>68U10, 94A08</tag>
      
      <tag>I.4.3; I.4.4; I.5.1; J.3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?</title>
    <link href="/2025/10/28/highlights/2025-10-28-2511_00060v1/"/>
    <url>/2025/10/28/highlights/2025-10-28-2511_00060v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00060v1">http://arxiv.org/abs/2511.00060v1</a><br>LiDAR-based roadside perception is a cornerstone of advanced Intelligent Transportation Systems (ITS). While considerable research has addressed optimal LiDAR placement for infrastructure, the profound impact of differing LiDAR scanning patterns on perceptual performance remains comparatively under-investigated. The inherent nature of various scanning modes - such as traditional repetitive (mechanical&#x2F;solid-state) versus emerging non-repetitive (e.g. prism-based) systems - leads to distinct point cloud distributions at varying distances, critically dictating the efficacy of object detection and overall environmental understanding. To systematically investigate these differences in infrastructure-based contexts, we introduce the “InfraLiDARs’ Benchmark,” a novel dataset meticulously collected in the CARLA simulation environment using concurrently operating infrastructure-based LiDARs exhibiting both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive statistical analysis of the respective LiDAR scanning abilities and evaluate the impact of these distinct patterns on the performance of various leading 3D object detection algorithms. Our findings reveal that non-repetitive scanning LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable detection performance across various scenarios. Despite non-repetitive LiDAR’s limited perception range, it’s a cost-effective option considering its low price. Ultimately, this study provides insights for setting up roadside perception system with optimal LiDAR scanning patterns and compatible algorithms for diverse roadside applications, and publicly releases the “InfraLiDARs’ Benchmark” dataset to foster further research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.RO</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>World Simulation with Video Foundation Models for Physical AI</title>
    <link href="/2025/10/28/highlights/2025-10-28-2511_00062v1/"/>
    <url>/2025/10/28/highlights/2025-10-28-2511_00062v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00062v1">http://arxiv.org/abs/2511.00062v1</a><br>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a> and <a href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">https://github.com/nvidia-cosmos/cosmos-transfer2.5</a>. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.RO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra</title>
    <link href="/2025/10/27/cs.DC/2025-10-27-2511_00037v1/"/>
    <url>/2025/10/27/cs.DC/2025-10-27-2511_00037v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00037v1">http://arxiv.org/abs/2511.00037v1</a><br>Federated Learning (FL) has emerged as a transformative paradigm in medical AI, enabling collaborative model training across institutions without direct data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE, Flower, and Owkin Substra to evaluate their suitability for medical imaging applications in real-world settings. Using the PathMNIST dataset, we assess model performance, convergence efficiency, communication overhead, scalability, and developer experience. Results indicate that NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features. Each framework exhibits strengths optimized for distinct use cases, emphasizing their relevance to practical deployment in healthcare environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.DC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra</title>
    <link href="/2025/10/27/cs.CV/2025-10-27-2511_00037v1/"/>
    <url>/2025/10/27/cs.CV/2025-10-27-2511_00037v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00037v1">http://arxiv.org/abs/2511.00037v1</a><br>Federated Learning (FL) has emerged as a transformative paradigm in medical AI, enabling collaborative model training across institutions without direct data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE, Flower, and Owkin Substra to evaluate their suitability for medical imaging applications in real-world settings. Using the PathMNIST dataset, we assess model performance, convergence efficiency, communication overhead, scalability, and developer experience. Results indicate that NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features. Each framework exhibits strengths optimized for distinct use cases, emphasizing their relevance to practical deployment in healthcare environments.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra</title>
    <link href="/2025/10/27/highlights/2025-10-27-2511_00037v1/"/>
    <url>/2025/10/27/highlights/2025-10-27-2511_00037v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00037v1">http://arxiv.org/abs/2511.00037v1</a><br>Federated Learning (FL) has emerged as a transformative paradigm in medical AI, enabling collaborative model training across institutions without direct data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE, Flower, and Owkin Substra to evaluate their suitability for medical imaging applications in real-world settings. Using the PathMNIST dataset, we assess model performance, convergence efficiency, communication overhead, scalability, and developer experience. Results indicate that NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features. Each framework exhibits strengths optimized for distinct use cases, emphasizing their relevance to practical deployment in healthcare environments.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.DC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mutual Information guided Visual Contrastive Learning</title>
    <link href="/2025/10/26/cs.AI/2025-10-26-2511_00028v1/"/>
    <url>/2025/10/26/cs.AI/2025-10-26-2511_00028v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00028v1">http://arxiv.org/abs/2511.00028v1</a><br>Representation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mutual Information guided Visual Contrastive Learning</title>
    <link href="/2025/10/26/cs.CV/2025-10-26-2511_00028v1/"/>
    <url>/2025/10/26/cs.CV/2025-10-26-2511_00028v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00028v1">http://arxiv.org/abs/2511.00028v1</a><br>Representation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData</title>
    <link href="/2025/10/26/cs.CV/2025-10-26-2511_02849v1/"/>
    <url>/2025/10/26/cs.CV/2025-10-26-2511_02849v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02849v1">http://arxiv.org/abs/2511.02849v1</a><br>Individualized therapy is driven forward by medical data analysis, which provides insight into the patient’s context. In particular, for Type 1 Diabetes (T1D), which is an autoimmune disease, relationships between demographics, sensor data, and context can be analyzed. However, outliers, noisy data, and small data volumes cannot provide a reliable analysis. Hence, the research domain requires large volumes of high-quality data. Moreover, missing values can lead to information loss. To address this limitation, this study improves the data quality of DiaData, an integration of 15 separate datasets containing glucose values from 2510 subjects with T1D. Notably, we make the following contributions: 1) Outliers are identified with the interquartile range (IQR) approach and treated by replacing them with missing values. 2) Small gaps ($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30 and $&lt;$ 120 min) with Stineman interpolation. Based on a visual comparison, Stineman interpolation provides more realistic glucose estimates than linear interpolation for larger gaps. 3) After data cleaning, the correlation between glucose and heart rate is analyzed, yielding a moderate relation between 15 and 60 minutes before hypoglycemia ($\le$ 70 mg&#x2F;dL). 4) Finally, a benchmark for hypoglycemia classification is provided with a state-of-the-art ResNet model. The model is trained with the Maindatabase and Subdatabase II of DiaData to classify hypoglycemia onset up to 2 hours in advance. Training with more data improves performance by 7% while using quality-refined data yields a 2-3% gain compared to raw data.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData</title>
    <link href="/2025/10/26/eess.IV/2025-10-26-2511_02849v1/"/>
    <url>/2025/10/26/eess.IV/2025-10-26-2511_02849v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02849v1">http://arxiv.org/abs/2511.02849v1</a><br>Individualized therapy is driven forward by medical data analysis, which provides insight into the patient’s context. In particular, for Type 1 Diabetes (T1D), which is an autoimmune disease, relationships between demographics, sensor data, and context can be analyzed. However, outliers, noisy data, and small data volumes cannot provide a reliable analysis. Hence, the research domain requires large volumes of high-quality data. Moreover, missing values can lead to information loss. To address this limitation, this study improves the data quality of DiaData, an integration of 15 separate datasets containing glucose values from 2510 subjects with T1D. Notably, we make the following contributions: 1) Outliers are identified with the interquartile range (IQR) approach and treated by replacing them with missing values. 2) Small gaps ($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30 and $&lt;$ 120 min) with Stineman interpolation. Based on a visual comparison, Stineman interpolation provides more realistic glucose estimates than linear interpolation for larger gaps. 3) After data cleaning, the correlation between glucose and heart rate is analyzed, yielding a moderate relation between 15 and 60 minutes before hypoglycemia ($\le$ 70 mg&#x2F;dL). 4) Finally, a benchmark for hypoglycemia classification is provided with a state-of-the-art ResNet model. The model is trained with the Maindatabase and Subdatabase II of DiaData to classify hypoglycemia onset up to 2 hours in advance. Training with more data improves performance by 7% while using quality-refined data yields a 2-3% gain compared to raw data.</p>]]></content>
    
    
    <categories>
      
      <category>eess.IV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData</title>
    <link href="/2025/10/26/eess.SP/2025-10-26-2511_02849v1/"/>
    <url>/2025/10/26/eess.SP/2025-10-26-2511_02849v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02849v1">http://arxiv.org/abs/2511.02849v1</a><br>Individualized therapy is driven forward by medical data analysis, which provides insight into the patient’s context. In particular, for Type 1 Diabetes (T1D), which is an autoimmune disease, relationships between demographics, sensor data, and context can be analyzed. However, outliers, noisy data, and small data volumes cannot provide a reliable analysis. Hence, the research domain requires large volumes of high-quality data. Moreover, missing values can lead to information loss. To address this limitation, this study improves the data quality of DiaData, an integration of 15 separate datasets containing glucose values from 2510 subjects with T1D. Notably, we make the following contributions: 1) Outliers are identified with the interquartile range (IQR) approach and treated by replacing them with missing values. 2) Small gaps ($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30 and $&lt;$ 120 min) with Stineman interpolation. Based on a visual comparison, Stineman interpolation provides more realistic glucose estimates than linear interpolation for larger gaps. 3) After data cleaning, the correlation between glucose and heart rate is analyzed, yielding a moderate relation between 15 and 60 minutes before hypoglycemia ($\le$ 70 mg&#x2F;dL). 4) Finally, a benchmark for hypoglycemia classification is provided with a state-of-the-art ResNet model. The model is trained with the Maindatabase and Subdatabase II of DiaData to classify hypoglycemia onset up to 2 hours in advance. Training with more data improves performance by 7% while using quality-refined data yields a 2-3% gain compared to raw data.</p>]]></content>
    
    
    <categories>
      
      <category>eess.SP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mutual Information guided Visual Contrastive Learning</title>
    <link href="/2025/10/26/highlights/2025-10-26-2511_00028v1/"/>
    <url>/2025/10/26/highlights/2025-10-26-2511_00028v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00028v1">http://arxiv.org/abs/2511.00028v1</a><br>Representation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData</title>
    <link href="/2025/10/26/highlights/2025-10-26-2511_02849v1/"/>
    <url>/2025/10/26/highlights/2025-10-26-2511_02849v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.02849v1">http://arxiv.org/abs/2511.02849v1</a><br>Individualized therapy is driven forward by medical data analysis, which provides insight into the patient’s context. In particular, for Type 1 Diabetes (T1D), which is an autoimmune disease, relationships between demographics, sensor data, and context can be analyzed. However, outliers, noisy data, and small data volumes cannot provide a reliable analysis. Hence, the research domain requires large volumes of high-quality data. Moreover, missing values can lead to information loss. To address this limitation, this study improves the data quality of DiaData, an integration of 15 separate datasets containing glucose values from 2510 subjects with T1D. Notably, we make the following contributions: 1) Outliers are identified with the interquartile range (IQR) approach and treated by replacing them with missing values. 2) Small gaps ($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30 and $&lt;$ 120 min) with Stineman interpolation. Based on a visual comparison, Stineman interpolation provides more realistic glucose estimates than linear interpolation for larger gaps. 3) After data cleaning, the correlation between glucose and heart rate is analyzed, yielding a moderate relation between 15 and 60 minutes before hypoglycemia ($\le$ 70 mg&#x2F;dL). 4) Finally, a benchmark for hypoglycemia classification is provided with a state-of-the-art ResNet model. The model is trained with the Maindatabase and Subdatabase II of DiaData to classify hypoglycemia onset up to 2 hours in advance. Training with more data improves performance by 7% while using quality-refined data yields a 2-3% gain compared to raw data.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>eess.SP</tag>
      
      <tag>eess.IV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Detection of Fake Reviews using BERT and ResNet-50</title>
    <link href="/2025/10/24/cs.AI/2025-10-24-2511_00020v1/"/>
    <url>/2025/10/24/cs.AI/2025-10-24-2511_00020v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00020v1">http://arxiv.org/abs/2511.00020v1</a><br>In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model’s ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets</title>
    <link href="/2025/10/24/cs.AI/2025-10-24-2511_00021v1/"/>
    <url>/2025/10/24/cs.AI/2025-10-24-2511_00021v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00021v1">http://arxiv.org/abs/2511.00021v1</a><br>Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Detection of Fake Reviews using BERT and ResNet-50</title>
    <link href="/2025/10/24/cs.CL/2025-10-24-2511_00020v1/"/>
    <url>/2025/10/24/cs.CL/2025-10-24-2511_00020v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00020v1">http://arxiv.org/abs/2511.00020v1</a><br>In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model’s ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Detection of Fake Reviews using BERT and ResNet-50</title>
    <link href="/2025/10/24/cs.CV/2025-10-24-2511_00020v1/"/>
    <url>/2025/10/24/cs.CV/2025-10-24-2511_00020v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00020v1">http://arxiv.org/abs/2511.00020v1</a><br>In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model’s ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets</title>
    <link href="/2025/10/24/cs.CV/2025-10-24-2511_00021v1/"/>
    <url>/2025/10/24/cs.CV/2025-10-24-2511_00021v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00021v1">http://arxiv.org/abs/2511.00021v1</a><br>Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline</title>
    <link href="/2025/10/24/cs.CV/2025-10-24-2511_00022v1/"/>
    <url>/2025/10/24/cs.CV/2025-10-24-2511_00022v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00022v1">http://arxiv.org/abs/2511.00022v1</a><br>Coral reef monitoring in the Western Indian Ocean is limited by the labor demands of underwater visual censuses. This work evaluates a YOLOv8-based deep learning pipeline for automating family-level fish identification from video transects collected in Kenya and Tanzania. A curated dataset of 24 families was tested under different configurations, providing the first region-specific benchmark for automated reef fish monitoring in the Western Indian Ocean. The best model achieved <a href="mailto:&#109;&#x41;&#x50;&#64;&#48;&#46;&#53;">mAP@0.5</a> of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa. Results demonstrate the potential of deep learning as a scalable complement to traditional monitoring methods.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Detection of Fake Reviews using BERT and ResNet-50</title>
    <link href="/2025/10/24/highlights/2025-10-24-2511_00020v1/"/>
    <url>/2025/10/24/highlights/2025-10-24-2511_00020v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00020v1">http://arxiv.org/abs/2511.00020v1</a><br>In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model’s ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets</title>
    <link href="/2025/10/24/highlights/2025-10-24-2511_00021v1/"/>
    <url>/2025/10/24/highlights/2025-10-24-2511_00021v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00021v1">http://arxiv.org/abs/2511.00021v1</a><br>Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline</title>
    <link href="/2025/10/24/highlights/2025-10-24-2511_00022v1/"/>
    <url>/2025/10/24/highlights/2025-10-24-2511_00022v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00022v1">http://arxiv.org/abs/2511.00022v1</a><br>Coral reef monitoring in the Western Indian Ocean is limited by the labor demands of underwater visual censuses. This work evaluates a YOLOv8-based deep learning pipeline for automating family-level fish identification from video transects collected in Kenya and Tanzania. A curated dataset of 24 families was tested under different configurations, providing the first region-specific benchmark for automated reef fish monitoring in the Western Indian Ocean. The best model achieved <a href="mailto:&#x6d;&#65;&#80;&#64;&#x30;&#x2e;&#x35;">mAP@0.5</a> of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa. Results demonstrate the potential of deep learning as a scalable complement to traditional monitoring methods.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative human motion mimicking through feature extraction in denoising diffusion settings</title>
    <link href="/2025/10/16/cs.AI/2025-10-16-2511_00011v1/"/>
    <url>/2025/10/16/cs.AI/2025-10-16-2511_00011v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00011v1">http://arxiv.org/abs/2511.00011v1</a><br>Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also “creatively” enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative human motion mimicking through feature extraction in denoising diffusion settings</title>
    <link href="/2025/10/16/cs.HC/2025-10-16-2511_00011v1/"/>
    <url>/2025/10/16/cs.HC/2025-10-16-2511_00011v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00011v1">http://arxiv.org/abs/2511.00011v1</a><br>Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also “creatively” enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.</p>]]></content>
    
    
    <categories>
      
      <category>cs.HC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative human motion mimicking through feature extraction in denoising diffusion settings</title>
    <link href="/2025/10/16/cs.CV/2025-10-16-2511_00011v1/"/>
    <url>/2025/10/16/cs.CV/2025-10-16-2511_00011v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00011v1">http://arxiv.org/abs/2511.00011v1</a><br>Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also “creatively” enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative human motion mimicking through feature extraction in denoising diffusion settings</title>
    <link href="/2025/10/16/highlights/2025-10-16-2511_00011v1/"/>
    <url>/2025/10/16/highlights/2025-10-16-2511_00011v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00011v1">http://arxiv.org/abs/2511.00011v1</a><br>Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also “creatively” enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.HC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
    <link href="/2025/10/04/cs.AI/2025-10-04-2511_00004v1/"/>
    <url>/2025/10/04/cs.AI/2025-10-04-2511_00004v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00004v1">http://arxiv.org/abs/2511.00004v1</a><br>Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
    <link href="/2025/10/04/cs.CL/2025-10-04-2511_00004v1/"/>
    <url>/2025/10/04/cs.CL/2025-10-04-2511_00004v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00004v1">http://arxiv.org/abs/2511.00004v1</a><br>Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
    <link href="/2025/10/04/cs.CY/2025-10-04-2511_00004v1/"/>
    <url>/2025/10/04/cs.CY/2025-10-04-2511_00004v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00004v1">http://arxiv.org/abs/2511.00004v1</a><br>Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CY</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
    <link href="/2025/10/04/cs.CV/2025-10-04-2511_00004v1/"/>
    <url>/2025/10/04/cs.CV/2025-10-04-2511_00004v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00004v1">http://arxiv.org/abs/2511.00004v1</a><br>Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
    <link href="/2025/10/04/highlights/2025-10-04-2511_00004v1/"/>
    <url>/2025/10/04/highlights/2025-10-04-2511_00004v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00004v1">http://arxiv.org/abs/2511.00004v1</a><br>Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.AI</tag>
      
      <tag>cs.CY</tag>
      
      <tag>cs.CL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games</title>
    <link href="/2025/09/18/cs.AI/2025-09-18-2511_00002v1/"/>
    <url>/2025/09/18/cs.AI/2025-09-18-2511_00002v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00002v1">http://arxiv.org/abs/2511.00002v1</a><br>Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry’s rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent’s temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.</p>]]></content>
    
    
    <categories>
      
      <category>cs.AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games</title>
    <link href="/2025/09/18/cs.LG/2025-09-18-2511_00002v1/"/>
    <url>/2025/09/18/cs.LG/2025-09-18-2511_00002v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00002v1">http://arxiv.org/abs/2511.00002v1</a><br>Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry’s rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent’s temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.</p>]]></content>
    
    
    <categories>
      
      <category>cs.LG</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games</title>
    <link href="/2025/09/18/cs.CV/2025-09-18-2511_00002v1/"/>
    <url>/2025/09/18/cs.CV/2025-09-18-2511_00002v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00002v1">http://arxiv.org/abs/2511.00002v1</a><br>Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry’s rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent’s temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.</p>]]></content>
    
    
    <categories>
      
      <category>cs.CV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games</title>
    <link href="/2025/09/18/highlights/2025-09-18-2511_00002v1/"/>
    <url>/2025/09/18/highlights/2025-09-18-2511_00002v1/</url>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/2511.00002v1">http://arxiv.org/abs/2511.00002v1</a><br>Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry’s rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent’s temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs.CV</tag>
      
      <tag>cs.LG</tag>
      
      <tag>cs.AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
