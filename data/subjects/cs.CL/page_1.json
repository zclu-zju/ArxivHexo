[{"doi":"2511.05017v1","title":"Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings","authors":["Aakriti Agrawal","Gouthaman KV","Rohith Aralikatti","Gauri Jagatap","Jiaxin Yuan","Vijay Kamarshi","Andrea Fanelli","Furong Huang"],"published":"2025-11-07","url":"http://arxiv.org/abs/2511.05017v1"},{"doi":"2511.04570v1","title":"Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm","authors":["Jingqi Tong","Yurong Mou","Hangcheng Li","Mingzhe Li","Yongzhuo Yang","Ming Zhang","Qiguang Chen","Tianyi Liang","Xiaomeng Hu","Yining Zheng","Xinchi Chen","Jun Zhao","Xuanjing Huang","Xipeng Qiu"],"published":"2025-11-06","url":"http://arxiv.org/abs/2511.04570v1"},{"doi":"2511.04161v1","title":"Seeing Straight: Document Orientation Detection for Efficient OCR","authors":["Suranjan Goswami","Abhinav Ravi","Raja Kolla","Ali Faraz","Shaharukh Khan","Akash","Chandra Khatri","Shubham Agarwal"],"published":"2025-11-06","url":"http://arxiv.org/abs/2511.04161v1"},{"doi":"2511.04583v1","title":"Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper","authors":["Atsuyuki Miyai","Mashiro Toyooka","Takashi Otonari","Zaiying Zhao","Kiyoharu Aizawa"],"published":"2025-11-06","url":"http://arxiv.org/abs/2511.04583v1"},{"doi":"2511.03328v1","title":"Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks","authors":["Jindong Hong","Tianjie Chen","Lingjie Luo","Chuanyang Zheng","Ting Xu","Haibao Yu","Jianing Qiu","Qianzhong Chen","Suning Huang","Yan Xu","Yong Gui","Yijun He","Jiankai Sun"],"published":"2025-11-05","url":"http://arxiv.org/abs/2511.03328v1"},{"doi":"2511.02778v1","title":"VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation","authors":["Kevin Qinghong Lin","Yuhao Zheng","Hangyu Ran","Dantong Zhu","Dongxing Mao","Linjie Li","Philip Torr","Alex Jinpeng Wang"],"published":"2025-11-04","url":"http://arxiv.org/abs/2511.02778v1"},{"doi":"2511.02607v1","title":"UniChange: Unifying Change Detection with Multimodal Large Language Model","authors":["Xu Zhang","Danyang Li","Xiaohang Dong","Tianhao Wu","Hualong Yu","Jianye Wang","Qicheng Li","Xiang Li"],"published":"2025-11-04","url":"http://arxiv.org/abs/2511.02607v1"},{"doi":"2511.02495v1","title":"DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding","authors":["Zixuan Liu","Siavash H. Khajavi","Guangkai Jiang"],"published":"2025-11-04","url":"http://arxiv.org/abs/2511.02495v1"},{"doi":"2511.02360v1","title":"CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning","authors":["Jizheng Ma","Xiaofei Zhou","Yanlong Song","Han Yan"],"published":"2025-11-04","url":"http://arxiv.org/abs/2511.02360v1"},{"doi":"2511.02288v1","title":"Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions","authors":["Cuong Tuan Nguyen","Ngoc Tuan Nguyen","Triet Hoang Minh Dao","Huy Minh Nhat","Huy Truong Dinh"],"published":"2025-11-04","url":"http://arxiv.org/abs/2511.02288v1"},{"doi":"2511.02280v1","title":"SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning","authors":["Fangxun Shu","Yongjie Ye","Yue Liao","Zijian Kang","Weijie Yin","Jiacong Wang","Xiao Liang","Shuicheng Yan","Chao Feng"],"published":"2025-11-04","url":"http://arxiv.org/abs/2511.02280v1"},{"doi":"2511.01618v1","title":"Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models","authors":["Xiaoyu Zhan","Wenxuan Huang","Hao Sun","Xinyu Fu","Changfeng Ma","Shaosheng Cao","Bohan Jia","Shaohui Lin","Zhenfei Yin","Lei Bai","Wanli Ouyang","Yuanqi Li","Jie Guo","Yanwen Guo"],"published":"2025-11-03","url":"http://arxiv.org/abs/2511.01618v1"},{"doi":"2511.01340v1","title":"$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles","authors":["Trishanu Das","Abhilash Nandy","Khush Bajaj","Deepiha S"],"published":"2025-11-03","url":"http://arxiv.org/abs/2511.01340v1"},{"doi":"2511.00810v1","title":"GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding","authors":["Shijie Zhou","Viet Dac Lai","Hao Tan","Jihyung Kil","Wanrong Zhu","Changyou Chen","Ruiyi Zhang"],"published":"2025-11-02","url":"http://arxiv.org/abs/2511.00810v1"},{"doi":"2511.00749v2","title":"Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models","authors":["Tanvi Dinkar","Aiqi Jiang","Gavin Abercrombie","Ioannis Konstas"],"published":"2025-11-02","url":"http://arxiv.org/abs/2511.00749v2"},{"doi":"2511.04699v1","title":"Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding","authors":["Haneen Al-Homoud","Asma Ibrahim","Murtadha Al-Jubran","Fahad Al-Otaibi","Yazeed Al-Harbi","Daulet Toibazar","Kesen Wang","Pedro J. Moreno"],"published":"2025-11-01","url":"http://arxiv.org/abs/2511.04699v1"},{"doi":"2511.00270v1","title":"POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation","authors":["Abhinav Joshi","Vaibhav Sharma","Sanjeet Singh","Ashutosh Modi"],"published":"2025-10-31","url":"http://arxiv.org/abs/2511.00270v1"},{"doi":"2511.00020v1","title":"Multimodal Detection of Fake Reviews using BERT and ResNet-50","authors":["Suhasnadh Reddy Veluru","Sai Teja Erukude","Viswa Chaitanya Marella"],"published":"2025-10-24","url":"http://arxiv.org/abs/2511.00020v1"},{"doi":"2511.00004v1","title":"Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment","authors":["Adrian-Dinu Urse","Dumitru-Clementin Cercel","Florin Pop"],"published":"2025-10-04","url":"http://arxiv.org/abs/2511.00004v1"}]